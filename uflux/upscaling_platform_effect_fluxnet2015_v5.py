# -*- coding: utf-8 -*-
"""Upscaling-platform-effect-FLUXNET2015-v5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1751rnaIGiwqcitSPLEy5TqkQLFvwb0rR
"""

!pip install scitbx --quiet
# !pip install netcdf4 --quiet

from scitbx import google
root = google.mount_drive()
root_proj = root.joinpath("workspace/project_data/platform")

from scitbx.easy_import import *
from scitbx.utils import *
from scitbx.sciplt import *
from sciml.regress2 import regress2
from scigeo.remotesensing import *
from scigeo import climate
from sciml import pipelines

# ==============================================================================

from shapely.geometry import box
# from shapely.ops import cascaded_union

world = gpd.read_file((r'https://github.com/soonyenju/scieco/blob/main/scieco/data/WB_countries_Admin0_10m.json?raw=true'))

MODIS_IGBP_dict = get_MODIS_IGBPcode()
coef = convert_gCm2d1_PgCyr_025deg()
df_el = climate.get_ONI()

meta = pd.read_csv((r'https://github.com/soonyenju/scieco/blob/main/scieco/data/fluxnet_meta_212.csv?raw=true'),index_col = 0)
# meta = pd.read_excel(root.joinpath("fmt_fluxdata/fluxnet_meta_212.xlsx"), index_col = 1)
# meta = meta.replace('Wet', 'WET')
# meta['LAT'] = meta['LAT'].astype('float')
# meta['LON'] = meta['LON'].astype('float')

quiet()

"""# EC site level data download

| IGBP Code | Land Cover Type                          | Typical Canopy Height | Typical Tower Height | 80% Footprint Radius Estimate |
|-----------|-------------------------------------------|------------------------|-----------------------|-------------------------------|
| BSV       | Barren Sparse Vegetation                 | < 0.1 m                | ~2 m                  | 50–100 m                      |
| CRO       | Croplands                                | 0.5–2 m                | 2–4 m                 | 100–200 m                     |
| CSH       | Closed Shrublands                        | 1–2 m                  | 3–5 m                 | 100–200 m                     |
| CVM       | Cropland/Natural Vegetation Mosaics      | Mixed (0.5–10 m)       | 5–10 m                | 150–300 m                     |
| DBF       | Deciduous Broadleaf Forests              | 15–30 m                | 30–40 m               | 300–600 m                     |
| DNF       | Deciduous Needleleaf Forests             | 10–20 m                | 30–40 m               | 300–600 m                     |
| EBF       | Evergreen Broadleaf Forests              | 20–45 m                | 40–50 m               | 400–700 m                     |
| ENF       | Evergreen Needleleaf Forests             | 15–30 m                | 30–40 m               | 300–600 m                     |
| GRA       | Grasslands                               | 0.2–1 m                | 2–3 m                 | 80–150 m                      |
| MF        | Mixed Forests                            | 15–35 m                | 30–40 m               | 300–600 m                     |
| OSH       | Open Shrublands                          | 0.5–2 m                | 2–4 m                 | 100–200 m                     |
| SAV       | Savannas                                 | 2–10 m                 | 10–20 m               | 200–400 m                     |
| SNO       | Snow and Ice                             | —                      | —                     | 100–500 m (highly variable)   |
| URB       | Urban and Built-Up Lands                 | ~5–20 m (obstacles)    | 10–30 m               | 200–500 m                     |
| WAT       | Water Bodies                             | Flat surface           | 2–10 m                | 100–300 m                     |
| WET       | Permanent Wetlands                       | 0.5–3 m                | 3–5 m                 | 100–250 m                     |
| WSA       | Woody Savannas (assumed from WSA)        | 5–15 m                 | 10–20 m               | 200–400 m                     |
"""

# !pip install scigee --quiet
!pip install git+https://github.com/soonyenju/scigee.git  --quiet
from scigee import *
from scigee.utils import landsat_apply_scale_factors, mask_landsatsr_clouds, mask_sentinel2sr_clouds

geeface.init_gee('ee-zhusy93')

"""## 5 km resolution satellite R and NIR reflectance"""

# Download

savefile_raw_pickle = root_proj.joinpath('Satellite_surface_reflectance_NIR_R_5km.pkl')

if savefile_raw_pickle.exists():
    ddo = load_pickle(savefile_raw_pickle)
else:
    ddo = {}
    for site, row in meta.iterrows():
        if site in list(ddo.keys()): continue
        print(site)
        lat, lon = meta.loc[site, ['LAT', 'LON']]

        collection_l5 = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2').map(landsat_apply_scale_factors).map(mask_landsatsr_clouds)
        collection_l7 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2').map(landsat_apply_scale_factors).map(mask_landsatsr_clouds)
        collection_l8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2').map(landsat_apply_scale_factors).map(mask_landsatsr_clouds)
        # collection_s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').map(mask_sentinel2sr_clouds)
        collection_s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 20))

        ls5 = geeface.gee2df(collection_l5, lat, lon, ['2000-01-01', '2024-01-01'], ['SR_B3', 'SR_B4'], 5000)
        ls7 = geeface.gee2df(collection_l7, lat, lon, ['2000-01-01', '2024-01-01'], ['SR_B3', 'SR_B4'], 5000)
        ls8 = geeface.gee2df(collection_l8, lat, lon, ['2000-01-01', '2024-01-01'], ['SR_B4', 'SR_B5'], 5000)
        s2 = geeface.gee2df(collection_s2, lat, lon, ['2000-01-01', '2024-01-01'], ['B4', 'B8'], 5000) / 1e4
        modis = geeface.gee2df('MODIS/061/MCD43A4', lat, lon, ['2000-01-01', '2024-01-01'], ['Nadir_Reflectance_Band1', 'Nadir_Reflectance_Band2'], 5000) / 1e4

        dd = {
            'Landsat5': ls5,
            'Landsat7': ls7,
            'Landsat8': ls8,
            'Sentinel2': s2,
            'MODIS': modis
        }
        ddo[site] = dd
    dump_pickle(ddo, savefile_raw_pickle)

# # -------------------------------------------------------------------------------------------------------
# # Combine two pickles
# ddo_new = load_pickle(savefile_raw_pickle)
# ddo_old = load_pickle(root_proj.joinpath('0cloud_unmasked/Satellite_surface_reflectance_NIR_R_5km.pkl'))

# ddo = {}
# for site in ddo_new.keys():
#     dd_new = ddo_new[site]
#     dd_old = ddo_old[site]
#     dd = deepcopy(dd_new)
#     dd['Sentinel2'] = dd['Sentinel2'] / 1e4 # If Sentinel-2 is not scaled
#     dd['MODIS'] = deepcopy(dd_old['MODIS'])
#     ddo[site] = dd

savefile_nc = root_proj.joinpath('Satellite_surface_reflectance_NIR_R_5km.nc')

if savefile_nc.exists():
    nc_sat_5km = xr.open_dataset(savefile_nc)
else:
    ddo = load_pickle(savefile_raw_pickle)
    dfo = []
    for site, dd in ddo.items():
        df_site = []
        for sate, dft in dd.items():
            # print(sate)
            # dft.columns = sate + '-' + dft.columns
            if not dft.empty: dft = dft.resample('1D').mean().dropna()
            dft.columns = ['R', 'NIR']
            dft['satellite'] = sate
            df_site.append(dft)
        df_site = pd.concat(df_site, axis = 0)
        # df_site = pd.concat([df_site, pd.DataFrame(index = pd.date_range('2000-01-01', '2023-12-31', freq = '1D'))], axis = 1)
        df_site = df_site.reset_index()
        df_site['ID'] = site
        dfo.append(df_site)
    dfo = pd.concat(dfo, axis = 0)
    dfo = dfo.rename(columns = {'DATETIME': 'time'})
    nco = dfo.set_index(['ID', 'satellite', 'time']).to_xarray()
    nco.R.attrs['long_name'] = "'SR_B3' for Landsat-5/-7;\n'SR_B4' for Landsat-8;\n'B4' for Sentinel-2;\n'Nadir_Reflectance_Band1' for MODIS"
    nco.NIR.attrs['long_name'] = "'SR_B4' for Landsat-5/-7;\n'SR_B5' for Landsat-8;\n'B8' for Sentinel-2;\n'Nadir_Reflectance_Band2' for MODIS"
    nco.ID.attrs['eddy_covariance'] = '212 FLUXNET2015 Tier-1 and Tier-2 eddy covariance towers'
    nco.satellite.attrs['satellite_info'] = "Interpolation resolution: 5 km;\nLandsat-5: 'LANDSAT/LT05/C02/T1_L2';\nLandsat-7: 'LANDSAT/LE07/C02/T1_L2';\nLandsat-8: 'LANDSAT/LC08/C02/T1_L2';\nSentinel-2: 'COPERNICUS/S2_SR_HARMONIZED';\nMODIS: 'MODIS/061/MCD43A4'"
    nco.time.attrs['time_info'] = '2000-01-01 to 2023-12-31 on a daily basis'
    nco.to_netcdf(savefile_nc)
    nc_sat_5km = nco; nco.close(); del(nco)

"""## Moderate (500-m) resolution satellite R and NIR reflectance"""

# Download

savefile_raw_pickle = root_proj.joinpath('Satellite_surface_reflectance_NIR_R_500m.pkl')

if savefile_raw_pickle.exists():
    ddo = load_pickle(savefile_raw_pickle)
else:
    ddo = {}
    for site, row in meta.iterrows():
        if site in list(ddo.keys()): continue
        print(site)
        lat, lon = meta.loc[site, ['LAT', 'LON']]

        collection_l5 = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2').map(landsat_apply_scale_factors).map(mask_landsatsr_clouds)
        collection_l7 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2').map(landsat_apply_scale_factors).map(mask_landsatsr_clouds)
        collection_l8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2').map(landsat_apply_scale_factors).map(mask_landsatsr_clouds)
        # collection_s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').map(mask_sentinel2sr_clouds)
        collection_s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 20))

        ls5 = geeface.gee2df(collection_l5, lat, lon, ['2000-01-01', '2024-01-01'], ['SR_B3', 'SR_B4'], 500)
        ls7 = geeface.gee2df(collection_l7, lat, lon, ['2000-01-01', '2024-01-01'], ['SR_B3', 'SR_B4'], 500)
        ls8 = geeface.gee2df(collection_l8, lat, lon, ['2000-01-01', '2024-01-01'], ['SR_B4', 'SR_B5'], 500)
        s2 = geeface.gee2df(collection_s2, lat, lon, ['2000-01-01', '2024-01-01'], ['B4', 'B8'], 500) / 1e4
        modis = geeface.gee2df('MODIS/061/MCD43A4', lat, lon, ['2000-01-01', '2024-01-01'], ['Nadir_Reflectance_Band1', 'Nadir_Reflectance_Band2'], 500) / 1e4

        dd = {
            'Landsat5': ls5,
            'Landsat7': ls7,
            'Landsat8': ls8,
            'Sentinel2': s2,
            'MODIS': modis
        }
        ddo[site] = dd
    dump_pickle(ddo, savefile_raw_pickle)

# # -------------------------------------------------------------------------------------------------------
# # Combine two pickles
# ddo_new = load_pickle(savefile_raw_pickle)
# ddo_old = load_pickle(root_proj.joinpath('0cloud_unmasked/Satellite_surface_reflectance_NIR_R_5km.pkl'))

# ddo = {}
# for site in ddo_new.keys():
#     dd_new = ddo_new[site]
#     dd_old = ddo_old[site]
#     dd = deepcopy(dd_new)
#     dd['Sentinel2'] = dd['Sentinel2'] / 1e4 # If Sentinel-2 is not scaled
#     dd['MODIS'] = deepcopy(dd_old['MODIS'])
#     ddo[site] = dd

savefile_nc = root_proj.joinpath('Satellite_surface_reflectance_NIR_R_500m.nc')

if savefile_nc.exists():
    nc_sat_5km = xr.open_dataset(savefile_nc)
else:
    ddo = load_pickle(savefile_raw_pickle)
    dfo = []
    for site, dd in ddo.items():
        df_site = []
        for sate, dft in dd.items():
            # print(sate)
            # dft.columns = sate + '-' + dft.columns
            if not dft.empty: dft = dft.resample('1D').mean().dropna()
            dft.columns = ['R', 'NIR']
            dft['satellite'] = sate
            df_site.append(dft)
        df_site = pd.concat(df_site, axis = 0)
        # df_site = pd.concat([df_site, pd.DataFrame(index = pd.date_range('2000-01-01', '2023-12-31', freq = '1D'))], axis = 1)
        df_site = df_site.reset_index()
        df_site['ID'] = site
        dfo.append(df_site)
    dfo = pd.concat(dfo, axis = 0)
    dfo = dfo.rename(columns = {'DATETIME': 'time'})
    nco = dfo.set_index(['ID', 'satellite', 'time']).to_xarray()
    nco.R.attrs['long_name'] = "'SR_B3' for Landsat-5/-7;\n'SR_B4' for Landsat-8;\n'B4' for Sentinel-2;\n'Nadir_Reflectance_Band1' for MODIS"
    nco.NIR.attrs['long_name'] = "'SR_B4' for Landsat-5/-7;\n'SR_B5' for Landsat-8;\n'B8' for Sentinel-2;\n'Nadir_Reflectance_Band2' for MODIS"
    nco.ID.attrs['eddy_covariance'] = '212 FLUXNET2015 Tier-1 and Tier-2 eddy covariance towers'
    nco.satellite.attrs['satellite_info'] = "Interpolation resolution: 500 m;\nLandsat-5: 'LANDSAT/LT05/C02/T1_L2';\nLandsat-7: 'LANDSAT/LE07/C02/T1_L2';\nLandsat-8: 'LANDSAT/LC08/C02/T1_L2';\nSentinel-2: 'COPERNICUS/S2_SR_HARMONIZED';\nMODIS: 'MODIS/061/MCD43A4'"
    nco.time.attrs['time_info'] = '2000-01-01 to 2023-12-31 on a daily basis'
    nco.to_netcdf(savefile_nc)
    nc_sat_5km = nco; nco.close(); del(nco)

"""## High resolution satellite R and NIR reflectance"""

# Download

savefile_raw_pickle = root_proj.joinpath('Satellite_surface_reflectance_NIR_R_high-resolution.pkl')

if savefile_raw_pickle.exists():
    ddo = load_pickle(savefile_raw_pickle)
else:
    ddo = {}
    for site, row in meta.iterrows():
        if site in list(ddo.keys()): continue
        print(site)
        lat, lon = meta.loc[site, ['LAT', 'LON']]

        collection_l5 = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2').map(landsat_apply_scale_factors).map(mask_landsatsr_clouds)
        collection_l7 = ee.ImageCollection('LANDSAT/LE07/C02/T1_L2').map(landsat_apply_scale_factors).map(mask_landsatsr_clouds)
        collection_l8 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2').map(landsat_apply_scale_factors).map(mask_landsatsr_clouds)
        # collection_s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').map(mask_sentinel2sr_clouds)
        collection_s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 20))

        ls5 = geeface.gee2df(collection_l5, lat, lon, ['2000-01-01', '2024-01-01'], ['SR_B3', 'SR_B4'], 30)
        ls7 = geeface.gee2df(collection_l7, lat, lon, ['2000-01-01', '2024-01-01'], ['SR_B3', 'SR_B4'], 30)
        ls8 = geeface.gee2df(collection_l8, lat, lon, ['2000-01-01', '2024-01-01'], ['SR_B4', 'SR_B5'], 30)
        s2 = geeface.gee2df(collection_s2, lat, lon, ['2000-01-01', '2024-01-01'], ['B4', 'B8'], 10) / 1e4
        modis = geeface.gee2df('MODIS/061/MCD43A4', lat, lon, ['2000-01-01', '2024-01-01'], ['Nadir_Reflectance_Band1', 'Nadir_Reflectance_Band2'], 500) / 1e4

        dd = {
            'Landsat5': ls5,
            'Landsat7': ls7,
            'Landsat8': ls8,
            'Sentinel2': s2,
            'MODIS': modis
        }
        ddo[site] = dd
    dump_pickle(ddo, savefile_raw_pickle)

savefile_nc = root_proj.joinpath('Satellite_surface_reflectance_NIR_R_high-resolution.nc')

if savefile_nc.exists():
    nc_sat_high = xr.open_dataset(savefile_nc)
else:
    ddo = load_pickle(savefile_raw_pickle)
    dfo = []
    for site, dd in ddo.items():
        df_site = []
        for sate, dft in dd.items():
            # print(sate)
            # dft.columns = sate + '-' + dft.columns
            if not dft.empty: dft = dft.resample('1D').mean().dropna()
            dft.columns = ['R', 'NIR']
            dft['satellite'] = sate
            df_site.append(dft)
        df_site = pd.concat(df_site, axis = 0)
        # df_site = pd.concat([df_site, pd.DataFrame(index = pd.date_range('2000-01-01', '2023-12-31', freq = '1D'))], axis = 1)
        df_site = df_site.reset_index()
        df_site['ID'] = site
        dfo.append(df_site)
    dfo = pd.concat(dfo, axis = 0)
    dfo = dfo.rename(columns = {'DATETIME': 'time'})
    nco = dfo.set_index(['ID', 'satellite', 'time']).to_xarray()
    nco.R.attrs['long_name'] = "'SR_B3' for Landsat-5/-7;\n'SR_B4' for Landsat-8;\n'B4' for Sentinel-2;\n'Nadir_Reflectance_Band1' for MODIS"
    nco.NIR.attrs['long_name'] = "'SR_B4' for Landsat-5/-7;\n'SR_B5' for Landsat-8;\n'B8' for Sentinel-2;\n'Nadir_Reflectance_Band2' for MODIS"
    nco.ID.attrs['eddy_covariance'] = '212 FLUXNET2015 Tier-1 and Tier-2 eddy covariance towers'
    nco.satellite.attrs['satellite_info'] = "Interpolation resolution: 30 m for Landsat, 10 m for Sentinel-2, and 500 m for MODIS;\nLandsat-5: 'LANDSAT/LT05/C02/T1_L2';\nLandsat-7: 'LANDSAT/LE07/C02/T1_L2';\nLandsat-8: 'LANDSAT/LC08/C02/T1_L2';\nSentinel-2: 'COPERNICUS/S2_SR_HARMONIZED';\nMODIS: 'MODIS/061/MCD43A4'"
    nco.time.attrs['time_info'] = '2000-01-01 to 2023-12-31 on a daily basis'
    nco.to_netcdf(savefile_nc)
    nc_sat_high = nco; nco.close(); del(nco)

"""## ERA5"""

# See this: https://confluence.ecmwf.int/pages/viewpage.action?pageId=197702790
# For monthly data, it is averaged as J m-2 per day
# Mannually add a scaling factor of 100 (roughly correct) as the net radiation's magnitude is about 100 W m-2

for yr in np.arange(2000, 2024):
    savefile_era5_daily = root_proj.joinpath(f'0raw_data/ERA5_{yr}.pkl')
    if savefile_era5_daily.exists(): continue
    print([f'{yr}-01-01', f'{yr + 1}-01-01'])
    era5_land_daily = {}
    for site, row in meta.iterrows():
        print(site)
        lat, lon = meta.loc[site, ['LAT', 'LON']]
        era5t = geeface.gee2df('ECMWF/ERA5_LAND/DAILY_AGGR', lat, lon, [f'{yr}-01-01', f'{yr + 1}-01-01'], [
            'dewpoint_temperature_2m', 'temperature_2m', 'soil_temperature_level_1', 'snow_cover',
            'volumetric_soil_water_layer_1', 'forecast_albedo', 'surface_latent_heat_flux_sum', 'surface_sensible_heat_flux_sum',
            'surface_solar_radiation_downwards_sum', 'surface_thermal_radiation_downwards_sum',
            'evaporation_from_the_top_of_canopy_sum', 'surface_pressure', 'total_precipitation_sum',
            'temperature_2m_min', 'temperature_2m_max', 'u_component_of_wind_10m', 'v_component_of_wind_10m',
            'surface_net_solar_radiation_sum', 'surface_net_thermal_radiation_sum',
            ], 11132)
        era5_land_daily[site] = era5t
    dump_pickle(era5_land_daily, savefile_era5_daily)

savefile_era5 = root_proj.joinpath('ERA5-Land-daily.nc')
if savefile_era5.exists():
    nc_era5 = xr.open_dataset(savefile_era5)
else:
    df_path_ERA5 = []
    for p in root_proj.joinpath('0raw_data').glob('ERA5*.pkl'):
        yr = int(p.stem.split('ERA5_')[1])
        df_path_ERA5.append([yr, p])
    df_path_ERA5 = pd.DataFrame(df_path_ERA5, columns = ['YEAR', 'PATH']).set_index('YEAR').sort_index()

    era5 = []
    for p in df_path_ERA5['PATH']:
        era5t = load_pickle(p)
        era5to = []
        for site, et in era5t.items():
            et['ID'] = site
            et = et.reset_index().rename(columns = {'DATETIME': 'time'}).set_index(['ID', 'time'])
            era5to.append(et)

        era5to = pd.concat(era5to, axis = 0)
        del(era5t)
        era5.append(era5to)
    era5 = pd.concat(era5, axis = 0)

    nco = era5.to_xarray()
    nco.attrs['source'] = 'ERA5-Land Daily Aggregated - ECMWF Climate Reanalysis: https://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_LAND_DAILY_AGGR'

    nco.ID.attrs['eddy_covariance'] = '212 FLUXNET2015 Tier-1 and Tier-2 eddy covariance towers'
    nco.time.attrs['time_info'] = '2000-01-01 to 2023-12-31 on a daily basis'

    nco.dewpoint_temperature_2m.attrs['unit'] = 'K'
    nco.temperature_2m.attrs['unit'] = 'K'
    nco.soil_temperature_level_1.attrs['unit'] = 'K'

    nco.snow_cover.attrs['unit'] = '%'
    nco.volumetric_soil_water_layer_1.attrs['unit'] = 'Volume fraction'
    nco.forecast_albedo.attrs['unit'] = ''

    nco.surface_latent_heat_flux_sum.attrs['unit'] = 'J/m^2'
    nco.surface_sensible_heat_flux_sum.attrs['unit'] = 'J/m^2'
    nco.surface_solar_radiation_downwards_sum.attrs['unit'] = 'J/m^2'
    nco.surface_thermal_radiation_downwards_sum.attrs['unit'] = 'J/m^2'
    nco.surface_net_solar_radiation_sum.attrs['unit'] = 'J/m^2'
    nco.surface_net_thermal_radiation_sum.attrs['unit'] = 'J/m^2'

    nco.evaporation_from_the_top_of_canopy_sum.attrs['unit'] = 'm of water equivalent'
    nco.surface_pressure.attrs['unit'] = 'Pa'
    nco.total_precipitation_sum.attrs['unit'] = 'm'
    nco.temperature_2m_min.attrs['unit'] = 'K'
    nco.temperature_2m_max.attrs['unit'] = 'K'
    nco.u_component_of_wind_10m.attrs['unit'] = 'm/s'
    nco.v_component_of_wind_10m.attrs['unit'] = 'm/s'

    nco.to_netcdf(savefile_era5)
    nc_era5 = nco; nco.close(); del(nco)

"""# Get Grid from GEE"""

# def image_downscale(image):
#     image = image.resample('bilinear').reproject(
#         crs = 'EPSG:4326',
#         scale = 0.25 * 1e5,
#         )
#     return image

'''
Landsat-5 SR
'''
import ee
ee.Authenticate()
ee.Initialize(project='ee-zhusy93')

import numpy as np
import pandas as pd
import geopandas as gpd
from pathlib import Path
from shapely.ops import unary_union
from shapely.geometry import box


rois = [[-179.9, -89.9, 179.9, 89.9]]
dates = pd.date_range('2000-01-01', '2013-01-01', freq = '1MS') # pd.DateOffset(days = 7)
datetimes = []
for (s_, e_) in zip(dates[0: -1], dates[1::]):
    s_ = s_.strftime('%Y-%m-%d')
    e_ = e_.strftime('%Y-%m-%d')
    # print(f"['{s_}', '{e_}'],")
    datetimes.append([s_, e_])


# Applies scaling factors.
def landsat_apply_scale_factors(image):
  optical_bands = image.select('SR_B.').multiply(0.0000275).add(-0.2)
  thermal_bands = image.select('ST_B.*').multiply(0.00341802).add(149.0)
  return image.addBands(optical_bands, None, True).addBands(
      thermal_bands, None, True
  )

def mask_landsatsr_clouds(image):
    # Bits 3 and 5 are cloud shadow and cloud, respectively.
    cloudShadowBitMask = 1 << 3
    cloudsBitMask = 1 << 5
    # Get the pixel QA band.
    qa = image.select('QA_PIXEL')
    # Both flags should be set to zero, indicating clear conditions.
    mask = (
        qa.bitwiseAnd(cloudShadowBitMask)
        .eq(0)
        .And(qa.bitwiseAnd(cloudsBitMask).eq(0))
    )
    return image.updateMask(mask)

for start_date, end_date in datetimes:
    print(start_date, end_date)
    dt = pd.to_datetime(start_date, format = '%Y-%m-%d')
    year = dt.year
    month = dt.month
    day = dt.day

    for i, roi in enumerate(rois):
        minlon_ = roi[0]
        maxlon_ = roi[2]
        minlat_ = roi[1]
        maxlat_ = roi[3]

        roi = ee.Geometry.BBox(minlon_, minlat_, maxlon_, maxlat_)
        # ------------------------------------------------------------------------------
        collection = (
            ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')
            .filterBounds(roi)
            .filterDate(start_date, end_date)
            .map(landsat_apply_scale_factors)
            .map(mask_landsatsr_clouds)
        )
        image = collection.select(['SR_B3', 'SR_B4']).reduce(ee.Reducer.mean())

        task = ee.batch.Export.image.toDrive(
            image=image,
            description=start_date,
            folder='GLOL5SR_025deg_monthly',
            region=roi,
            scale=0.25 * 1e5,
            maxPixels = 1e13,
            crs='EPSG:4326'
        )
        task.start()

'''
Landsat-7 SR
'''
import ee
ee.Authenticate()
ee.Initialize(project='ee-zhusy93')

import numpy as np
import pandas as pd
import geopandas as gpd
from pathlib import Path
from shapely.ops import unary_union
from shapely.geometry import box


rois = [[-179.9, -89.9, 179.9, 89.9]]
dates = pd.date_range('2000-01-01', '2024-01-01', freq = '1MS') # pd.DateOffset(days = 7)
datetimes = []
for (s_, e_) in zip(dates[0: -1], dates[1::]):
    s_ = s_.strftime('%Y-%m-%d')
    e_ = e_.strftime('%Y-%m-%d')
    # print(f"['{s_}', '{e_}'],")
    datetimes.append([s_, e_])


# Applies scaling factors.
def landsat_apply_scale_factors(image):
  optical_bands = image.select('SR_B.').multiply(0.0000275).add(-0.2)
  thermal_bands = image.select('ST_B.*').multiply(0.00341802).add(149.0)
  return image.addBands(optical_bands, None, True).addBands(
      thermal_bands, None, True
  )

def mask_landsatsr_clouds(image):
    # Bits 3 and 5 are cloud shadow and cloud, respectively.
    cloudShadowBitMask = 1 << 3
    cloudsBitMask = 1 << 5
    # Get the pixel QA band.
    qa = image.select('QA_PIXEL')
    # Both flags should be set to zero, indicating clear conditions.
    mask = (
        qa.bitwiseAnd(cloudShadowBitMask)
        .eq(0)
        .And(qa.bitwiseAnd(cloudsBitMask).eq(0))
    )
    return image.updateMask(mask)

for start_date, end_date in datetimes:
    print(start_date, end_date)
    dt = pd.to_datetime(start_date, format = '%Y-%m-%d')
    year = dt.year
    month = dt.month
    day = dt.day

    for i, roi in enumerate(rois):
        minlon_ = roi[0]
        maxlon_ = roi[2]
        minlat_ = roi[1]
        maxlat_ = roi[3]

        roi = ee.Geometry.BBox(minlon_, minlat_, maxlon_, maxlat_)
        # ------------------------------------------------------------------------------
        collection = (
            ee.ImageCollection('LANDSAT/LE07/C02/T1_L2')
            .filterBounds(roi)
            .filterDate(start_date, end_date)
            .map(landsat_apply_scale_factors)
            .map(mask_landsatsr_clouds)
        )
        image = collection.select(['SR_B3', 'SR_B4']).reduce(ee.Reducer.mean())

        task = ee.batch.Export.image.toDrive(
            image=image,
            description=start_date,
            folder='GLOL7SR_025deg_monthly',
            region=roi,
            scale=0.25 * 1e5,
            maxPixels = 1e13,
            crs='EPSG:4326'
        )
        task.start()

'''
Landsat-8 SR
'''
import ee
ee.Authenticate()
ee.Initialize(project='ee-zhusy93')

import numpy as np
import pandas as pd
import geopandas as gpd
from pathlib import Path
from shapely.ops import unary_union
from shapely.geometry import box


rois = [[-179.9, -89.9, 179.9, 89.9]]
dates = pd.date_range('2013-01-01', '2024-01-01', freq = '1MS') # pd.DateOffset(days = 7)
datetimes = []
for (s_, e_) in zip(dates[0: -1], dates[1::]):
    s_ = s_.strftime('%Y-%m-%d')
    e_ = e_.strftime('%Y-%m-%d')
    # print(f"['{s_}', '{e_}'],")
    datetimes.append([s_, e_])


# Applies scaling factors.
def landsat_apply_scale_factors(image):
  optical_bands = image.select('SR_B.').multiply(0.0000275).add(-0.2)
  thermal_bands = image.select('ST_B.*').multiply(0.00341802).add(149.0)
  return image.addBands(optical_bands, None, True).addBands(
      thermal_bands, None, True
  )

def mask_landsatsr_clouds(image):
    # Bits 3 and 5 are cloud shadow and cloud, respectively.
    cloudShadowBitMask = 1 << 3
    cloudsBitMask = 1 << 5
    # Get the pixel QA band.
    qa = image.select('QA_PIXEL')
    # Both flags should be set to zero, indicating clear conditions.
    mask = (
        qa.bitwiseAnd(cloudShadowBitMask)
        .eq(0)
        .And(qa.bitwiseAnd(cloudsBitMask).eq(0))
    )
    return image.updateMask(mask)

for start_date, end_date in datetimes:
    print(start_date, end_date)
    dt = pd.to_datetime(start_date, format = '%Y-%m-%d')
    year = dt.year
    month = dt.month
    day = dt.day

    for i, roi in enumerate(rois):
        minlon_ = roi[0]
        maxlon_ = roi[2]
        minlat_ = roi[1]
        maxlat_ = roi[3]

        roi = ee.Geometry.BBox(minlon_, minlat_, maxlon_, maxlat_)
        # ------------------------------------------------------------------------------
        collection = (
            ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')
            .filterBounds(roi)
            .filterDate(start_date, end_date)
            .map(landsat_apply_scale_factors)
            .map(mask_landsatsr_clouds)
        )
        image = collection.select(['SR_B4', 'SR_B5']).reduce(ee.Reducer.mean())

        task = ee.batch.Export.image.toDrive(
            image=image,
            description=start_date,
            folder='GLOL8SR_025deg_monthly',
            region=roi,
            scale=0.25 * 1e5,
            maxPixels = 1e13,
            crs='EPSG:4326'
        )
        task.start()

# '''
# Sentinel-2 SR
# '''
# import ee
# ee.Authenticate()
# ee.Initialize(project='ee-zhusy93')

# import numpy as np
# import pandas as pd
# import geopandas as gpd
# from pathlib import Path
# from shapely.ops import unary_union
# from shapely.geometry import box


# rois = [[-179.9, -89.9, 179.9, 89.9]]
# dates = pd.date_range('2021-01-01', '2023-01-01', freq = '1MS') # pd.DateOffset(days = 7)
# datetimes = []
# for (s_, e_) in zip(dates[0: -1], dates[1::]):
#     s_ = s_.strftime('%Y-%m-%d')
#     e_ = e_.strftime('%Y-%m-%d')
#     # print(f"['{s_}', '{e_}'],")
#     datetimes.append([s_, e_])

# # Tutorial: https://developers.google.com/earth-engine/tutorials/community/sentinel-2-s2cloudless

# def get_s2_sr_cld_col(aoi, start_date, end_date, CLOUD_FILTER = 60):
#     # Import and filter S2 SR.
#     s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') # 'COPERNICUS/S2_SR'
#         .filterBounds(aoi)
#         .filterDate(start_date, end_date)
#         .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))

#     # Import and filter s2cloudless.
#     s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')
#         .filterBounds(aoi)
#         .filterDate(start_date, end_date))

#     # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.
#     return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{
#         'primary': s2_sr_col,
#         'secondary': s2_cloudless_col,
#         'condition': ee.Filter.equals(**{
#             'leftField': 'system:index',
#             'rightField': 'system:index'
#         })
#     }))


# def add_cloud_bands(img, CLD_PRB_THRESH = 40):
#     # Get s2cloudless image, subset the probability band.
#     cld_prb = ee.Image(img.get('s2cloudless')).select('probability')

#     # Condition s2cloudless by the probability threshold value.
#     is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')

#     # Add the cloud probability layer and cloud mask as image bands.
#     return img.addBands(ee.Image([cld_prb, is_cloud]))


# def add_shadow_bands(img, NIR_DRK_THRESH = 0.15, CLD_PRJ_DIST = 2):
#     # Identify water pixels from the SCL band.
#     not_water = img.select('SCL').neq(6)

#     # Identify dark NIR pixels that are not water (potential cloud shadow pixels).
#     SR_BAND_SCALE = 1e4
#     dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')

#     # Determine the direction to project cloud shadow from clouds (assumes UTM projection).
#     shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));

#     # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.
#     cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)
#         .reproject(**{'crs': img.select(0).projection(), 'scale': 100})
#         .select('distance')
#         .mask()
#         .rename('cloud_transform'))

#     # Identify the intersection of dark pixels with cloud shadow projection.
#     shadows = cld_proj.multiply(dark_pixels).rename('shadows')

#     # Add dark pixels, cloud projection, and identified shadows as image bands.
#     return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))

# def add_cld_shdw_mask(img, BUFFER = 100):
#     # Add cloud component bands.
#     img_cloud = add_cloud_bands(img)

#     # Add cloud shadow component bands.
#     img_cloud_shadow = add_shadow_bands(img_cloud)

#     # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.
#     is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)

#     # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.
#     # 20 m scale is for speed, and assumes clouds don't require 10 m precision.
#     is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)
#         .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})
#         .rename('cloudmask'))

#     # Add the final cloud-shadow mask to the image.
#     # return img_cloud_shadow.addBands(is_cld_shdw)
#     return img.addBands(is_cld_shdw)

# def apply_cld_shdw_mask(img):
#     # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.
#     not_cld_shdw = img.select('cloudmask').Not()

#     # Subset reflectance bands and update their masks, return the result.
#     return img.select('B.*').updateMask(not_cld_shdw)

# for start_date, end_date in datetimes:
#     print(start_date, end_date)
#     dt = pd.to_datetime(start_date, format = '%Y-%m-%d')
#     year = dt.year
#     month = dt.month
#     day = dt.day

#     for i, roi in enumerate(rois):
#         minlon_ = roi[0]
#         maxlon_ = roi[2]
#         minlat_ = roi[1]
#         maxlat_ = roi[3]

#         roi = ee.Geometry.BBox(minlon_, minlat_, maxlon_, maxlat_)
#         # ------------------------------------------------------------------------------
#         s2_sr_cld_col = get_s2_sr_cld_col(roi, start_date, end_date)
#         image = s2_sr_median = (s2_sr_cld_col.map(add_cld_shdw_mask)
#                                     .map(apply_cld_shdw_mask)
#                                     # .select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'])
#                                     .select(['B4', 'B8'])
#                                     .median())

#         task = ee.batch.Export.image.toDrive(
#             image=image,
#             description=start_date,
#             folder='GLOS2SR_025deg_monthly',
#             region=roi,
#             scale=0.25 * 1e5,
#             maxPixels = 1e13,
#             crs='EPSG:4326'
#         )
#         task.start()

'''
MODIS BRDF
'''
import ee
ee.Authenticate()
ee.Initialize(project='ee-zhusy93')

import numpy as np
import pandas as pd
import geopandas as gpd
from pathlib import Path
from shapely.ops import unary_union
from shapely.geometry import box


rois = [[-179.9, -89.9, 179.9, 89.9]]
dates = pd.date_range('2000-01-01', '2024-01-01', freq = '1MS') # pd.DateOffset(days = 7)
datetimes = []
for (s_, e_) in zip(dates[0: -1], dates[1::]):
    s_ = s_.strftime('%Y-%m-%d')
    e_ = e_.strftime('%Y-%m-%d')
    # print(f"['{s_}', '{e_}'],")
    datetimes.append([s_, e_])

for (start_date, end_date) in datetimes:
    print(start_date, end_date)
    dt = pd.to_datetime(start_date, format = '%Y-%m-%d')
    year = dt.year
    month = dt.month
    day = dt.day

    for i, roi in enumerate(rois):
        minlon_ = roi[0]
        maxlon_ = roi[2]
        minlat_ = roi[1]
        maxlat_ = roi[3]

        roi = ee.Geometry.BBox(minlon_, minlat_, maxlon_, maxlat_)
        # ------------------------------------------------------------------------------
        collection = (
            ee.ImageCollection('MODIS/061/MCD43A4')
            .filterBounds(roi)
            .filterDate(start_date, end_date)
        )
        image = collection.select(['Nadir_Reflectance_Band1', 'Nadir_Reflectance_Band2']).reduce(ee.Reducer.mean())

        task = ee.batch.Export.image.toDrive(
            image=image,
            description=start_date,
            folder='GLOMODIS_025deg_monthly',
            region=roi,
            scale=0.25 * 1e5,
            maxPixels = 1e13,
            crs='EPSG:4326'
        )
        task.start()

'''
Sentinel-2 SR
'''
import ee
ee.Authenticate()
ee.Initialize(project='ee-zhusy93')

import numpy as np
import pandas as pd
import geopandas as gpd
from pathlib import Path
from shapely.ops import unary_union
from shapely.geometry import box


rois = [[-179.9, -89.9, 179.9, 89.9]]
dates = pd.date_range('2017-01-01', '2024-01-01', freq = '1MS') # pd.DateOffset(days = 7)
datetimes = []
for (s_, e_) in zip(dates[0: -1], dates[1::]):
    s_ = s_.strftime('%Y-%m-%d')
    e_ = e_.strftime('%Y-%m-%d')
    # print(f"['{s_}', '{e_}'],")
    datetimes.append([s_, e_])


def image_downscale(image):
    image = image.resample('bilinear').reproject(
        crs = 'EPSG:4326',
        scale = 0.25 * 1e5,
        )
    return image

for start_date, end_date in datetimes:
    print(start_date, end_date)
    dt = pd.to_datetime(start_date, format = '%Y-%m-%d')
    year = dt.year
    month = dt.month
    day = dt.day

    for i, roi in enumerate(rois):
        minlon_ = roi[0]
        maxlon_ = roi[2]
        minlat_ = roi[1]
        maxlat_ = roi[3]

        roi = ee.Geometry.BBox(minlon_, minlat_, maxlon_, maxlat_)
        # ------------------------------------------------------------------------------
        collection = (
            ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
            .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 20))
            .filterBounds(roi)
            .filterDate(start_date, end_date)
        )
        image = collection.select(['B4', 'B8']).reduce(ee.Reducer.mean())

        task = ee.batch.Export.image.toDrive(
            image=image,
            description=start_date,
            folder='GLOS2SR_025deg_monthly',
            region=roi,
            scale=0.25 * 1e5,
            maxPixels = 1e13,
            crs='EPSG:4326'
        )
        task.start()

'''
ERA5-Land daily aggregates
'''
# See this: https://confluence.ecmwf.int/pages/viewpage.action?pageId=197702790
# For monthly data, it is averaged as J m-2 per day
# Mannually add a scaling factor of 100 (roughly correct) as the net radiation's magnitude is about 100 W m-2

import ee
ee.Authenticate()
ee.Initialize(project='ee-zhusy93')

import numpy as np
import pandas as pd
import geopandas as gpd
from pathlib import Path
from shapely.ops import unary_union
from shapely.geometry import box


rois = [[-179.9, -89.9, 179.9, 89.9]]
dates = pd.date_range('2000-01-01', '2024-01-01', freq = '1MS') # pd.DateOffset(days = 7)
datetimes = []
for (s_, e_) in zip(dates[0: -1], dates[1::]):
    s_ = s_.strftime('%Y-%m-%d')
    e_ = e_.strftime('%Y-%m-%d')
    # print(f"['{s_}', '{e_}'],")
    datetimes.append([s_, e_])

for (start_date, end_date) in datetimes:
    print(start_date, end_date)
    dt = pd.to_datetime(start_date, format = '%Y-%m-%d')
    year = dt.year
    month = dt.month
    day = dt.day

    for i, roi in enumerate(rois):
        minlon_ = roi[0]
        maxlon_ = roi[2]
        minlat_ = roi[1]
        maxlat_ = roi[3]

        roi = ee.Geometry.BBox(minlon_, minlat_, maxlon_, maxlat_)
        # ------------------------------------------------------------------------------
        collection = (
            ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR')
            .filterBounds(roi)
            .filterDate(start_date, end_date)
        )
        image = collection.select([
            'dewpoint_temperature_2m', 'temperature_2m', 'soil_temperature_level_1', 'snow_cover',
            'volumetric_soil_water_layer_1', 'forecast_albedo', 'surface_latent_heat_flux_sum', 'surface_sensible_heat_flux_sum',
            'surface_solar_radiation_downwards_sum', 'surface_thermal_radiation_downwards_sum',
            'evaporation_from_the_top_of_canopy_sum', 'surface_pressure', 'total_precipitation_sum',
            'temperature_2m_min', 'temperature_2m_max', 'u_component_of_wind_10m', 'v_component_of_wind_10m'
        ]).reduce(ee.Reducer.mean())

        task = ee.batch.Export.image.toDrive(
            image=image,
            description=start_date,
            folder='GLOERA5_025deg_monthly',
            region=roi,
            scale=0.25 * 1e5,
            maxPixels = 1e13,
            crs='EPSG:4326'
        )
        task.start()

'''
MODIS LUCC
'''
import ee
ee.Authenticate()
ee.Initialize(project='ee-zhusy93')

import numpy as np
import pandas as pd
import geopandas as gpd
from pathlib import Path
from shapely.ops import unary_union
from shapely.geometry import box


rois = [[-179.9, -89.9, 179.9, 89.9]]
dates = pd.date_range('2000-01-01', '2024-01-01', freq = '1YS') # pd.DateOffset(days = 7)
datetimes = []
for (s_, e_) in zip(dates[0: -1], dates[1::]):
    s_ = s_.strftime('%Y-%m-%d')
    e_ = e_.strftime('%Y-%m-%d')
    # print(f"['{s_}', '{e_}'],")
    datetimes.append([s_, e_])

for (start_date, end_date) in datetimes:
    print(start_date, end_date)
    dt = pd.to_datetime(start_date, format = '%Y-%m-%d')
    year = dt.year
    month = dt.month
    day = dt.day

    for i, roi in enumerate(rois):
        minlon_ = roi[0]
        maxlon_ = roi[2]
        minlat_ = roi[1]
        maxlat_ = roi[3]

        roi = ee.Geometry.BBox(minlon_, minlat_, maxlon_, maxlat_)
        # ------------------------------------------------------------------------------
        collection = (
            ee.ImageCollection('MODIS/061/MCD12Q1')
            .filterBounds(roi)
            .filterDate(start_date, end_date)
        )
        image = collection.select(['LC_Type1']).reduce(ee.Reducer.mean())

        task = ee.batch.Export.image.toDrive(
            image=image,
            description=start_date,
            folder='GLOMODISLUCC_025deg_YEARLYbetter',
            region=roi,
            scale=0.25 * 1e5,
            maxPixels = 1e13,
            crs='EPSG:4326'
        )
        task.start()

from scigee import geeface_lite

scale = 0.25 * 1e5 # 0.25 degree
start_date, end_date = ['2022-01-01', '2022-03-01']
# roi = [-179.9, -89.9, 179.9, 89.9] # minlon, minlat, maxlon, maxlat
roi = [-3, 50, 3, 52]

roi = ee.Geometry.BBox(*roi)
collection = (
    ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
    .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 20))
    .filterBounds(roi)
    .filterDate(start_date, end_date)
)
image = collection.select(['B4', 'B8']).reduce(ee.Reducer.mean())

df = geeface_lite.region2arrayC(image, roi, ['B4_mean', 'B8_mean'], scale, max_pixels = 1e50, mask_value = -9999)
df = df.replace(-9999, np.nan).dropna()
old_colnames = list(df.columns.drop(['latitude', 'longitude']))
new_colnames = [n.split('_mean')[0] for n in old_colnames]
df = df.rename(columns = dict(zip(old_colnames, new_colnames)))
nc = df.set_index(['latitude', 'longitude']).to_xarray()
nc

'''
ERA5-Land daily aggregates UK Jingya
'''
# See this: https://confluence.ecmwf.int/pages/viewpage.action?pageId=197702790
# For monthly data, it is averaged as J m-2 per day
# Mannually add a scaling factor of 100 (roughly correct) as the net radiation's magnitude is about 100 W m-2

import ee
ee.Authenticate()
ee.Initialize(project='ee-zhusy93')

import numpy as np
import pandas as pd
import geopandas as gpd
from pathlib import Path
from shapely.ops import unary_union
from shapely.geometry import box


rois = [[-7.64133, 50.10319, 1.75159, 60.15456]]
dates = pd.date_range('2002-08-01', '2024-01-01', freq = '1D') # pd.DateOffset(days = 7)
datetimes = []
for (s_, e_) in zip(dates[0: -1], dates[1::]):
    s_ = s_.strftime('%Y-%m-%d')
    e_ = e_.strftime('%Y-%m-%d')
    # print(f"['{s_}', '{e_}'],")
    datetimes.append([s_, e_])

for (start_date, end_date) in datetimes:
    print(start_date, end_date)
    dt = pd.to_datetime(start_date, format = '%Y-%m-%d')
    year = dt.year
    month = dt.month
    day = dt.day

    for i, roi in enumerate(rois):
        minlon_ = roi[0]
        maxlon_ = roi[2]
        minlat_ = roi[1]
        maxlat_ = roi[3]

        roi = ee.Geometry.BBox(minlon_, minlat_, maxlon_, maxlat_)
        # ------------------------------------------------------------------------------
        collection = (
            ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR')
            .filterBounds(roi)
            .filterDate(start_date, end_date)
        )
        image = collection.select([
            'dewpoint_temperature_2m', 'temperature_2m',
            'surface_solar_radiation_downwards_sum', 'total_precipitation_sum',
            'u_component_of_wind_10m', 'v_component_of_wind_10m'
        ]).reduce(ee.Reducer.mean())

        task = ee.batch.Export.image.toDrive(
            image=image,
            description=start_date,
            folder='UK_Jingya',
            region=roi,
            scale=0.1 * 1e5,
            maxPixels = 1e13,
            crs='EPSG:4326'
        )
        task.start()

"""# Resolution effect on GPP estimation

## Function
"""

def get_4VIs(data, r_name, nir_name, as_one = True):
    r = data[r_name]
    nir = data[nir_name]
    ndvi = get_NDVI(r, nir)
    evi = get_EVI2band(r, nir)
    nirv = get_NIRv(ndvi, nir)
    kndvi = get_kNDVI(ndvi)

    if as_one:
        ndvi.name = 'NDVI'
        evi.name = 'EVI'
        nirv.name = 'NIRv'
        kndvi.name = 'kNDVI'
        ndvi = ndvi.to_dataset()
        evi = evi.to_dataset()
        nirv = nirv.to_dataset()
        kndvi = kndvi.to_dataset()
        return xr.merge([ndvi, evi, nirv, kndvi])
    else:
        return (ndvi, evi, nirv, kndvi)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

def run_ml(X_train, X_test, y_train, y_test):
    xgb_params = {
        "objective": "reg:squarederror",
        "random_state": 0,
        'seed': 0,
    }

    regr = XGBRegressor(**xgb_params)
    regr.fit(X_train, y_train)
    y_pred = regr.predict(X_test)
    # print(regr.score(X_train, y_train))
    # print(regr.score(X_test, y_test))

    dfo = y_test.copy()#.to_frame()
    dfo.columns = ['truth']
    dfo['pred'] = y_pred
    # dfo = dfo / np.power(10, vexp)
    # dfo.index = dfo.index.droplevel([0, 1])

    # eval_res = regress2(dfo['truth'].values, dfo['pred'].values)
    # rvalue = eval_res['r']
    # slope = eval_res['slope']
    # intercept = eval_res['intercept']
    eval_res = pipelines.get_metrics(dfo, truth = 'truth', pred = 'pred', return_dict = True)
    # print(eval_res)
    return regr, dfo, eval_res

def agg_global_GPP_025deg(nct, world, name = 'GPP'):
    coef = 365 * 0.25 * 0.25 * 1e5 * 1e5 / 1e15
    nct = nct.where(nct[name] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
    dft = nct[name].sum(dim = ['latitude', 'longitude']).drop_vars('spatial_ref').to_dataframe() * coef
    return dft

def load_GPP(name, foldername, world):
    dfo = []
    for p in foldername.glob(f'{name}*.nc'):
        print(p.stem)
        nct = xr.open_dataset(p)
        dft = agg_global_GPP_025deg(nct, world)
        dfo.append(dft)
        nct.close(); del(nct)
    dfo = pd.concat(dfo, axis = 0)
    return dfo

def get_grid_NIRv(p, FAPAR_ok = False, reproj_ok = False):
    rnc = rxr.open_rasterio(p, band_as_variable = True)
    if reproj_ok:
        rnc = rnc.rio.reproject("EPSG:4326")
    assert (rnc.rio.crs == 'EPSG:4326'), rnc.rio.crs
    name_dict = dict(zip(rnc.keys(), ['R', 'NIR']))
    name_dict.update({'x': 'longitude', 'y': 'latitude'})
    rnc = rnc.rename(name_dict)
    R = rnc['R']
    NIR = rnc['NIR']
    if np.nanmean(NIR.data) > 100:
        NIR = NIR / 10000
        R = R / 10000
    NIRv = (NIR - R) / (NIR + R) * NIR
    NIRv.name = 'NIRv'
    if FAPAR_ok:
        EVI = 2.5 * (NIR - R) / (NIR + 2.4 * R + 1)
        FAPAR = (EVI - 0.1 + 0.1) * 1.25
        FAPAR.name = 'FAPAR'
        return NIRv, FAPAR
    else:
        return NIRv

def get_Amazon_patch(nct, minx = -35.2, miny = -9, maxx = -35, maxy = -6):
    cond = (nct.longitude > minx) & (nct.longitude < maxx) & (nct.latitude > miny) & (nct.latitude < maxy)
    return ~cond

"""## Load VI data"""

# Load reflectance

nc_sat_5km = xr.open_dataset(root_proj.parent.joinpath('platform').joinpath('0tower_level/Satellite_surface_reflectance_NIR_R_5km.nc'))
nc_sat_500m = xr.open_dataset(root_proj.parent.joinpath('platform').joinpath('0tower_level/Satellite_surface_reflectance_NIR_R_500m.nc'))
nc_sat_high = xr.open_dataset(root_proj.parent.joinpath('platform').joinpath('0tower_level/Satellite_surface_reflectance_NIR_R_high-resolution.nc'))
nc_era5 = xr.open_dataset(root_proj.parent.joinpath('platform').joinpath('0tower_level/ERA5-Land-daily.nc'))

# # ----------------------------------------------------------------------------
# Calibrate Landsat

# Harmonise Landsat satellites
# https://developers.google.com/earth-engine/tutorials/community/landsat-etm-to-oli-harmonization
# https://openprairie.sdstate.edu/cgi/viewcontent.cgi?referer=https://scholar.google.com/&httpsredir=1&article=1035&context=gsce_pubs

# Red: OLI = 0.0061 + 0.9047 ETM+
# NIR: OLI = 0.0412 + 0.8462 ETM+

dft = nc_sat_5km.to_dataframe()
index = dft.index.get_level_values(2)
dft.loc[index == 'Landsat5', 'R'] = dft.loc[index == 'Landsat5', 'R'] * 0.9047 + 0.0061
dft.loc[index == 'Landsat5', 'NIR'] = dft.loc[index == 'Landsat5', 'NIR'] * 0.8462 + 0.0412
dft.loc[index == 'Landsat7', 'R'] = dft.loc[index == 'Landsat7', 'R'] * 0.9047 + 0.0061
dft.loc[index == 'Landsat7', 'NIR'] = dft.loc[index == 'Landsat7', 'NIR'] * 0.8462 + 0.0412
nc_sat_5km = dft.to_xarray()

dft = nc_sat_500m.to_dataframe()
index = dft.index.get_level_values(2)
dft.loc[index == 'Landsat5', 'R'] = dft.loc[index == 'Landsat5', 'R'] * 0.9047 + 0.0061
dft.loc[index == 'Landsat5', 'NIR'] = dft.loc[index == 'Landsat5', 'NIR'] * 0.8462 + 0.0412
dft.loc[index == 'Landsat7', 'R'] = dft.loc[index == 'Landsat7', 'R'] * 0.9047 + 0.0061
dft.loc[index == 'Landsat7', 'NIR'] = dft.loc[index == 'Landsat7', 'NIR'] * 0.8462 + 0.0412
nc_sat_500m = dft.to_xarray()

dft = nc_sat_high.to_dataframe()
index = dft.index.get_level_values(2)
dft.loc[index == 'Landsat5', 'R'] = dft.loc[index == 'Landsat5', 'R'] * 0.9047 + 0.0061
dft.loc[index == 'Landsat5', 'NIR'] = dft.loc[index == 'Landsat5', 'NIR'] * 0.8462 + 0.0412
dft.loc[index == 'Landsat7', 'R'] = dft.loc[index == 'Landsat7', 'R'] * 0.9047 + 0.0061
dft.loc[index == 'Landsat7', 'NIR'] = dft.loc[index == 'Landsat7', 'NIR'] * 0.8462 + 0.0412
nc_sat_high = dft.to_xarray()

# # ----------------------------------------------------------------------------
# Calculate VI

# ndvi_5km, evi_5km, nirv_5km, kndvi_5km = get_4VIs(nc_sat_5km, 'R', 'NIR', as_one = False)
# ndvi_high, evi_high, nirv_high, kndvi_high = get_4VIs(nc_sat_high, 'R', 'NIR', as_one = False)
vi_5km = get_4VIs(nc_sat_5km, 'R', 'NIR')
vi_500m = get_4VIs(nc_sat_500m, 'R', 'NIR')
vi_high = get_4VIs(nc_sat_high, 'R', 'NIR')

# # ----------------------------------------------------------------------------

pd.concat([
    vi_5km.where(vi_5km['satellite'] != 'MODIS', np.nan)['NIRv'].mean(dim = ['time', 'satellite']).to_dataframe()['NIRv'].rename('5km'),
    vi_500m.where(vi_5km['satellite'] != 'MODIS', np.nan)['NIRv'].mean(dim = ['time', 'satellite']).to_dataframe()['NIRv'].rename('500m'),
    vi_high.where(vi_5km['satellite'] != 'MODIS', np.nan)['NIRv'].mean(dim = ['time', 'satellite']).to_dataframe()['NIRv'].rename('30m'),
    meta['IGBP']
], axis = 1).groupby('IGBP').mean()

pd.concat([
    vi_5km.where(vi_5km['satellite'] != 'MODIS', np.nan)['NIRv'].mean(dim = ['time', 'satellite']).to_dataframe()['NIRv'].rename('5km'),
    vi_500m.where(vi_5km['satellite'] != 'MODIS', np.nan)['NIRv'].mean(dim = ['time', 'satellite']).to_dataframe()['NIRv'].rename('500m'),
    vi_high.where(vi_5km['satellite'] != 'MODIS', np.nan)['NIRv'].mean(dim = ['time', 'satellite']).to_dataframe()['NIRv'].rename('30m'),
], axis = 1).mean().to_frame().T

# @title Compare when considering footprint and non-additive VI

from scigee.utils import footprint_size, harmonise_ETM, add_ndvi, add_nirv, add_kndvi, add_evi2, radiometric_calibration, mask_landsatsr_clouds, landsat_apply_scale_factors
# Download

site = 'DE-Gri'
lat, lon = meta.loc[site, ['LAT', 'LON']]
igbp = meta.loc[site, 'IGBP']

collection_l5 = (
    ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')
    .map(landsat_apply_scale_factors)
    .map(mask_landsatsr_clouds)
    .map(harmonise_ETM)
    .map(lambda img: add_ndvi(img, red_band='SR_B3_har', nir_band='SR_B4_har', output_band='NDVI'))
    .map(lambda img: add_nirv(img, red_band='SR_B3_har', nir_band='SR_B4_har', output_band='NIRv'))
    .map(lambda img: add_kndvi(img, red_band='SR_B3_har', nir_band='SR_B4_har', output_band='kNDVI'))
    .map(lambda img: add_evi2(img, red_band='SR_B3_har', nir_band='SR_B4_har', output_band='EVI2'))
)

collection_l7 = (
    ee.ImageCollection('LANDSAT/LE07/C02/T1_L2')
    .map(landsat_apply_scale_factors)
    .map(mask_landsatsr_clouds)
    .map(harmonise_ETM)
    .map(lambda img: add_ndvi(img, red_band='SR_B3_har', nir_band='SR_B4_har', output_band='NDVI'))
    .map(lambda img: add_nirv(img, red_band='SR_B3_har', nir_band='SR_B4_har', output_band='NIRv'))
    .map(lambda img: add_kndvi(img, red_band='SR_B3_har', nir_band='SR_B4_har', output_band='kNDVI'))
    .map(lambda img: add_evi2(img, red_band='SR_B3_har', nir_band='SR_B4_har', output_band='EVI2'))
)

collection_l8 = (
    ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')
    .map(landsat_apply_scale_factors)
    .map(mask_landsatsr_clouds)
    .map(lambda img: add_ndvi(img, red_band='SR_B4', nir_band='SR_B5', output_band='NDVI'))
    .map(lambda img: add_nirv(img, red_band='SR_B4', nir_band='SR_B5', output_band='NIRv'))
    .map(lambda img: add_kndvi(img, red_band='SR_B4', nir_band='SR_B5', output_band='kNDVI'))
    .map(lambda img: add_evi2(img, red_band='SR_B4', nir_band='SR_B5', output_band='EVI2'))
)

collection_s2 = (
    ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
    .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 20))
    .map(lambda img: radiometric_calibration(img, input_band='B4', output_band='B4_cal'))
    .map(lambda img: radiometric_calibration(img, input_band='B8', output_band='B8_cal'))
    .map(lambda img: add_ndvi(img, red_band='B4_cal', nir_band='B8_cal', output_band='NDVI'))
    .map(lambda img: add_nirv(img, red_band='B4_cal', nir_band='B8_cal', output_band='NIRv'))
    .map(lambda img: add_kndvi(img, red_band='B4_cal', nir_band='B8_cal', output_band='kNDVI'))
    .map(lambda img: add_evi2(img, red_band='B4_cal', nir_band='B8_cal', output_band='EVI2'))
)

collection_modis = (
    ee.ImageCollection('MODIS/061/MCD43A4')
    .map(lambda img: radiometric_calibration(img, input_band='Nadir_Reflectance_Band1', output_band='B1_cal'))
    .map(lambda img: radiometric_calibration(img, input_band='Nadir_Reflectance_Band2', output_band='B2_cal'))
    .map(lambda img: add_ndvi(img, red_band='B1_cal', nir_band='B2_cal', output_band='NDVI'))
    .map(lambda img: add_nirv(img, red_band='B1_cal', nir_band='B2_cal', output_band='NIRv'))
    .map(lambda img: add_kndvi(img, red_band='B1_cal', nir_band='B2_cal', output_band='kNDVI'))
    .map(lambda img: add_evi2(img, red_band='B1_cal', nir_band='B2_cal', output_band='EVI2'))
)

ls5 = geeface.collection2ts(collection_l5, [lat, lon], ['2000-01-01', '2024-01-01'], ['NDVI', 'NIRv', 'kNDVI', 'EVI2'], 30, radius = footprint_size[igbp] * 2 / 1e5).resample('1D').mean().dropna(how = 'all')
ls7 = geeface.collection2ts(collection_l7, [lat, lon], ['2000-01-01', '2024-01-01'], ['NDVI', 'NIRv', 'kNDVI', 'EVI2'], 30, radius = footprint_size[igbp] * 2 / 1e5).resample('1D').mean().dropna(how = 'all')
ls8 = geeface.collection2ts(collection_l8, [lat, lon], ['2000-01-01', '2024-01-01'], ['NDVI', 'NIRv', 'kNDVI', 'EVI2'], 30, radius = footprint_size[igbp] * 2 / 1e5).resample('1D').mean().dropna(how = 'all')
s2 = geeface.collection2ts(collection_s2, [lat, lon], ['2000-01-01', '2024-01-01'], ['NDVI', 'NIRv', 'kNDVI', 'EVI2'], 30, radius = footprint_size[igbp] * 2 / 1e5).resample('1D').mean().dropna(how = 'all')
modis = geeface.collection2ts(collection_modis, [lat, lon], ['2000-01-01', '2024-01-01'], ['NDVI', 'NIRv', 'kNDVI', 'EVI2'], 30, radius = footprint_size[igbp] * 2 / 1e5).resample('1D').mean().dropna(how = 'all')

dd_footprint = {
    'Landsat5': ls5,
    'Landsat7': ls7,
    'Landsat8': ls8,
    'Sentinel2': s2,
    'MODIS': modis
}

# @title plot the comparison with vi_high

satellite = 'Landsat7'
vi_name = 'NIRv'

df_non_additive = vi_high.sel(ID = site).sel(satellite = satellite).drop_vars(['ID', 'satellite']).to_dataframe()[vi_name].dropna(how = 'all')
df_additive = dd_footprint[satellite][vi_name].dropna(how = 'all')
dfp = pd.concat([df_non_additive.rename('non_additive'), df_additive.rename('additive')], axis = 1)

vmin = dfp.min().min()
vmax = dfp.max().max()
fig, ax = setup_canvas(1, 1, figsize = (5, 5), fontsize = 10)
dfp.plot.scatter(ax = ax, x = 'non_additive', y = 'additive')

ax.plot([vmin, vmax], [vmin, vmax], 'r--')
stats.linregress(dfp.dropna()['non_additive'], dfp.dropna()['additive'])

"""## Compare VIs"""

dft = []
for p in root_proj.joinpath('1grid_NIRv_yearly').glob('MODIS*.nc'):
    v_ = xr.open_dataset(p)['NIRv'].mean()
    dt = pd.to_datetime(p.stem.split('_')[1])
    dft.append([float(v_.data), dt])
dft = pd.DataFrame(dft, columns = ['NIRv', 'time']).set_index('time')

float(dft.mean().values) / float(vi_5km['NIRv'].sel(satellite = 'MODIS').mean().data)

nct = xr.open_dataset(root_proj.joinpath('0tower_level/training_dataset.nc'))
dft = pd.concat([nct['GPP_NT_VUT_REF'].to_dataframe(), nct['NIRv_high_MODIS'].to_dataframe()], axis = 1).dropna()
dft.columns = ['GPP', 'VI']
print(stats.linregress(dft['GPP'], dft['VI']).rvalue)
dft = pd.concat([nct['GPP_NT_VUT_REF'].to_dataframe(), nct['NDVI_high_MODIS'].to_dataframe()], axis = 1).dropna()
dft.columns = ['GPP', 'VI']
print(stats.linregress(dft['GPP'], dft['VI']).rvalue)
dft = pd.concat([nct['GPP_NT_VUT_REF'].to_dataframe(), nct['EVI_high_MODIS'].to_dataframe()], axis = 1).dropna()
dft.columns = ['GPP', 'VI']
print(stats.linregress(dft['GPP'], dft['VI']).rvalue)
dft = pd.concat([nct['GPP_NT_VUT_REF'].to_dataframe(), nct['kNDVI_high_MODIS'].to_dataframe()], axis = 1).dropna()
dft.columns = ['GPP', 'VI']
print(stats.linregress(dft['GPP'], dft['VI']).rvalue)
dft = pd.concat([nct['GPP_NT_VUT_REF'].to_dataframe(), nct['NIRv_5km_MODIS'].to_dataframe()], axis = 1).dropna()
dft.columns = ['GPP', 'VI']
print(stats.linregress(dft['GPP'], dft['VI']).rvalue)

0.7807819081766786 / 0.6676588143406981, 0.7807819081766786 / 0.7716920919023936, 0.7807819081766786 / 0.7069281389833114, 0.7807819081766786 / 0.7467632868412226

def all_res_1sat1vi1site(data_low, data_hgih, sat, vi, site):
    dft_low = data_low.sel(satellite = sat, ID = site)[vi].to_dataframe()[vi].reset_index().set_index('time').rename(columns = {vi: 'Low'})
    dft_high = data_hgih.sel(satellite = sat, ID = site)[vi].to_dataframe()[vi].reset_index().set_index('time').rename(columns = {vi: 'High'})
    return pd.concat([dft_low, dft_high], axis = 1)

df_res = []
for site in tqdm(meta.index):
    for sat in ['Landsat5', 'Landsat7', 'Landsat8', 'MODIS', 'Sentinel2']:
        for vi in ['NDVI', 'EVI', 'NIRv', 'kNDVI']:
            dfp = all_res_1sat1vi1site(vi_5km, vi_high, sat, vi, site).dropna().resample('1YS').mean()
            if dfp.empty: continue
            res = pipelines.get_metrics(dfp, truth = 'Low', pred = 'High', return_dict = True)
            res['ID'] = site
            res['Satellite'] = sat
            res['VI'] = vi
            df_res.append(res)
df_res = pd.concat(df_res, axis = 0)
df_res['IGBP'] = meta.loc[df_res['ID'], 'IGBP'].values
df_highlow = df_res.copy()
df_res

df_res = df_highlow.copy()
df_res['Satellite'].replace({'Landsat5': 'Landsat-5', 'Landsat7': 'Landsat-7', 'Landsat8': 'Landsat-8', 'Sentinel2': 'Sentinel-2'}, inplace=True)
df_res.rename(columns = {'R2': '$R^2$'}, inplace=True)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 12)
dfp = df_res[['$R^2$', 'Slope', 'RMSE', 'MBE', 'MAE', 'Satellite', 'VI', 'ID']].groupby(['Satellite', 'VI', 'ID']).mean().reset_index()
dfp.boxplot(column=['$R^2$'], by=['Satellite', 'VI'], rot= 90, fontsize = 10, ax = ax)
ax.set_ylim(0, 1)
fig.suptitle('')
google.download_file(fig, 'VI_highVSlow.png')

# ===================================================================================================================================
# dfp = df_res[df_res['Satellite'] == 'Sentinel2']
dfp = df_res[['$R^2$', 'Slope', 'RMSE', 'MBE', 'MAE', 'Satellite', 'VI', 'ID', 'IGBP']]
fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 12)
dfp.boxplot(column=['$R^2$'], by=['IGBP'], rot= 45, fontsize = 10, ax = ax)
fig.suptitle('')
google.download_file(fig, 'VI_highVSlow-IGBP.png')
# ax.set_ylim(0, 5)

def all_sat_1vi1site(data, vi, site):
    dft = data.sel(ID = site)[vi].to_dataframe().drop(['ID'], axis = 1).reset_index().pivot(index = 'time', columns = 'satellite')
    dft.columns = dft.columns.get_level_values(1)
    return dft

df_res = []
for site in tqdm(meta.index):
    for vi in ['NDVI', 'EVI', 'NIRv', 'kNDVI']:
        dfp = all_sat_1vi1site(vi_5km, vi, site)
        for sat in ['Landsat5', 'Landsat7', 'Landsat8', 'Sentinel2']:
            dft = dfp.copy()[['MODIS', sat]].dropna().resample('1YS').mean()
            if dft.empty: continue
            res = pipelines.get_metrics(dft, truth = 'MODIS', pred = sat, return_dict = True)
            res['ID'] = site
            res['Satellite'] = sat
            res['VI'] = vi
            df_res.append(res)
df_res = pd.concat(df_res, axis = 0)
df_res['IGBP'] = meta.loc[df_res['ID'], 'IGBP'].values
df_2modis = df_res.copy()
df_res

df_res = df_2modis.copy()
df_res['Satellite'].replace({'Landsat5': 'Landsat-5', 'Landsat7': 'Landsat-7', 'Landsat8': 'Landsat-8', 'Sentinel2': 'Sentinel-2'}, inplace=True)
df_res.rename(columns = {'R2': '$R^2$'}, inplace=True)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 12)
dfp = df_res[['$R^2$', 'Slope', 'RMSE', 'MBE', 'MAE', 'Satellite', 'VI', 'ID']].groupby(['Satellite', 'VI', 'ID']).mean().reset_index()
dfp.boxplot(column=['$R^2$'], by=['Satellite', 'VI'], rot= 90, fontsize = 10, ax = ax)
ax.set_ylim(0, 1)
fig.suptitle('')
google.download_file(fig, 'VI_satVSMODIS.png')

# ===================================================================================================================================
# dfp = df_res[df_res['Satellite'] == 'Sentinel2']
dfp = df_res[['$R^2$', 'Slope', 'RMSE', 'MBE', 'MAE', 'Satellite', 'VI', 'ID', 'IGBP']]
fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 12)
dfp.boxplot(column=['$R^2$'], by=['IGBP'], rot= 45, fontsize = 10, ax = ax)
fig.suptitle('')
google.download_file(fig, 'VI_satVSMODIS-IGBP.png')
# ax.set_ylim(0, 5)

def all_vi_1sat1site(data, sat, site):
    return data.sel(satellite = sat, ID = site).to_dataframe().drop(['ID', 'satellite'], axis = 1)

df_res = []
for site in tqdm(meta.index):
    for sat in ['Landsat5', 'Landsat7', 'Landsat8', 'MODIS', 'Sentinel2']:
        dfp = all_vi_1sat1site(vi_high, sat, site).dropna().resample('1YS').mean()
        if dfp.empty: continue
        for vi in ['EVI', 'NIRv', 'kNDVI']:
            res = pipelines.get_metrics(dfp, truth = 'NDVI', pred = vi, return_dict = True)
            res['ID'] = site
            res['Satellite'] = sat
            res['VI'] = vi
            df_res.append(res)
df_res = pd.concat(df_res, axis = 0)
df_res['IGBP'] = meta.loc[df_res['ID'], 'IGBP'].values
df_vi2nd = df_res.copy()
df_res

df_res = df_vi2nd.copy()
df_res['Satellite'].replace({'Landsat5': 'Landsat-5', 'Landsat7': 'Landsat-7', 'Landsat8': 'Landsat-8', 'Sentinel2': 'Sentinel-2'}, inplace=True)
df_res.rename(columns = {'R2': '$R^2$'}, inplace=True)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 12)
dfp = df_res[['$R^2$', 'Slope', 'RMSE', 'MBE', 'MAE', 'Satellite', 'VI', 'ID']].groupby(['Satellite', 'VI', 'ID']).mean().reset_index()
dfp.boxplot(column=['$R^2$'], by=['Satellite', 'VI'], rot= 90, fontsize = 10, ax = ax)
ax.set_ylim(0, 1)
fig.suptitle('')
google.download_file(fig, 'VI_VIsVSNDVI.png')

# ===================================================================================================================================
# dfp = df_res[df_res['Satellite'] == 'Sentinel2']
dfp = df_res[['$R^2$', 'Slope', 'RMSE', 'MBE', 'MAE', 'Satellite', 'VI', 'ID', 'IGBP']]
fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 12)
dfp.boxplot(column=['$R^2$'], by=['IGBP'], rot= 45, fontsize = 10, ax = ax)
fig.suptitle('')
google.download_file(fig, 'VI_VIsVSNDVI-IGBP.png')
# ax.set_ylim(0, 5)

def upper_legend(ax, xloc = 0.5, yloc = 1.1, ncols = None, nrows = None, user_labels = [], loc = "upper center", framealpha = 0., frameon = False):
    def reorder(list_in, nrows):
        ncols = len(list_in) // nrows
        if nrows * ncols != len(list_in): ncols += 1
        list_out = []
        for c in range(ncols):
            for r in range(nrows):
                if r * ncols + c >= len(list_in): continue
                list_out.append(list_in[r * ncols + c])
        assert len(list_in) == len(list_out), 'ERROR: len in != len out'
        return list_out, ncols
    handles, labels = ax.get_legend_handles_labels()
    if user_labels: labels = user_labels
    if len(handles) != len(labels): print('WARNING: the lengths are unequal')
    if nrows:
        labels, ncols = reorder(labels, nrows)
        handles, ncols = reorder(handles, nrows)
    by_label = dict(zip(labels, handles))
    if not ncols: ncols = len(labels)

    x0 = ax.get_position().x0
    x1 = ax.get_position().x1
    y0 = ax.get_position().y0
    y1 = ax.get_position().y1

    if xloc and yloc:
        ax.legend(by_label.values(), by_label.keys(), loc = loc, framealpha = framealpha, frameon = frameon, bbox_to_anchor=(xloc, yloc), ncol=ncols)
    else:
        ax.legend(by_label.values(), by_label.keys(), loc = loc, framealpha = framealpha, frameon = frameon, ncol=ncols, bbox_to_anchor=(0., -0.05, 1., 0.), borderaxespad=0, mode='expand')
    return ax

df_avg_5km = vi_5km.mean(dim = 'ID').to_dataframe().mean(axis = 1).reset_index().pivot(index = 'time', columns = 'satellite').resample('1YS').mean()
df_avg_5km.columns = df_avg_5km.columns.get_level_values(1)
df_avg_5km = df_avg_5km[['MODIS', 'Landsat5', 'Landsat7', 'Landsat8', 'Sentinel2']]
df_avg_5km.loc[df_avg_5km.index.year < 2015, 'Landsat8'] = np.nan
df_avg_5km.loc[df_avg_5km.index.year < 2019, 'Sentinel2'] = np.nan
df_avg_5km = df_avg_5km.rename(columns = {'MODIS': 'MODIS (coarse)', 'Landsat5': 'Landsat-5 (coarse)', 'Landsat7': 'Landsat-7 (coarse)', 'Landsat8': 'Landsat-8 (coarse)', 'Sentinel2': 'Sentinel-2 (coarse)'})

df_avg_high = vi_high.mean(dim = 'ID').to_dataframe().mean(axis = 1).reset_index().pivot(index = 'time', columns = 'satellite').resample('1YS').mean()
df_avg_high.columns = df_avg_high.columns.get_level_values(1)
df_avg_high = df_avg_high[['MODIS', 'Landsat5', 'Landsat7', 'Landsat8', 'Sentinel2']]
df_avg_high.loc[df_avg_high.index.year < 2015, 'Landsat8'] = np.nan
df_avg_high.loc[df_avg_high.index.year < 2019, 'Sentinel2'] = np.nan
df_avg_high = df_avg_high.rename(columns = {'MODIS': 'MODIS (high)', 'Landsat5': 'Landsat-5 (high)', 'Landsat7': 'Landsat-7 (high)', 'Landsat8': 'Landsat-8 (high)', 'Sentinel2': 'Sentinel-2 (high)'})

fig, ax = setup_canvas(1, 1)
df_avg_5km.plot(ax = ax)
df_avg_high.plot(ax = ax, style = '--', color = colors, legend = True)

for c in df_avg_5km.columns:
    ax.scatter(df_avg_5km.index, df_avg_5km[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
for c in df_avg_high.columns:
    ax.scatter(df_avg_high.index, df_avg_high[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

upper_legend(ax, ncols = 2, yloc = None)

ax.set_ylabel('Averaged VI')

# google.download_file(fig, 'Global_VI_avg.png')

def upper_legend(ax, xloc = 0.5, yloc = 1.1, ncols = None, nrows = None, user_labels = [], loc = "upper center", framealpha = 0., frameon = False):
    def reorder(list_in, nrows):
        ncols = len(list_in) // nrows
        if nrows * ncols != len(list_in): ncols += 1
        list_out = []
        for c in range(ncols):
            for r in range(nrows):
                if r * ncols + c >= len(list_in): continue
                list_out.append(list_in[r * ncols + c])
        assert len(list_in) == len(list_out), 'ERROR: len in != len out'
        return list_out, ncols
    handles, labels = ax.get_legend_handles_labels()
    if user_labels: labels = user_labels
    if len(handles) != len(labels): print('WARNING: the lengths are unequal')
    if nrows:
        labels, ncols = reorder(labels, nrows)
        handles, ncols = reorder(handles, nrows)
    by_label = dict(zip(labels, handles))
    if not ncols: ncols = len(labels)

    x0 = ax.get_position().x0
    x1 = ax.get_position().x1
    y0 = ax.get_position().y0
    y1 = ax.get_position().y1

    if xloc and yloc:
        ax.legend(by_label.values(), by_label.keys(), loc = loc, framealpha = framealpha, frameon = frameon, bbox_to_anchor=(xloc, yloc), ncol=ncols)
    else:
        ax.legend(by_label.values(), by_label.keys(), loc = loc, framealpha = framealpha, frameon = frameon, ncol=ncols, bbox_to_anchor=(0., -0.05, 1., 0.), borderaxespad=0, mode='expand')
    return ax

df_avg_5km = vi_5km.mean(dim = 'ID').to_dataframe().mean(axis = 1).reset_index().pivot(index = 'time', columns = 'satellite').resample('1YS').mean()
df_avg_5km.columns = df_avg_5km.columns.get_level_values(1)
df_avg_5km = df_avg_5km[['MODIS', 'Landsat5', 'Landsat7', 'Landsat8', 'Sentinel2']]
df_avg_5km.loc[df_avg_5km.index.year < 2015, 'Landsat8'] = np.nan
df_avg_5km.loc[df_avg_5km.index.year < 2019, 'Sentinel2'] = np.nan
df_avg_5km = df_avg_5km.rename(columns = {'MODIS': 'MODIS', 'Landsat5': 'Landsat-5', 'Landsat7': 'Landsat-7', 'Landsat8': 'Landsat-8', 'Sentinel2': 'Sentinel-2'})

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10, )
df_avg_5km.plot(ax = ax)

for c in df_avg_5km.columns:
    ax.scatter(df_avg_5km.index, df_avg_5km[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

upper_legend(ax, ncols = 3, yloc = 1.16)

ax.set_ylabel('Earth observations')
ax.set_xlabel('')

# google.download_file(fig, 'Global_VI_avg.png')

"""## Load EC and ERA5 data etc

### Load df_ml and nc_ml
"""

savefile_training = root_proj.joinpath('0tower_level/training_dataset_v2.nc') # v2 with 500 level training data
nc_ml = xr.open_dataset(savefile_training)
df_mlr = nc_ml.to_dataframe()
df_mlr = df_mlr.reset_index().set_index('time')

use_res = '500m' # 5km, 500m, or high
basecols = [c for c in df_mlr.columns if not ('_5km_' in c or '_500m_' in c or '_high_' in c)]

usecols = [c for c in df_mlr.columns if f'_5km_' in c]
rename_cols = dict(zip(usecols, [c.replace(f'_5km', '') for c in usecols]))
df_ml_5km = deepcopy(df_mlr)[basecols + usecols]#.rename(columns = rename_cols)

usecols = [c for c in df_mlr.columns if f'_500m_' in c]
rename_cols = dict(zip(usecols, [c.replace(f'_500m', '') for c in usecols]))
df_ml_500m = deepcopy(df_mlr)[basecols + usecols]#.rename(columns = rename_cols)

usecols = [c for c in df_mlr.columns if f'_high_' in c]
rename_cols = dict(zip(usecols, [c.replace(f'_high', '') for c in usecols]))
df_ml_high = deepcopy(df_mlr)[basecols + usecols]#.rename(columns = rename_cols)

if use_res == '5km':
    df_ml = deepcopy(df_ml_5km)
elif use_res == '500m':
    df_ml = deepcopy(df_ml_500m)
elif use_res == 'high':
    df_ml = deepcopy(df_ml_high)
else:
    raise Exception(f'ERROR: use_res is {use_res}')

# @title EC site NIRv vs. grid NIRv

df_mlt = df_mlr.copy().reset_index().set_index(['ID', 'time'])
dft = (df_mlt['NIRv_high_MODIS'] / df_mlt['NIRv_5km_MODIS']).dropna()
dft = dft[dft > 0].rename('h2l')
dft = dft.reset_index()[['ID', 'h2l']].groupby('ID').mean()
print(dft['h2l'].mean())

# dft['latitude'] = meta.loc[dft.index, 'LAT']
# dft['longitude'] = meta.loc[dft.index, 'LON']

# fig, ax = setup_canvas(1, 1, figsize = (8, 6))
# im = ax.scatter(dft['longitude'], dft['latitude'], c = dft['h2l'], cmap = 'Spectral_r', s = 20, vmin = 0, vmax = 5)

# world.plot(ax = ax, fc = "None", ec="black", alpha=0.5, zorder=2)

# x0 = ax.get_position().x0
# x1 = ax.get_position().x1
# y0 = ax.get_position().y0
# y1 = ax.get_position().y1
# cbar_ax = fig.add_axes([x1 + 0.001, y0, 0.02, y1 - y0])
# cbar = fig.colorbar(im, cax=cbar_ax, orientation = 'vertical')
# cbar.ax.locator_params(nbins = 5)
# # cbar.set_label(unit_dict['GPP'], labelpad = 0, fontsize = 22)

# ax.set_xlabel('Longitude'); ax.set_ylabel('Latitude')

vhigh = df_mlt[df_mlt['NIRv_high_MODIS'] > 0]['NIRv_high_MODIS'].mean()
v500m = df_mlt[df_mlt['NIRv_500m_MODIS'] > 0]['NIRv_500m_MODIS'].mean()
v5km = df_mlt[df_mlt['NIRv_5km_MODIS'] > 0]['NIRv_5km_MODIS'].mean()
vt = np.mean([xr.open_dataset(p)['NIRv'].mean(dim = 'time').mean() for p in root_proj.joinpath('1grid_inputs').glob('MODIS_*')])

print(vhigh / vt)
print(vhigh, v5km, v500m, vt)

"""### Generate df_ml and nc_ml

!!!NOTE: run the following cells in this section if savefile_training NOT exists!
"""

savefile_ec = root_proj.joinpath('FLUXNET2015_DD.nc')
if savefile_ec.exists():
    nc_ec = xr.open_dataset(savefile_ec)
else:
    df_path_fluxnet = []
    for p in root.joinpath('fmt_fluxdata/fluxnet_DD').glob('*.csv'):
        site = p.stem.split('_')[0]
        df_path_fluxnet.append([site, p])

    df_path_fluxnet = pd.DataFrame(df_path_fluxnet, columns = ['ID', 'PATH']).set_index('ID')

    df_ec = []
    for site in tqdm(df_path_fluxnet.index):
        p = df_path_fluxnet.loc[site, 'PATH']
        dft = pd.read_csv(p, index_col = 0, usecols = ['TIMESTAMP', 'LE_F_MDS', 'LE_F_MDS_QC', 'H_F_MDS', 'H_F_MDS_QC', 'NEE_VUT_REF', 'NEE_VUT_REF_QC', 'GPP_NT_VUT_REF', 'GPP_DT_VUT_REF', 'RECO_NT_VUT_REF', 'RECO_DT_VUT_REF']).replace(-9999., np.nan)
        dft.index = pd.to_datetime(dft.index, format = '%Y%m%d')
        dft['ID'] = site
        dft = dft.reset_index().rename(columns = {'TIMESTAMP': 'time'}).set_index(['ID', 'time'])
        df_ec.append(dft)

    df_ec = pd.concat(df_ec)
    nco = df_ec.to_xarray()

    nco.attrs['description'] = 'FLUXNET2015 DD 212 sites of NEE, GPP, Reco, H, and LE fluxes'
    nco.attrs['source'] = 'https://fluxnet.org/data/fluxnet2015-dataset/fullset-data-product/'

    nco.ID.attrs['eddy_covariance'] = '212 FLUXNET2015 Tier-1 and Tier-2 eddy covariance towers'
    nco.time.attrs['time_info'] = '1991-01-01 to 2014-12-31 on a daily basis'

    nco.LE_F_MDS.attrs['unit'] = 'W m-2'
    nco.LE_F_MDS_QC.attrs['unit'] = ''
    nco.H_F_MDS.attrs['unit'] = 'W m-2'
    nco.H_F_MDS_QC.attrs['unit'] = ''
    nco.NEE_VUT_REF.attrs['unit'] = 'gC m-2 d-1'
    nco.NEE_VUT_REF_QC.attrs['unit'] = ''
    nco.GPP_NT_VUT_REF.attrs['unit'] = 'gC m-2 d-1'
    nco.GPP_DT_VUT_REF.attrs['unit'] = 'gC m-2 d-1'
    nco.RECO_NT_VUT_REF.attrs['unit'] = 'gC m-2 d-1'
    nco.RECO_DT_VUT_REF.attrs['unit'] = 'gC m-2 d-1'

    nco.to_netcdf(savefile_ec)
    nc_ec = nco; nco.close(); del(nco)

from scigeo.meteo import saturation_vapor_pressure, specific_humidity2vapor_pressure

nc_era5 = xr.open_dataset(root_proj.joinpath('0tower_level/ERA5-Land-daily.nc'))

df_era5 = nc_era5.to_dataframe()
df_era5['dewpoint_temperature_2m'] -= 273.15
df_era5['temperature_2m'] -= 273.15
df_era5['soil_temperature_level_1'] -= 273.15

df_era5['surface_latent_heat_flux_sum'] /= 86400
df_era5['surface_sensible_heat_flux_sum'] /= 86400
df_era5['surface_solar_radiation_downwards_sum'] /= 86400
df_era5['surface_thermal_radiation_downwards_sum'] /= 86400

df_era5['surface_pressure'] /= 100

df_era5['temperature_2m_min'] -= 273.15
df_era5['temperature_2m_max'] -= 273.15

df_era5['VPD'] = saturation_vapor_pressure(df_era5['temperature_2m']) - saturation_vapor_pressure(df_era5['dewpoint_temperature_2m'])

era5r = df_era5[[
    'temperature_2m', 'soil_temperature_level_1',
    'snow_cover', 'volumetric_soil_water_layer_1', 'forecast_albedo',
    'surface_latent_heat_flux_sum', 'surface_sensible_heat_flux_sum',
    'surface_solar_radiation_downwards_sum',
    'surface_thermal_radiation_downwards_sum',
    'evaporation_from_the_top_of_canopy_sum', 'surface_pressure',
    'total_precipitation_sum', 'temperature_2m_min', 'temperature_2m_max',
    'u_component_of_wind_10m', 'v_component_of_wind_10m', 'VPD'
]].to_xarray()

root_proj_lucc = root.joinpath("workspace/project_data/LUCC")
df_c4 = pd.read_csv(root_proj_lucc.joinpath(f'0Training/c4map_tower.csv'), index_col = 0)
df_co2 = load_pickle(root_proj_lucc.joinpath(f'0Training/CO2_tower.pkl'))
df_koppen = pd.read_csv(root_proj_lucc.joinpath(f'0Training/KOPPEN_tower.csv'), index_col = 0)
df_dem = pd.read_csv(root_proj_lucc.joinpath(f'0Training/DEM_tower.csv'), index_col = 0)
df_fpar = pd.read_csv(root_proj_lucc.joinpath('0Training/MODIS_Fpar_tower.csv'), index_col = 0)
df_fpar.index = pd.to_datetime(df_fpar.index, format = '%Y-%m-%d')
df_fpar = df_fpar.resample('1MS').mean()

# df_ml monthly 5 km or high-res and use P-model

from scieco import photosynthesis

era5 = deepcopy(era5r)
nc_ec_mon = nc_ec.resample(time = '1MS').mean()
vi_mon_5km = vi_5km.resample(time = '1MS').mean()
vi_mon_500m = vi_500m.resample(time = '1MS').mean()
vi_mon_high = vi_high.resample(time = '1MS').mean()
era5_mon = era5.resample(time = '1MS').mean()

nc_ec_mon = nc_ec_mon.where((nc_ec_mon['time.year'] > 1999) & (nc_ec_mon['time.year'] < 2015), drop = True)
vi_mon_5km = vi_mon_5km.where((vi_mon_5km['time.year'] > 1999) & (vi_mon_5km['time.year'] < 2015), drop = True)
vi_mon_500m = vi_mon_500m.where((vi_mon_500m['time.year'] > 1999) & (vi_mon_500m['time.year'] < 2015), drop = True)
vi_mon_high = vi_mon_high.where((vi_mon_high['time.year'] > 1999) & (vi_mon_high['time.year'] < 2015), drop = True)
era5_mon = era5_mon.where((era5_mon['time.year'] > 1999) & (era5_mon['time.year'] < 2015), drop = True)

df_ml = []
for site in nc_ec.ID.data:
    df_ec = nc_ec_mon.sel(ID = site).drop_vars('ID').to_dataframe()

    df_ec = df_ec[df_ec['GPP_NT_VUT_REF'] >= 0]
    df_ec = df_ec[(df_ec['GPP_NT_VUT_REF'] - df_ec['GPP_DT_VUT_REF']).abs() <= 3]

    df_ec = df_ec[df_ec['RECO_NT_VUT_REF'] >= 0]
    df_ec = df_ec[(df_ec['RECO_NT_VUT_REF'] - df_ec['RECO_DT_VUT_REF']).abs() <= 3]

    df_vi_5km = []
    for vi_name in ['NDVI', 'EVI', 'NIRv', 'kNDVI']:
        dft = vi_mon_5km.sel(ID = site).to_dataframe()[vi_name].rename(vi_name + '_5km').reset_index().pivot(index = 'time', columns = 'satellite')
        dft.columns = dft.columns.map('_'.join).str.strip('_')
        df_vi_5km.append(dft)
    df_vi_5km = pd.concat(df_vi_5km, axis = 1)

    df_vi_500m = []
    for vi_name in ['NDVI', 'EVI', 'NIRv', 'kNDVI']:
        dft = vi_mon_500m.sel(ID = site).to_dataframe()[vi_name].rename(vi_name + '_500m').reset_index().pivot(index = 'time', columns = 'satellite')
        dft.columns = dft.columns.map('_'.join).str.strip('_')
        df_vi_500m.append(dft)
    df_vi_500m = pd.concat(df_vi_500m, axis = 1)

    df_vi_high = []
    for vi_name in ['NDVI', 'EVI', 'NIRv', 'kNDVI']:
        dft = vi_mon_high.sel(ID = site).to_dataframe()[vi_name].rename(vi_name + '_high').reset_index().pivot(index = 'time', columns = 'satellite')
        dft.columns = dft.columns.map('_'.join).str.strip('_')
        df_vi_high.append(dft)
    df_vi_high = pd.concat(df_vi_high, axis = 1)

    df_era5 = era5_mon.sel(ID = site).drop_vars('ID').to_dataframe()

    dft = pd.concat([df_ec, df_vi_5km, df_vi_500m, df_vi_high, df_era5], axis = 1)

    dft = pd.concat([dft, df_co2[df_co2.index.get_level_values(1) == site].droplevel('ID')], axis = 1)
    dft = pd.concat([dft, df_fpar[site].rename('FAPAR_MODIS_PROD')], axis = 1)

    # dft_cold = dft[dft['temperature_2m'] <= -30]
    # dft = dft[dft['temperature_2m'] > -30]
    dft.loc[dft['temperature_2m'] < -30, 'temperature_2m'] = -30

    tc = dft['temperature_2m'].values
    co2 = dft['co2'].values
    patm = dft['surface_pressure'].values * 100
    vpd = dft['VPD'].values * 100
    do_ftemp_kphio = True

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = True, limitation_factors = 'none')
    # ----------------------------------------------------------------------------------------------------------------------------
    dft['C3_LUE'] = c3_lue
    dft['IWUE'] = iwue
    dft['C4_LUE'] = c4_lue
    # dft = pd.concat([dft, dft_cold], axis = 0)
    dft['C4_RATIO'] = df_c4.loc[site, 'C4_area'] # df_c4 has no NaN, min is 0
    dft['KOPPEN'] = df_koppen.loc[site, 'KOPPEN']
    dft['DEM'] = df_dem.loc[site, 'DEM']
    dft['ID'] = site

    dft['YEAR'] = dft.index.year
    dft['MONTH'] = dft.index.month
    dft['LAT'] = meta.loc[site, 'LAT']
    dft['LON'] = meta.loc[site, 'LON']

    df_ml.append(dft)

df_ml = pd.concat(df_ml, axis = 0)
for c in [c for c in df_ml.columns if c.startswith('EVI')]:
    df_ml[c.replace('EVI_', 'FAPAR_')] = (df_ml[c] - 0.1) * 1.25

df_ml['IGBP'] = [MODIS_IGBP_dict[igbp] for igbp in meta.loc[df_ml['ID'], 'IGBP'].values]
df_ml['El-Nino-La-Nina'] = df_el.loc[df_ml.index.year, 'El-Nino-La-Nina'].values

df_ml.reset_index().rename(columns = {'index': 'time'}).set_index(['ID', 'time']).to_xarray().to_netcdf(savefile_training)

"""## Compare GPP site level"""

df_ml_5km_ = df_ml_5km.copy()
df_ml_5km_.columns = [c.replace('_5km_', '_') for c in df_ml_5km_.columns]

df_ml_high_ = df_ml_high.copy()
df_ml_high_.columns = [c.replace('_high_', '_') for c in df_ml_high_.columns]

df_vi_gpp = []
for vi_name in [
    'NDVI_Landsat5', 'NDVI_Landsat7', 'NDVI_Landsat8',
    'NDVI_MODIS', 'NDVI_Sentinel2', 'EVI_Landsat5', 'EVI_Landsat7',
    'EVI_Landsat8', 'EVI_MODIS', 'EVI_Sentinel2', 'NIRv_Landsat5',
    'NIRv_Landsat7', 'NIRv_Landsat8', 'NIRv_MODIS', 'NIRv_Sentinel2',
    'kNDVI_Landsat5', 'kNDVI_Landsat7', 'kNDVI_Landsat8', 'kNDVI_MODIS',
    'kNDVI_Sentinel2', 'FAPAR_Landsat5',
    'FAPAR_Landsat7', 'FAPAR_Landsat8', 'FAPAR_MODIS', 'FAPAR_Sentinel2', 'FAPAR_MODIS_PROD'
    ]:

    dft = df_ml_5km_[['GPP_NT_VUT_REF', vi_name]].dropna()
    if dft.empty: continue
    res = pipelines.get_metrics(dft, truth = 'GPP_NT_VUT_REF', pred = vi_name, return_dict = True)
    res['VI'] = vi_name.split('_')[0]
    res['Satellite'] = vi_name.split('_')[1]
    df_vi_gpp.append(res)
df_vi_gpp = pd.concat(df_vi_gpp, axis = 0)

# df_vi_gpp['Satellite'].replace({'Landsat5': 'Landsat-5', 'Landsat7': 'Landsat-7', 'Landsat8': 'Landsat-8', 'Sentinel2': 'Sentinel-2'}, inplace=True)
df_vi_gpp.rename(columns = {'R2': '$R^2$'}, inplace=True)

dfp_5km = pd.concat([
    df_vi_gpp.drop('Satellite', axis = 1).groupby('VI').mean(),
    df_vi_gpp.drop('VI', axis = 1).groupby('Satellite').mean()
], axis = 0)
dfp_5km['Resolution'] = 0 # 'coarse'
dfp_5km = dfp_5km.rename(index = {
    'EVI': 'EVI',
    'FAPAR': 'FAPAR',
    'NDVI': 'NDVI',
    'NIRv': 'NIRv',
    'kNDVI': 'kNDVI',
    'MODIS': 'MODIS',
    'Landsat5': 'Landsat-5',
    'Landsat7': 'Landsat-7',
    'Landsat8': 'Landsat-8',
    'Sentinel2': 'Sentinel-2'
    }
)

# ------------------------------------------------------------------------------
df_vi_gpp = []
for vi_name in [
    'NDVI_Landsat5', 'NDVI_Landsat7', 'NDVI_Landsat8',
    'NDVI_MODIS', 'NDVI_Sentinel2', 'EVI_Landsat5', 'EVI_Landsat7',
    'EVI_Landsat8', 'EVI_MODIS', 'EVI_Sentinel2', 'NIRv_Landsat5',
    'NIRv_Landsat7', 'NIRv_Landsat8', 'NIRv_MODIS', 'NIRv_Sentinel2',
    'kNDVI_Landsat5', 'kNDVI_Landsat7', 'kNDVI_Landsat8', 'kNDVI_MODIS',
    'kNDVI_Sentinel2', 'FAPAR_Landsat5',
    'FAPAR_Landsat7', 'FAPAR_Landsat8', 'FAPAR_MODIS', 'FAPAR_Sentinel2', 'FAPAR_MODIS_PROD'
    ]:

    dft = df_ml_high_[['GPP_NT_VUT_REF', vi_name]].dropna()
    if dft.empty: continue
    res = pipelines.get_metrics(dft, truth = 'GPP_NT_VUT_REF', pred = vi_name, return_dict = True)
    res['VI'] = vi_name.split('_')[0]
    res['Satellite'] = vi_name.split('_')[1]
    df_vi_gpp.append(res)
df_vi_gpp = pd.concat(df_vi_gpp, axis = 0)

# df_vi_gpp['Satellite'].replace({'Landsat5': 'Landsat-5', 'Landsat7': 'Landsat-7', 'Landsat8': 'Landsat-8', 'Sentinel2': 'Sentinel-2'}, inplace=True)

dfp_high = pd.concat([
    df_vi_gpp.drop('Satellite', axis = 1).groupby('VI').mean(),
    df_vi_gpp.drop('VI', axis = 1).groupby('Satellite').mean()
], axis = 0)
dfp_high['Resolution'] = 1 # 'high'
dfp_high = dfp_high.rename(index = {
    'EVI': 'EVI',
    'FAPAR': 'FAPAR',
    'NDVI': 'NDVI',
    'NIRv': 'NIRv',
    'kNDVI': 'kNDVI',
    'MODIS': 'MODIS',
    'Landsat5': 'Landsat-5',
    'Landsat7': 'Landsat-7',
    'Landsat8': 'Landsat-8',
    'Sentinel2': 'Sentinel-2'
    }
)
dfpr = pd.concat([dfp_5km, dfp_high], axis = 0)

dfpr.rename(columns = {'r2': '$R^2$'}, inplace=True)
dfpr

dfp = dfpr.copy()
dfp['$R^2$'] = np.sqrt(dfp['$R^2$'])

fig, ax = setup_canvas(1, 1, figsize = (6, 6), fontsize = 12, labelsize = 12)
ax.scatter(dfp.loc[['EVI', 'NDVI', 'NIRv', 'kNDVI'], 'Resolution'], dfp.loc[['EVI', 'NDVI', 'NIRv', 'kNDVI'], '$R^2$'], s = 20, edgecolor = 'k', color = colors[0])
ax.scatter(dfp.loc[['Landsat-5', 'Landsat-7', 'Landsat-8', 'MODIS'], 'Resolution'], dfp.loc[['Landsat-5', 'Landsat-7', 'Landsat-8', 'MODIS'], '$R^2$'], s = 20, edgecolor = 'k', color = colors[1])

# -------------------------------------------------------------------------------------------------------------------------------------------
txt = 'EVI'
xloc = 0
yloc = dfp.query('Resolution == 0').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.5, yloc-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
# # txt = 'FAPAR'
txt = 'NDVI'
xloc = 0
yloc = dfp.query('Resolution == 0').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.5, yloc-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
txt = 'NIRv'
xloc = 0
yloc = dfp.query('Resolution == 0').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.5, yloc-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
txt = 'kNDVI'
xloc = 0
yloc = dfp.query('Resolution == 0').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.5, yloc-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))

txt = 'Landsat-5'
xloc = 0
yloc = dfp.query('Resolution == 0').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.8, yloc+0.002), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
txt = 'Landsat-7'
xloc = 0
yloc = dfp.query('Resolution == 0').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.8, yloc-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
txt = 'Landsat-8'
xloc = 0
yloc = dfp.query('Resolution == 0').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.8, yloc-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
txt = 'MODIS'
xloc = 0
yloc = dfp.query('Resolution == 0').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.5, yloc-0.002), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
# -------------------------------------------------------------------------------------------------------------------------------------------
txt = 'EVI'
xloc = 1
yloc = dfp.query('Resolution == 1').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc+0.5, yloc-0.005), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
# # txt = 'FAPAR'
txt = 'NDVI'
xloc = 1
yloc = dfp.query('Resolution == 1').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.5, yloc-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
txt = 'NIRv'
xloc = 1
yloc = dfp.query('Resolution == 1').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.5, yloc-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
txt = 'kNDVI'
xloc = 1
yloc = dfp.query('Resolution == 1').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.5, yloc-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))

txt = 'Landsat-5'
xloc = 1
yloc = dfp.query('Resolution == 1').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.8, yloc+0.002), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
txt = 'Landsat-7'
xloc = 1
yloc = dfp.query('Resolution == 1').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc+0.3, yloc-0.005), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
txt = 'Landsat-8'
xloc = 1
yloc = dfp.query('Resolution == 1').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.8, yloc-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))
txt = 'MODIS'
xloc = 1
yloc = dfp.query('Resolution == 1').loc[txt, '$R^2$']
ax.annotate(txt, (xloc, yloc), xytext=(xloc-0.5, yloc-0.002), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-20"))

ax.set_xticks([0, 1])
ax.set_xticklabels(['5 km', 'Original Spatial Res.'])
ax.set_xlim(-1, 2)

ax.set_ylabel('Correlation')
# google.download_file(fig, 'VI_vs_GPP.png')

# print(list(dfp.index))

# fig, ax = setup_canvas(1, 1, figsize = (6, 6), fontsize = 12)
# ax.scatter(dfp.loc[['EVI', 'NDVI', 'NIRv', 'kNDVI'], '$R^2$'], dfp.loc[['EVI', 'NDVI', 'NIRv', 'kNDVI'], 'RMSE'], s = 20, edgecolor = 'k', color = colors[0])
# ax.scatter(dfp.loc[['Landsat-5', 'Landsat-7', 'Landsat-8', 'MODIS'], '$R^2$'], dfp.loc[['Landsat-5', 'Landsat-7', 'Landsat-8', 'MODIS'], 'RMSE'], s = 20, edgecolor = 'k', color = colors[1])

# # txt = 'EVI'
# # ax.annotate(txt, (dfp.loc[txt, '$R^2$'], dfp.loc[txt, 'RMSE']), xytext=(dfp.loc[txt, '$R^2$']+0.0, dfp.loc[txt, 'RMSE']-0.5), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=40,angleB=-70"))
# # # txt = 'FAPAR'
# # # ax.annotate(txt, (dfp.loc[txt, '$R^2$'], dfp.loc[txt, 'RMSE']), xytext=(dfp.loc[txt, '$R^2$']+0.03, dfp.loc[txt, 'RMSE']-0.), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=30,angleB=-10"))
# # txt = 'NDVI'
# # ax.annotate(txt, (dfp.loc[txt, '$R^2$'], dfp.loc[txt, 'RMSE']), xytext=(dfp.loc[txt, '$R^2$']-0.1, dfp.loc[txt, 'RMSE']-0.4), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-90"))
# # txt = 'NIRv'
# # ax.annotate(txt, (dfp.loc[txt, '$R^2$'], dfp.loc[txt, 'RMSE']), xytext=(dfp.loc[txt, '$R^2$']+0.3, dfp.loc[txt, 'RMSE']-0.0), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=20,angleB=0"))
# # txt = 'kNDVI'
# # ax.annotate(txt, (dfp.loc[txt, '$R^2$'], dfp.loc[txt, 'RMSE']), xytext=(dfp.loc[txt, '$R^2$']-0.0, dfp.loc[txt, 'RMSE']-0.7), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-90"))

# txt = 'Landsat-5'
# ax.annotate(txt, (dfp.loc[txt, '$R^2$'], dfp.loc[txt, 'RMSE']), xytext=(dfp.loc[txt, '$R^2$']+0.2, dfp.loc[txt, 'RMSE']+0.4), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=-10,angleB=-90"))
# txt = 'Landsat-7'
# ax.annotate(txt, (dfp.loc[txt, '$R^2$'], dfp.loc[txt, 'RMSE']), xytext=(dfp.loc[txt, '$R^2$']-0.2, dfp.loc[txt, 'RMSE']+0.4), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=-90"))
# txt = 'Landsat-8'
# ax.annotate(txt, (dfp.loc[txt, '$R^2$'], dfp.loc[txt, 'RMSE']), xytext=(dfp.loc[txt, '$R^2$']+0.0, dfp.loc[txt, 'RMSE']+0.9), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=-10,angleB=-90"))
# txt = 'MODIS'
# ax.annotate(txt, (dfp.loc[txt, '$R^2$'], dfp.loc[txt, 'RMSE']), xytext=(dfp.loc[txt, '$R^2$']+0.2, dfp.loc[txt, 'RMSE']-0.4), arrowprops = dict(arrowstyle="->", connectionstyle="angle3,angleA=10,angleB=90"))

# ax.set_xlim(0, 1)
# ax.set_ylim(3.5, 7)
# ax.set_xlabel('$R^2$')
# ax.set_ylabel('RMSE') #  ($g \ C m^{-2} d^{-1}$)

# # google.download_file(fig, 'VI_vs_GPP_5km.png')

dfi = df_ml.copy()
dfi.index.name = 'time'
dfi = dfi.reset_index().set_index(['ID', 'time'])

y_name = ['GPP_NT_VUT_REF']
X_names = [
    'temperature_2m', 'surface_solar_radiation_downwards_sum','VPD',
    'soil_temperature_level_1', 'surface_thermal_radiation_downwards_sum', 'total_precipitation_sum', #'temperature_2m_min', 'temperature_2m_max',
    'co2', 'C3_LUE', 'IWUE', 'C4_LUE', 'C4_RATIO', 'KOPPEN', 'DEM', 'IGBP',
    'YEAR', 'MONTH', 'LAT', 'LON'
]

# for comp_vi in ['NDVI_MODIS', 'EVI_MODIS', 'kNDVI_MODIS', 'NIRv_Landsat5', 'NIRv_Landsat7', 'EVI_Landsat5', 'EVI_Landsat7', 'NDVI_Landsat5', 'NDVI_Landsat7', 'kNDVI_Landsat5', 'kNDVI_Landsat7',]:
for comp_vi in ['NIRv_Landsat5', 'NIRv_Landsat7']:
    dft = dfi.loc[dfi[['NIRv_MODIS', comp_vi] + X_names + y_name].dropna().index, :]
    df_res = []
    for vi_name in ['NIRv_MODIS', comp_vi]:
        # print(vi_name)
        # Method1: ======================================================================
        np.random.seed(42)
        sites = dft.index.get_level_values(0).drop_duplicates()
        test_sites = np.random.choice(sites, len(sites) // 4, replace=False)
        train_sites = sites.difference(test_sites)
        test_index = dft.index[dft.index.get_level_values(0).isin(test_sites)]
        train_index = dft.index[dft.index.get_level_values(0).isin(train_sites)]

        X_train = dft.loc[train_index, X_names + [vi_name]]
        y_train = dft.loc[train_index, y_name]
        X_test = dft.loc[test_index, X_names + [vi_name]]
        y_test = dft.loc[test_index, y_name]
        # # Method2: ======================================================================
        # Xs = dft[X_names + [vi_name]]
        # ys = dft[y_name]
        # X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.33, random_state=42)

        regr, dfo, res = run_ml(X_train, X_test, y_train, y_test)
        res.index = [vi_name]
        df_res.append(res)
    df_res = pd.concat(df_res, axis = 0)
    df_res = (df_res.iloc[1, :] / df_res.iloc[0, :]).rename(f'{comp_vi}/NIRv_MODIS').to_frame().T
    print(df_res)

"""# Modelling"""

# P-model

dfi = df_ml.copy()
dfi.index.name = 'time'
dfi = dfi.reset_index().set_index(['ID', 'time'])

y_name = ['GPP_NT_VUT_REF']
X_names = [
    'temperature_2m', 'surface_solar_radiation_downwards_sum','VPD',
    'soil_temperature_level_1', 'surface_thermal_radiation_downwards_sum', 'total_precipitation_sum',
    # 'temperature_2m_min', 'temperature_2m_max',
    # 'u_component_of_wind_10m', 'v_component_of_wind_10m', 'volumetric_soil_water_layer_1',
    # 'co2', 'C3_LUE', 'IWUE', 'C4_LUE', 'C4_RATIO',
    'KOPPEN', 'DEM', 'IGBP', 'El-Nino-La-Nina', 'IWUE',
    'MONTH', 'LAT', 'LON',
    # 'YEAR', 'MONTH', 'LAT', 'LON'
]


vi_type = 'EVI'
df_res = []; dd_regr = {}; dd_fit = {}; dd_dfo = {}
for vi_name in [f'{vi_type}_{use_res}_MODIS', f'{vi_type}_{use_res}_Landsat5', f'{vi_type}_{use_res}_Landsat7']:
    evi_name = vi_name.replace('NIRv', 'EVI')
    dft = dfi.loc[dfi[[vi_name, evi_name] + X_names + y_name].dropna().index, :]
    dft['FAPAR'] = (dfi[evi_name] - 0.1 + 0.1) * 1.25
    if vi_name in ['NIRv_Landsat5', 'NIRv_Landsat7']:
        dft['FAPAR'] = dft[vi_name] * 1.5
    dft = dft[dft[vi_name] > 1e-9]
    dft = dft[dft['FAPAR'] > 1e-9]
    dft['GPPm'] = (dft['C3_LUE'] * (1 - dft['C4_RATIO'] / 100) + dft['C4_LUE']  * dft['C4_RATIO'] / 100) * dft['FAPAR'] * dft['surface_solar_radiation_downwards_sum'] * 2.3
    slope, intercept, rvalue, pvalue, stderr = stats.linregress(dft[y_name[0]], dft['GPPm'])
    dft['GPPm'] = (dft['GPPm'] - intercept) / slope
    dft['GPPd'] = dft[y_name[0]].values - dft['GPPm'].values
    df_gppm = dft[['GPPm']]
    dft = dft[X_names + ['GPPd'] + [vi_name]].dropna()

    # print(vi_name)
    # Method1: ======================================================================
    np.random.seed(42)
    sites = dft.index.get_level_values(0).drop_duplicates()
    test_sites = np.random.choice(sites, len(sites) // 4, replace=False)
    train_sites = sites.difference(test_sites)
    test_index = dft.index[dft.index.get_level_values(0).isin(test_sites)]
    train_index = dft.index[dft.index.get_level_values(0).isin(train_sites)]

    X_train = dft.loc[train_index, X_names + [vi_name]].rename(columns = {vi_name: 'NIRv'})
    y_train = dft.loc[train_index, ['GPPd']]
    X_test = dft.loc[test_index, X_names + [vi_name]].rename(columns = {vi_name: 'NIRv'})
    y_test = dft.loc[test_index, ['GPPd']]
    # Method2: ======================================================================
    # Xs = dft[X_names + [vi_name]].rename(columns = {vi_name: 'NIRv'})
    # ys = dft[['GPPd']]
    # X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.33, random_state=42)

    regr, dfo, res = run_ml(X_train, X_test, y_train, y_test)
    res.index = [vi_name]
    df_res.append(res)
    dd_regr[vi_name] = regr
    dd_fit[vi_name] = {'slope': slope, 'intercept': intercept, 'rvalue': rvalue, 'pvalue': pvalue, 'stderr': stderr}
    dfo['IGBP'] = meta.loc[dfo.index.get_level_values('ID'), 'IGBP'].values
    dfo[y_name[0]] = dfi.loc[dfo.index, y_name[0]]
    dfo['GPPm'] = df_gppm.loc[dfo.index, 'GPPm']
    dfo['GPPmML'] = dfo['pred'] + dfo['GPPm']
    dd_dfo[vi_name] = dfo
df_res = pd.concat(df_res, axis = 0)
print(df_res)

from sciml.metrics import get_rmse
from sciml.regress2 import regress2

sat_name = 'Landsat7'
vi_name = vi_name.split('_')[0] + '_' + use_res + '_' + sat_name
assert vi_name.split('_')[1] == use_res, 'ERR: wrong spatial resolution'

sat_label = sat_name
res_label = use_res
if sat_label == 'Landsat7': sat_label = 'Landsat-7'
if res_label == '5km':
    res_label = '5 km'
elif res_label == '500m':
    res_label = '500 m'
elif res_label == 'high':
    if sat_label == 'Landsat-7':
        res_label = '30 m'
    elif sat_label == 'MODIS':
        res_label = '500 m'

# ------------------------------------------------------------------------------

dfo = dd_dfo[vi_name]
dfo = dfo[dfo['GPPmML'] >= 0]
dfo = dfo[['GPP_NT_VUT_REF', 'GPPmML']]

fig, ax = setup_canvas(1, 1, figsize = (4, 4))
eval_res = regress2(dfo['GPP_NT_VUT_REF'].values, dfo['GPPmML'].values)
rvalue = eval_res['r']
slope = roundit(eval_res['slope'])
intercept = roundit(eval_res['intercept'])
rmse = get_rmse(dfo['GPP_NT_VUT_REF'].values, dfo['GPPmML'].values)
rmse = roundit(rmse)
print(eval_res)
# ax.scatter(dfo['GPP_NT_VUT_REF'].values, dfo['GPPmML'].values)
kde_scatter(ax, dfo, 'GPP_NT_VUT_REF', 'GPPmML', frac = 1, v_scale = 0.01)
ax.text(0.1, 0.75, f"{sat_label} {vi_name.split('_')[0]} ({res_label})\n" + f'y = {slope}x + {intercept}\n' + '$R^2$: ' + f'{roundit(rvalue**2)}; '+ 'RMSE: ' + f'{rmse}', transform = ax.transAxes)
ax.set_xlabel('EC GPP')
ax.set_ylabel('UFLUX-2 GPP')

dfmt = pd.Series(
    [sat_label, vi_name.split('_')[0], res_label, roundit(rvalue**2), rmse, slope, intercept],
    index = ['Satellite', 'VI', 'Resolution', '$R^2$', 'RMSE', 'Slope', 'Intercept']
).to_frame().T

# google.download_file(fig, f'UFLUX-2_FLUXNET_val-{vi_name}.pdf')
google.download_file(dfmt, f'UFLUX-2_FLUXNET_val-{vi_name}.csv')

df_sat_reso_val = pd.read_excel(root_proj.joinpath('202505/sat_reso_val.xlsx'), index_col = 0)

fig, ax = setup_canvas(1, 1, figsize = (5, 5))
dfpt = df_sat_reso_val[df_sat_reso_val['Satellite'] == 'MODIS']
dfptt = dfpt[dfpt['Resolution'] == '5 km']
ax.scatter(dfptt['RMSE'], dfptt['$R^2$'], s = 50, color = colors[0], edgecolor = 'k', marker = 'o')
dfptt = dfpt[dfpt['Resolution'] == '500 m']
ax.scatter(dfptt['RMSE'], dfptt['$R^2$'], s = 50, color = colors[1], edgecolor = 'k', marker = 'o')

dfpt = df_sat_reso_val[df_sat_reso_val['Satellite'] == 'Landsat-7']
dfptt = dfpt[dfpt['Resolution'] == '5 km']
ax.scatter(dfptt['RMSE'], dfptt['$R^2$'], s = 50, color = colors[0], edgecolor = 'k', marker = 's')
dfptt = dfpt[dfpt['Resolution'] == '500 m']
ax.scatter(dfptt['RMSE'], dfptt['$R^2$'], s = 50, color = colors[1], edgecolor = 'k', marker = 's')
dfptt = dfpt[dfpt['Resolution'] == '30 m']
ax.scatter(dfptt['RMSE'], dfptt['$R^2$'], s = 50, color = colors[2], edgecolor = 'k', marker = 's')

ax.scatter([], [], s = 100, color = 'w', edgecolor = 'k', marker = 'o', label = 'MODIS')
ax.scatter([], [], s = 100, color = 'w', edgecolor = 'k', marker = 's', label = 'Landsat-7')
ax.plot([], [], lw = 5, color = colors[0], label = '5 km')
ax.plot([], [], lw = 5, color = colors[1], label = '500 m')
ax.plot([], [], lw = 5, color = colors[2], label = '30 m')

upper_legend(ax)
ax.set_xlabel('RMSE')
ax.set_ylabel('$R^2$')

google.download_file(fig, 'sat_reso_val.pdf')

from sciml.metrics import get_rmse
from sciml.regress2 import regress2

dfo = dd_dfo[f'NIRv_{use_res}_MODIS']
dfo = dfo[dfo['GPPmML'] >= 0]
dfo = dfo[['GPP_NT_VUT_REF', 'GPPmML']]

fig, ax = setup_canvas(1, 1, figsize = (6, 6))
eval_res = regress2(dfo['GPP_NT_VUT_REF'].values, dfo['GPPmML'].values)
rvalue = eval_res['r']
slope = roundit(eval_res['slope'])
intercept = roundit(eval_res['intercept'])
rmse = get_rmse(dfo['GPP_NT_VUT_REF'].values, dfo['GPPmML'].values)
rmse = roundit(rmse)
print(eval_res)
# ax.scatter(dfo['GPP_NT_VUT_REF'].values, dfo['GPPmML'].values)
kde_scatter(ax, dfo, 'GPP_NT_VUT_REF', 'GPPmML', frac = 1, v_scale = 0.01)
ax.text(0.1, 0.75, f'y = {slope}x + {intercept}\n' + '$R^2$: ' + f'{roundit(rvalue**2)}; '+ 'RMSE: ' + f'{rmse}', transform = ax.transAxes)
ax.set_xlabel('EC GPP')
ax.set_ylabel('UFLUX-2 GPP')

# google.download_file(fig, 'UFLUX-2_FLUXNET_val.png')

# Leave one out

from sklearn.model_selection import KFold

kf = KFold(n_splits = 5)
# P-model

dfi = df_ml.copy()
dfi.index.name = 'time'
dfi = dfi.reset_index().set_index(['ID', 'time'])

y_name = ['GPP_NT_VUT_REF']
X_names = [
    'temperature_2m', 'surface_solar_radiation_downwards_sum','VPD',
    'soil_temperature_level_1', 'surface_thermal_radiation_downwards_sum', 'total_precipitation_sum',
    # 'temperature_2m_min', 'temperature_2m_max',
    # 'u_component_of_wind_10m', 'v_component_of_wind_10m', 'volumetric_soil_water_layer_1',
    # 'co2', 'C3_LUE', 'IWUE', 'C4_LUE', 'C4_RATIO',
    'KOPPEN', 'DEM', 'IGBP', 'El-Nino-La-Nina', 'IWUE',
    'MONTH', 'LAT', 'LON',
    # 'YEAR', 'MONTH', 'LAT', 'LON'
]

vi_name = f'NIRv_{use_res}_MODIS'
evi_name = vi_name.replace('NIRv', 'EVI')
dft = dfi.loc[dfi[[vi_name, evi_name] + X_names + y_name].dropna().index, :]
dft['FAPAR'] = (dfi[evi_name] - 0.1 + 0.1) * 1.25
if vi_name in ['NIRv_Landsat5', 'NIRv_Landsat7']:
    dft['FAPAR'] = dft[vi_name] * 1.5
dft = dft[dft[vi_name] > 1e-9]
dft = dft[dft['FAPAR'] > 1e-9]
dft['GPPm'] = (dft['C3_LUE'] * (1 - dft['C4_RATIO'] / 100) + dft['C4_LUE']  * dft['C4_RATIO'] / 100) * dft['FAPAR'] * dft['surface_solar_radiation_downwards_sum'] * 2.3
slope, intercept, rvalue, pvalue, stderr = stats.linregress(dft[y_name[0]], dft['GPPm'])
dft['GPPm'] = (dft['GPPm'] - intercept) / slope
dft['GPPd'] = dft[y_name[0]].values - dft['GPPm'].values
df_gppm = dft[['GPPm']]
dft = dft[X_names + ['GPPd'] + [vi_name]].dropna()

# K-folding: ======================================================================
df_res = []; dd_regr = {}; dd_fit = {}; dd_dfo = []
Xs = dft[X_names + [vi_name]].rename(columns = {vi_name: 'NIRv'})
ys = dft[['GPPd']]
# X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.33, random_state=42)
for i, (train_index, test_index) in enumerate(kf.split(Xs)):
    X_train = Xs.iloc[train_index, :]
    y_train = ys.iloc[train_index, :]
    X_test = Xs.iloc[test_index, :]
    y_test = ys.iloc[test_index, :]
    regr, dfo, res = run_ml(X_train, X_test, y_train, y_test)
    res.index = [vi_name]
    df_res.append(res)
    dd_regr[vi_name] = regr
    dd_fit[vi_name] = {'slope': slope, 'intercept': intercept, 'rvalue': rvalue, 'pvalue': pvalue, 'stderr': stderr}
    dfo['IGBP'] = meta.loc[dfo.index.get_level_values('ID'), 'IGBP'].values
    dfo[y_name[0]] = dfi.loc[dfo.index, y_name[0]]
    dfo['GPPm'] = df_gppm.loc[dfo.index, 'GPPm']
    dfo['GPPmML'] = dfo['pred'] + dfo['GPPm']
    dd_dfo.append(dfo)
df_res = pd.concat(dd_dfo, axis = 0)
print(df_res)

dfo = df_res.copy()
dfo = dfo[dfo['GPPmML'] >= 0]
dfo = dfo[['GPP_NT_VUT_REF', 'GPPmML']]

fig, ax = setup_canvas(1, 1, figsize = (6, 6))
eval_res = regress2(dfo['GPP_NT_VUT_REF'].values, dfo['GPPmML'].values)
rvalue = eval_res['r']
slope = roundit(eval_res['slope'])
intercept = roundit(eval_res['intercept'])
rmse = get_rmse(dfo['GPP_NT_VUT_REF'].values, dfo['GPPmML'].values)
rmse = roundit(rmse)
print(eval_res)
# ax.scatter(dfo['GPP_NT_VUT_REF'].values, dfo['GPPmML'].values)
kde_scatter(ax, dfo, 'GPP_NT_VUT_REF', 'GPPmML', frac = 1, v_scale = 0.01)
ax.text(0.1, 0.75, f'y = {slope}x + {intercept}\n' + '$R^2$: ' + f'{roundit(rvalue**2)}; '+ 'RMSE: ' + f'{rmse}', transform = ax.transAxes)
ax.set_xlabel('EC GPP')
ax.set_ylabel('UFLUX-2 GPP')

# google.download_file(fig, 'UFLUX-2_FLUXNET_val.png')

dfo = df_res.copy()
dfo = dfo[dfo['GPPmML'] >= 0]

def get_rmse(observations, estimates):
    return np.sqrt(((estimates - observations) ** 2).mean())

# dfo['IGBP'].drop_duplicates().sort_values() # remove ['SNO']

a_name = 'GPP_NT_VUT_REF'
b_name = 'GPPm'
fig, axes = setup_canvas(4, 3, figsize = (12, 12))
for i, igbp in enumerate(['CRO', 'CSH', 'DBF', 'DNF', 'EBF', 'ENF', 'GRA', 'MF', 'OSH', 'SAV', 'WET', 'WSA']):
    dfit = dfo[dfo['IGBP'] == igbp]
    ax = axes[i]
    # ax.scatter(dfit[a_name], dfit[b_name])
    kde_scatter(ax, dfit, a_name, b_name, frac = 1, v_scale = 0.01)
    # ax.text(0.1, 0.9, igbp, transform = ax.transAxes, fontsize = 10)
    metrics = regress2(dfit[a_name].values, dfit[b_name].values)
    print(igbp, roundit([metrics['slope'], metrics['r']**2, get_rmse(dfit[a_name].values, dfit[b_name].values)]))

    eval_res = regress2(dfit[a_name].values, dfit[b_name].values)
    rvalue = eval_res['r']
    slope = roundit(eval_res['slope'])
    intercept = roundit(eval_res['intercept'])
    rmse = get_rmse(dfit[a_name].values, dfit[b_name].values)
    rmse = roundit(rmse)

    ax.text(0.1, 0.7, f'{igbp}\n' + f'y = {slope}x + {intercept}\n' + '$R^2$: ' + f'{roundit(rvalue**2)}; '+ 'RMSE: ' + f'{rmse}', transform = ax.transAxes, fontsize = 10)

axes[-2].set_xlabel('EC GPP')
axes[3].set_ylabel('Process model GPP')

google.download_file(fig, 'Process_FLUXNET_val_IGBP.png')

def get_rmse(observations, estimates):
    return np.sqrt(((estimates - observations) ** 2).mean())

# dfo['IGBP'].drop_duplicates().sort_values() # remove ['SNO']

a_name = 'GPP_NT_VUT_REF'
b_name = 'GPPmML'
fig, axes = setup_canvas(4, 3, figsize = (12, 12))
for i, igbp in enumerate(['CRO', 'CSH', 'DBF', 'DNF', 'EBF', 'ENF', 'GRA', 'MF', 'OSH', 'SAV', 'WET', 'WSA']):
    dfit = dfo[dfo['IGBP'] == igbp]
    ax = axes[i]
    # ax.scatter(dfit[a_name], dfit[b_name])
    kde_scatter(ax, dfit, a_name, b_name, frac = 1, v_scale = 0.01)
    # ax.text(0.1, 0.9, igbp, transform = ax.transAxes, fontsize = 10)
    metrics = regress2(dfit[a_name].values, dfit[b_name].values)
    print(igbp, roundit([metrics['slope'], metrics['r']**2, get_rmse(dfit[a_name].values, dfit[b_name].values)]))

    eval_res = regress2(dfit[a_name].values, dfit[b_name].values)
    rvalue = eval_res['r']
    slope = roundit(eval_res['slope'])
    intercept = roundit(eval_res['intercept'])
    rmse = get_rmse(dfit[a_name].values, dfit[b_name].values)
    rmse = roundit(rmse)

    ax.text(0.1, 0.7, f'{igbp}\n' + f'y = {slope}x + {intercept}\n' + '$R^2$: ' + f'{roundit(rvalue**2)}; '+ 'RMSE: ' + f'{rmse}', transform = ax.transAxes, fontsize = 10)

axes[-2].set_xlabel('EC GPP')
axes[3].set_ylabel('UFLUX-2 GPP')

google.download_file(fig, 'UFLUX-2_FLUXNET_val_IGBP.png')

lons = np.arange(-179.9, 179.9, 0.25)
lats = np.arange(-89.9, 89.9, 0.25)
grids = xr.DataArray(
    data=np.zeros([len(lons), len(lats)]),
    dims=["longitude", "latitude"],
    coords=dict(
        longitude=(["longitude"], lons),
        latitude=(["latitude"], lats),
    ),
    attrs=dict(
        description="0.25 grids.",
    ),
)

def load_WorldDem():
    p = root.joinpath('workspace/project_data/CARDAMOM/Global/WORLD_DEM-UCDAVIS/wc2.1_10m_elev.tif')
    rnc = rxr.open_rasterio(p, band_as_variable = True).rio.reproject("EPSG:4326")
    name_dict = dict(zip(rnc.keys(), ['DEM']))
    name_dict.update({'x': 'longitude', 'y': 'latitude'})
    rnc = rnc.rename(name_dict)
    rnc = rnc.where(rnc['DEM'] != -32768.)
    # nco = rnc.to_dataframe().loc[df_card.index, ['DEM']].to_xarray()['DEM'].sortby(['longitude'], ascending=True)
    nco = rnc.drop_vars('spatial_ref')[['DEM']]
    return nco

def load_Koppen():
    p = root.joinpath('workspace/datasets/Beck_KG_V1/Beck_KG_V1_present_0p0083.tif') # Beck_KG_V1_present_0p0083 Beck_KG_V1_present_0p5
    rnc = rxr.open_rasterio(p, band_as_variable = True).rio.reproject("EPSG:4326")
    name_dict = dict(zip(rnc.keys(), ['KOPPEN']))
    name_dict.update({'x': 'longitude', 'y': 'latitude'})
    rnc = rnc.rename(name_dict)
    # nco = rnc.to_dataframe().loc[df_card.index, ['KOPPEN']].to_xarray()['KOPPEN'].sortby(['longitude'], ascending=True)
    nco = rnc.drop_vars('spatial_ref')[['KOPPEN']]
    return nco

savefile_geo = root_proj.joinpath('1grid_inputs/geo_info.nc')
if savefile_geo.exists():
    nc_geor = xr.open_dataset(savefile_geo)
else:
    worlddem = load_WorldDem()
    koppen = load_Koppen()

    c4map = xr.open_dataset(root_proj_lucc.joinpath('C4map/C4_distribution_NUS_v2.2.nc')).rename({'lat': 'latitude', 'lon': 'longitude', 'years': 'time'})
    c4map = c4map['C4_area'].mean(dim = 'time')

    worlddem = worlddem.interp(latitude = grids.latitude, longitude = grids.longitude)
    koppen = koppen.interp(latitude = grids.latitude, longitude = grids.longitude, method = 'nearest')
    c4map = c4map.interp(latitude = grids.latitude, longitude = grids.longitude)
    c4map = c4map.transpose().to_dataset()

    nc_geor = xr.merge([worlddem, koppen, c4map])
    nc_geor.to_netcdf(savefile_geo)

savefile_co2 = root_proj.joinpath('1grid_inputs/co2_info.nc')
if savefile_co2.exists():
    co2r = xr.open_dataset(savefile_co2)
else:
    root_ct = root_proj_lucc.joinpath("carbontracker")
    co2r = []
    for p in root_ct.glob('*.nc'):
        co2t = xr.open_dataset(p)
        co2r.append(co2t)
    co2r = xr.merge(co2r)

    co2r = co2r.interp(latitude = grids.latitude, longitude = grids.longitude)
    co2r.to_netcdf(savefile_co2)

def get_grid_NIRv(p, FAPAR_ok = False):
    rnc = rxr.open_rasterio(p, band_as_variable = True)#.rio.reproject("EPSG:4326")
    assert (rnc.rio.crs == 'EPSG:4326'), rnc.rio.crs
    name_dict = dict(zip(rnc.keys(), ['R', 'NIR']))
    name_dict.update({'x': 'longitude', 'y': 'latitude'})
    rnc = rnc.rename(name_dict)
    R = rnc['R']
    NIR = rnc['NIR']
    if np.nanmean(NIR.data) > 100:
        NIR = NIR / 10000
        R = R / 10000
    NIRv = (NIR - R) / (NIR + R) * NIR
    NIRv.name = 'NIRv'
    if FAPAR_ok:
        EVI = 2.5 * (NIR - R) / (NIR + 2.4 * R + 1)
        FAPAR = (EVI - 0.1 + 0.1) * 1.25
        FAPAR.name = 'FAPAR'
        return NIRv, FAPAR
    else:
        return NIRv

def get_grid_IGBP(p):
    rnc = rxr.open_rasterio(p, band_as_variable = True)#.rio.reproject("EPSG:4326")
    assert (rnc.rio.crs == 'EPSG:4326'), rnc.rio.crs
    name_dict = dict(zip(rnc.keys(), ['IGBP']))
    name_dict.update({'x': 'longitude', 'y': 'latitude'})
    rnc = rnc.rename(name_dict)
    return rnc

from scigeo.meteo import saturation_vapor_pressure, specific_humidity2vapor_pressure
def get_grid_ERA5(p):
    rnc = rxr.open_rasterio(p, band_as_variable = True)#.rio.reproject("EPSG:4326")
    assert (rnc.rio.crs == 'EPSG:4326'), rnc.rio.crs
    name_dict = dict(zip(rnc.keys(), [
        'dewpoint_temperature_2m', 'temperature_2m', 'soil_temperature_level_1', 'snow_cover',
        'volumetric_soil_water_layer_1', 'forecast_albedo', 'surface_latent_heat_flux_sum', 'surface_sensible_heat_flux_sum',
        'surface_solar_radiation_downwards_sum', 'surface_thermal_radiation_downwards_sum',
        'evaporation_from_the_top_of_canopy_sum', 'surface_pressure', 'total_precipitation_sum',
        'temperature_2m_min', 'temperature_2m_max', 'u_component_of_wind_10m', 'v_component_of_wind_10m'
    ]))
    name_dict.update({'x': 'longitude', 'y': 'latitude'})
    rnc = rnc.rename(name_dict)
    rnc['dewpoint_temperature_2m'] -= 273.15
    rnc['temperature_2m'] -= 273.15
    rnc['soil_temperature_level_1'] -= 273.15

    rnc['surface_latent_heat_flux_sum'] /= 86400
    rnc['surface_sensible_heat_flux_sum'] /= 86400
    rnc['surface_solar_radiation_downwards_sum'] /= 86400
    rnc['surface_thermal_radiation_downwards_sum'] /= 86400

    rnc['surface_pressure'] /= 100

    rnc['temperature_2m_min'] -= 273.15
    rnc['temperature_2m_max'] -= 273.15

    rnc['VPD'] = saturation_vapor_pressure(rnc['temperature_2m']) - saturation_vapor_pressure(rnc['dewpoint_temperature_2m'])
    return rnc

df_path_MODIS = []
for p in root_proj.joinpath('1grid/GLOMODIS_025deg_monthly').glob('*.tif'):
    dt = p.stem
    dt = pd.to_datetime(dt, format = '%Y-%m-%d')
    df_path_MODIS.append([dt, p])
df_path_MODIS = pd.DataFrame(df_path_MODIS, columns = ['DATETIME', 'PATH']).set_index('DATETIME').sort_index()

df_path_LS5 = []
for p in root_proj.joinpath('1grid/GLOL5SR_025deg_monthly').glob('*.tif'):
    dt = p.stem
    dt = pd.to_datetime(dt, format = '%Y-%m-%d')
    df_path_LS5.append([dt, p])
df_path_LS5 = pd.DataFrame(df_path_LS5, columns = ['DATETIME', 'PATH']).set_index('DATETIME').sort_index()
df_path_LS5 = df_path_LS5[df_path_LS5.index.year < 2012]

df_path_LS7 = []
for p in root_proj.joinpath('1grid/GLOL7SR_025deg_monthly').glob('*.tif'):
    dt = p.stem
    dt = pd.to_datetime(dt, format = '%Y-%m-%d')
    df_path_LS7.append([dt, p])
df_path_LS7 = pd.DataFrame(df_path_LS7, columns = ['DATETIME', 'PATH']).set_index('DATETIME').sort_index()

df_path_LS8 = []
for p in root_proj.joinpath('1grid/GLOL8SR_025deg_monthly').glob('*.tif'):
    dt = p.stem
    dt = pd.to_datetime(dt, format = '%Y-%m-%d')
    df_path_LS8.append([dt, p])
df_path_LS8 = pd.DataFrame(df_path_LS8, columns = ['DATETIME', 'PATH']).set_index('DATETIME').sort_index()
df_path_LS8 = df_path_LS8[df_path_LS8.index.year > 2013]

df_path_S2 = []
for p in root_proj.joinpath('1grid/GLOS2SR_025deg_monthly').glob('*.tif'):
    dt = p.stem
    dt = pd.to_datetime(dt, format = '%Y-%m-%d')
    df_path_S2.append([dt, p])
df_path_S2 = pd.DataFrame(df_path_S2, columns = ['DATETIME', 'PATH']).set_index('DATETIME').sort_index()
df_path_S2 = df_path_S2[df_path_S2.index.year > 2018]

df_path_ERA5 = []
for p in root_proj.joinpath('1grid/GLOERA5_025deg_monthly').glob('*.tif'):
    dt = p.stem
    dt = pd.to_datetime(dt, format = '%Y-%m-%d')
    df_path_ERA5.append([dt, p])
df_path_ERA5 = pd.DataFrame(df_path_ERA5, columns = ['DATETIME', 'PATH']).set_index('DATETIME').sort_index()

df_path_LUCC = []
for p in root_proj.joinpath('1grid/GLOMODISLUCC_025deg_YEARLY').glob('*.tif'):
    dt = p.stem
    dt = pd.to_datetime(dt, format = '%Y-%m-%d')
    df_path_LUCC.append([dt, p])
df_path_LUCC = pd.DataFrame(df_path_LUCC, columns = ['DATETIME', 'PATH']).set_index('DATETIME').sort_index()

savefile_lucc = root_proj.joinpath('1grid_inputs/lucc_info.nc')
if savefile_lucc.exists():
    luccr = xr.open_dataset(savefile_lucc)
else:
    luccr = []
    for dt in df_path_LUCC.index:
        p = df_path_LUCC.loc[dt, 'PATH']
        lucc = get_grid_IGBP(p)
        lucc = lucc.interp(latitude = grids.latitude, longitude = grids.longitude, method = 'nearest')
        lucc = lucc.drop_vars('spatial_ref')
        lucc = lucc.expand_dims(time = [dt])
        luccr.append(lucc)
    luccr = xr.merge(luccr)
    luccr.to_netcdf(savefile_lucc)

# ============================================================================================================================
# sat_name = 'MODIS'; df_path_sat = df_path_MODIS
# sat_name = 'Landsat-5'; df_path_sat = df_path_LS5
# sat_name = 'Landsat-7'; df_path_sat = df_path_LS7
# sat_name = 'Landsat-8'; df_path_sat = df_path_LS8
# sat_name = 'Sentinel-2'; df_path_sat = df_path_S2

for sat_name, df_path_sat in [
    ['MODIS', df_path_MODIS],
    ['Landsat-7', df_path_LS7],
    ['Landsat-5', df_path_LS5],
    ['Landsat-8', df_path_LS8],
    ['Sentinel-2', df_path_S2]
]:
    print(sat_name)
    print('=' * 50)

    for yr, gp in df_path_sat.groupby(df_path_sat.index.year):
        savefile = root_proj.joinpath('1grid_inputs').joinpath(f'{sat_name}_{yr}.nc')
        if savefile.exists(): continue
        print(yr)
        if (yr < 2001) or (yr > 2022): continue # MODIS LUCC only 2001 - 2022

        nc_sat_year = []
        for dt in tqdm(gp.index):
            p = gp.loc[dt, 'PATH']

            nirv, fapar = get_grid_NIRv(p, FAPAR_ok = True)
            nirv = nirv.where((nirv >= 1e-9) & (fapar >= 1e-9), 0.0001)
            fapar = fapar.where((nirv >= 1e-9) & (fapar >= 1e-9), 0.0001)
            nirv = nirv.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
            fapar = fapar.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)

            nc_sat = xr.merge([nirv, fapar])
            nirv.close(); del(nirv); fapar.close(); del(fapar)
            nc_sat = nc_sat.interp(latitude = grids.latitude, longitude = grids.longitude)
            nc_sat = nc_sat.drop_vars('spatial_ref')
            nc_sat = nc_sat.expand_dims(time = [dt])

            nc_sat_year.append(nc_sat)

        nc_sat_year = xr.merge(nc_sat_year)
        nc_sat_year.to_netcdf(savefile)
    # ============================================================================================================================

df_path_sat = df_path_ERA5
for yr, gp in df_path_sat.groupby(df_path_ERA5.index.year):
    savefile = root_proj.joinpath('1grid_inputs').joinpath(f'ERA5_{yr}.nc')
    if savefile.exists(): continue
    if (yr < 2001) or (yr > 2022): continue # MODIS LUCC only 2001 - 2022
    print(yr)
    nc_era5_year = []
    for dt in tqdm(gp.index):
        p = gp.loc[dt, 'PATH']
        era5 = get_grid_ERA5(p)
        era5 = era5[['temperature_2m', 'surface_solar_radiation_downwards_sum', 'VPD', 'soil_temperature_level_1', 'surface_thermal_radiation_downwards_sum', 'total_precipitation_sum', 'surface_pressure']]

        era5 = era5.interp(latitude = grids.latitude, longitude = grids.longitude)
        era5 = era5.drop_vars('spatial_ref')
        era5 = era5.expand_dims(time = [dt])
        nc_era5_year.append(era5)
    nc_era5_year = xr.merge(nc_era5_year)
    nc_era5_year.to_netcdf(savefile)

# # 5 km

# from scieco import photosynthesis

# savefolder = root_proj.joinpath('2output_global_Pmodel-5km')
# # savefolder.mkdir()

# # sat_name = 'MODIS'; df_path_sat = df_path_MODIS
# # sat_name = 'Landsat-5'; df_path_sat = df_path_LS5
# # sat_name = 'Landsat-7'; df_path_sat = df_path_LS7
# # sat_name = 'Landsat-8'; df_path_sat = df_path_LS8
# # sat_name = 'Sentinel-2'; df_path_sat = df_path_S2

# for sat_name, df_path_sat in [
#     # ['MODIS', df_path_MODIS],
#     ['Landsat-5', df_path_LS5],
#     ['Landsat-7', df_path_LS7],
#     # ['Landsat-8', df_path_LS8],
#     # ['Sentinel-2', df_path_S2]
# ]:

#     if sat_name in ['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
#         nirv_name = 'NIRv_MODIS'
#     # elif sat_name in ['Landsat-5']:
#     #     nirv_name = 'NIRv_Landsat7'
#     else:
#         nirv_name = f"NIRv_{sat_name.replace('-', '')}"
#     regr = dd_regr[nirv_name]

#     for p_sat in root_proj.joinpath('1grid_inputs').glob(f'{sat_name}*.nc'):
#         dt = p_sat.stem.split('_')[1]
#         dt = pd.to_datetime(dt, format = '%Y')
#         if (dt.year < 2001) or (dt.year > 2022): continue # MODIS LUCC only 2001 - 2022
#         if dt.year != 2010: continue
#         savefile = savefolder.joinpath(f"{sat_name}_{dt.strftime('%Y')}.nc")
#         # if savefile.exists(): continue
#         print(dt)
#         p_era5 = root_proj.joinpath('1grid_inputs').joinpath(f'ERA5_{dt.year}.nc')

#         nc_sat = xr.open_dataset(p_sat)#.sel(time = dt)#.expand_dims(time=[dt])
#         nc_era5 = xr.open_dataset(p_era5)#.sel(time = dt).expand_dims(time=[dt])

#         # if sat_name in ['Landsat-5', 'Landsat-7']:
#         #     nc_sat['FAPAR'] = nc_sat['NIRv'] * 1.5

#         if sat_name in ['Landsat-5', 'Landsat-7']:
#             nc_sat['FAPAR'] = nc_sat['NIRv'] + 0.22

#         if sat_name == 'Landsat-5':
#             nc_sat = nc_sat * 1.25
#         if sat_name == 'Landsat-7':
#             nc_sat = nc_sat * 1.25
#         if sat_name == 'Landsat-8':
#             nc_sat = nc_sat * 1.25
#         elif sat_name == 'Sentinel-2':
#             nc_sat = nc_sat * 1.3

#         nc_geo = nc_geor.expand_dims(time=nc_sat.time)
#         lucc = luccr.sel(time = dt).expand_dims(time=nc_sat.time)
#         co2t = co2r.sel(time = nc_sat.time)

#         nct = xr.merge([nc_sat, nc_era5, co2t, lucc, nc_geo])

#         dft = nct.to_dataframe()#.interpolate().bfill()#.dropna()
#         dft = dft.reset_index().rename(columns = {'latitude': 'LAT', 'longitude': 'LON'})
#         dft = dft.rename(columns = {'C4_area': 'C4_RATIO'})
#         dft = dft.loc[dft['NIRv'].dropna().index, :]
#         dft = dft[~dft['IGBP'].isin([13, 15, 16, 17])] # also exclude 13 (urban) and 15 (snow)

#         # dft_cold = dft[dft['temperature_2m'] <= -30]
#         # dft = dft[dft['temperature_2m'] > -30]
#         dft.loc[dft['temperature_2m'] <= -30, 'temperature_2m'] = -30

#         tc = dft['temperature_2m'].values
#         co2 = dft['co2'].values
#         patm = dft['surface_pressure'].values * 100
#         vpd = dft['VPD'].values * 100
#         do_ftemp_kphio = True

#         ca = photosynthesis.calc_co2_to_ca(co2, patm)
#         c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = False, limitation_factors = 'wang17')
#         c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = True, limitation_factors = 'none')
#         # ----------------------------------------------------------------------------------------------------------------------------
#         dft['C3_LUE'] = c3_lue
#         dft['IWUE'] = iwue
#         dft['C4_LUE'] = c4_lue
#         # dft = pd.concat([dft, dft_cold], axis = 0)

#         dft['YEAR'] = dt.year
#         dft['MONTH'] = dt.month
#         dft['El-Nino-La-Nina'] = df_el.loc[dt.year, 'El-Nino-La-Nina']

#         index = dft[['LAT', 'LON', 'time']].rename(columns = {'LAT': 'latitude', 'LON': 'longitude'}).set_index(['latitude', 'longitude', 'time']).index
#         dft['C4_RATIO'] = dft['C4_RATIO'].fillna(0)
#         gppm = (dft['C3_LUE'] * (1 - dft['C4_RATIO'] / 100) + dft['C4_LUE']  * dft['C4_RATIO'] / 100) * dft['FAPAR'] * dft['surface_solar_radiation_downwards_sum'] * 2.3
#         slope = dd_fit[nirv_name]['slope']
#         intercept = dd_fit[nirv_name]['intercept']
#         gppm = (gppm - intercept) / slope

#         dfo = pd.concat([
#             pd.DataFrame(gppm.values, columns = ['GPPm'], index = index),
#             pd.DataFrame(regr.predict(dft[X_names + ['NIRv']]), index = index, columns = ['GPPd']),
#         ], axis = 1)
#         # dfo['IGBP'] = dft['IGBP'].values
#         # dfo['KOPPEN'] = dft['KOPPEN'].values
#         dfo = dfo#.dropna()

#         # ----------------------------------------------------------------------------------------------------
#         dfo['GPP'] = dfo['GPPm']
#         idxc = dfo[(dfo['GPPm'] < 0) & (dfo['GPPd'] >= 0)].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         # idxc = dfo[(dfo['GPPm'] > 0) & ((dfo['GPPd'] + dfo['GPPm']) - dfo['GPPm'] > dfo['GPPm'] * 0.5) & dfo.index.get_level_values('time').month.isin([6, 7, 8])].index
#         idxc = dfo[(dfo['GPPm'] > 0) & (dfo['GPPd'] > 0) & dfo.index.get_level_values('time').month.isin([7, 8])].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         idxc = dfo[
#             (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
#             dfo.index.get_level_values('time').month.isin([12, 1]) &\
#             (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5)
#         ].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         idxc = dfo[
#             (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
#             dfo.index.get_level_values('time').month.isin([2, 3, 10, 11]) &\
#             (dfo['GPPd'].abs() < dfo['GPPm'] * 0.3)
#         ].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         dfo.loc[dfo['GPP'] < 0, 'GPP'] = 1e-4
#         dfo = dfo[['GPP', 'GPPm']]
#         nco = dfo.to_xarray()
#         # ----------------------------------------------------------------------------------------------------
#         nco.to_netcdf(savefile)
#         print(savefile.name)
#     #     break
#     # break

# # 500 m

# from scieco import photosynthesis

# savefolder = root_proj.joinpath('2output_global_Pmodel-500m')
# # savefolder.mkdir()

# # sat_name = 'MODIS'; df_path_sat = df_path_MODIS
# # sat_name = 'Landsat-5'; df_path_sat = df_path_LS5
# # sat_name = 'Landsat-7'; df_path_sat = df_path_LS7
# # sat_name = 'Landsat-8'; df_path_sat = df_path_LS8
# # sat_name = 'Sentinel-2'; df_path_sat = df_path_S2

# for sat_name, df_path_sat in [
#     ['MODIS', df_path_MODIS],
#     ['Landsat-5', df_path_LS5],
#     ['Landsat-7', df_path_LS7],
#     ['Landsat-8', df_path_LS8],
#     ['Sentinel-2', df_path_S2]
# ]:

#     if sat_name in ['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
#         nirv_name = 'NIRv_MODIS'
#     # elif sat_name in ['Landsat-5']:
#     #     nirv_name = 'NIRv_Landsat7'
#     else:
#         nirv_name = f"NIRv_{sat_name.replace('-', '')}"
#     regr = dd_regr[nirv_name.split('_')[0] + '_' + use_res + '_' + nirv_name.split('_')[1]]

#     for p_sat in root_proj.joinpath('1grid_inputs').glob(f'{sat_name}*.nc'):
#         dt = p_sat.stem.split('_')[1]
#         dt = pd.to_datetime(dt, format = '%Y')
#         if (dt.year < 2001) or (dt.year > 2022): continue # MODIS LUCC only 2001 - 2022
#         # if dt.year != 2010: continue
#         savefile = savefolder.joinpath(f"{sat_name}_{dt.strftime('%Y')}.nc")
#         if savefile.exists(): continue
#         print(dt)
#         p_era5 = root_proj.joinpath('1grid_inputs').joinpath(f'ERA5_{dt.year}.nc')

#         nc_sat = xr.open_dataset(p_sat)#.sel(time = dt)#.expand_dims(time=[dt])
#         nc_era5 = xr.open_dataset(p_era5)#.sel(time = dt).expand_dims(time=[dt])

#         # if sat_name in ['Landsat-5', 'Landsat-7']:
#         #     nc_sat['FAPAR'] = nc_sat['NIRv'] * 1.5

#         if sat_name in ['Landsat-5', 'Landsat-7']:
#             nc_sat['FAPAR'] = nc_sat['NIRv'] + 0.22

#         if sat_name == 'Landsat-5':
#             nc_sat = nc_sat * 1.25
#         if sat_name == 'Landsat-7':
#             nc_sat = nc_sat * 1.25
#         if sat_name == 'Landsat-8':
#             nc_sat = nc_sat * 1.25
#         elif sat_name == 'Sentinel-2':
#             nc_sat = nc_sat * 1.3

#         nc_geo = nc_geor.expand_dims(time=nc_sat.time)
#         lucc = luccr.sel(time = dt).expand_dims(time=nc_sat.time)
#         co2t = co2r.sel(time = nc_sat.time)

#         nct = xr.merge([nc_sat, nc_era5, co2t, lucc, nc_geo])

#         dft = nct.to_dataframe()#.interpolate().bfill()#.dropna()
#         dft = dft.reset_index().rename(columns = {'latitude': 'LAT', 'longitude': 'LON'})
#         dft = dft.rename(columns = {'C4_area': 'C4_RATIO'})
#         dft = dft.loc[dft['NIRv'].dropna().index, :]
#         dft = dft[~dft['IGBP'].isin([13, 15, 16, 17])] # also exclude 13 (urban) and 15 (snow)

#         # dft_cold = dft[dft['temperature_2m'] <= -30]
#         # dft = dft[dft['temperature_2m'] > -30]
#         dft.loc[dft['temperature_2m'] <= -30, 'temperature_2m'] = -30

#         tc = dft['temperature_2m'].values
#         co2 = dft['co2'].values
#         patm = dft['surface_pressure'].values * 100
#         vpd = dft['VPD'].values * 100
#         do_ftemp_kphio = True

#         ca = photosynthesis.calc_co2_to_ca(co2, patm)
#         c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = False, limitation_factors = 'wang17')
#         c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = True, limitation_factors = 'none')
#         # ----------------------------------------------------------------------------------------------------------------------------
#         dft['C3_LUE'] = c3_lue
#         dft['IWUE'] = iwue
#         dft['C4_LUE'] = c4_lue
#         # dft = pd.concat([dft, dft_cold], axis = 0)

#         dft['YEAR'] = dt.year
#         dft['MONTH'] = dt.month
#         dft['El-Nino-La-Nina'] = df_el.loc[dt.year, 'El-Nino-La-Nina']

#         index = dft[['LAT', 'LON', 'time']].rename(columns = {'LAT': 'latitude', 'LON': 'longitude'}).set_index(['latitude', 'longitude', 'time']).index
#         dft['C4_RATIO'] = dft['C4_RATIO'].fillna(0)
#         gppm = (dft['C3_LUE'] * (1 - dft['C4_RATIO'] / 100) + dft['C4_LUE']  * dft['C4_RATIO'] / 100) * dft['FAPAR'] * dft['surface_solar_radiation_downwards_sum'] * 2.3
#         slope = dd_fit[nirv_name.split('_')[0] + '_' + use_res + '_' + nirv_name.split('_')[1]]['slope']
#         intercept = dd_fit[nirv_name.split('_')[0] + '_' + use_res + '_' + nirv_name.split('_')[1]]['intercept']
#         gppm = (gppm - intercept) / slope

#         dfo = pd.concat([
#             pd.DataFrame(gppm.values, columns = ['GPPm'], index = index),
#             pd.DataFrame(regr.predict(dft[X_names + ['NIRv']]), index = index, columns = ['GPPd']),
#         ], axis = 1)
#         # dfo['IGBP'] = dft['IGBP'].values
#         # dfo['KOPPEN'] = dft['KOPPEN'].values
#         dfo = dfo#.dropna()

#         # ----------------------------------------------------------------------------------------------------
#         dfo['GPP'] = dfo['GPPm']
#         idxc = dfo[(dfo['GPPm'] < 0) & (dfo['GPPd'] >= 0)].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         # idxc = dfo[(dfo['GPPm'] > 0) & ((dfo['GPPd'] + dfo['GPPm']) - dfo['GPPm'] > dfo['GPPm'] * 0.5) & dfo.index.get_level_values('time').month.isin([6, 7, 8])].index
#         idxc = dfo[(dfo['GPPm'] > 0) & (dfo['GPPd'] > 0) & dfo.index.get_level_values('time').month.isin([7, 8])].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         idxc = dfo[
#             (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
#             dfo.index.get_level_values('time').month.isin([12, 1]) &\
#             (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5)
#         ].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         idxc = dfo[
#             (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
#             dfo.index.get_level_values('time').month.isin([2, 3, 10, 11]) &\
#             (dfo['GPPd'].abs() < dfo['GPPm'] * 0.3)
#         ].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         dfo.loc[dfo['GPP'] < 0, 'GPP'] = 1e-4
#         dfo = dfo[['GPP', 'GPPm']]
#         nco = dfo.to_xarray()
#         # ----------------------------------------------------------------------------------------------------
#         nco.to_netcdf(savefile)
#         print(savefile.name)
#     #     break
#     # break

# # high

# from scieco import photosynthesis

# savefolder = root_proj.joinpath('2output_global_Pmodel-high')
# savefolder.mkdir()

# # sat_name = 'MODIS'; df_path_sat = df_path_MODIS
# # sat_name = 'Landsat-5'; df_path_sat = df_path_LS5
# # sat_name = 'Landsat-7'; df_path_sat = df_path_LS7
# # sat_name = 'Landsat-8'; df_path_sat = df_path_LS8
# # sat_name = 'Sentinel-2'; df_path_sat = df_path_S2

# for sat_name, df_path_sat in [
#     ['MODIS', df_path_MODIS],
#     ['Landsat-5', df_path_LS5],
#     ['Landsat-7', df_path_LS7],
#     ['Landsat-8', df_path_LS8],
#     ['Sentinel-2', df_path_S2]
# ]:

#     if sat_name in ['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
#         nirv_name = 'NIRv_MODIS'
#     # elif sat_name in ['Landsat-5']:
#     #     nirv_name = 'NIRv_Landsat7'
#     else:
#         nirv_name = f"NIRv_{sat_name.replace('-', '')}"
#     regr = dd_regr[nirv_name]

#     for p_sat in root_proj.joinpath('1grid_inputs').glob(f'{sat_name}*.nc'):
#         dt = p_sat.stem.split('_')[1]
#         dt = pd.to_datetime(dt, format = '%Y')
#         if (dt.year < 2001) or (dt.year > 2022): continue # MODIS LUCC only 2001 - 2022
#         # if dt.year != 2010: continue
#         savefile = savefolder.joinpath(f"{sat_name}_{dt.strftime('%Y')}.nc")
#         if savefile.exists(): continue
#         print(dt)
#         p_era5 = root_proj.joinpath('1grid_inputs').joinpath(f'ERA5_{dt.year}.nc')

#         nc_sat = xr.open_dataset(p_sat)#.sel(time = dt)#.expand_dims(time=[dt])
#         nc_era5 = xr.open_dataset(p_era5)#.sel(time = dt).expand_dims(time=[dt])

#         # if sat_name in ['Landsat-5', 'Landsat-7']:
#         #     nc_sat['FAPAR'] = nc_sat['NIRv'] * 1.5

#         if sat_name in ['Landsat-5', 'Landsat-7']:
#             nc_sat['FAPAR'] = nc_sat['NIRv'] + 0.22

#         if sat_name == 'Landsat-5':
#             nc_sat = nc_sat * 1.25
#         if sat_name == 'Landsat-7':
#             nc_sat = nc_sat * 1.25
#         if sat_name == 'Landsat-8':
#             nc_sat = nc_sat * 1.25
#         elif sat_name == 'Sentinel-2':
#             nc_sat = nc_sat * 1.3

#         nc_geo = nc_geor.expand_dims(time=nc_sat.time)
#         lucc = luccr.sel(time = dt).expand_dims(time=nc_sat.time)
#         co2t = co2r.sel(time = nc_sat.time)

#         nct = xr.merge([nc_sat, nc_era5, co2t, lucc, nc_geo])

#         dft = nct.to_dataframe()#.interpolate().bfill()#.dropna()
#         dft = dft.reset_index().rename(columns = {'latitude': 'LAT', 'longitude': 'LON'})
#         dft = dft.rename(columns = {'C4_area': 'C4_RATIO'})
#         dft = dft.loc[dft['NIRv'].dropna().index, :]
#         dft = dft[~dft['IGBP'].isin([13, 15, 16, 17])] # also exclude 13 (urban) and 15 (snow)

#         # dft_cold = dft[dft['temperature_2m'] <= -30]
#         # dft = dft[dft['temperature_2m'] > -30]
#         dft.loc[dft['temperature_2m'] <= -30, 'temperature_2m'] = -30

#         tc = dft['temperature_2m'].values
#         co2 = dft['co2'].values
#         patm = dft['surface_pressure'].values * 100
#         vpd = dft['VPD'].values * 100
#         do_ftemp_kphio = True

#         ca = photosynthesis.calc_co2_to_ca(co2, patm)
#         c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = False, limitation_factors = 'wang17')
#         c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = True, limitation_factors = 'none')
#         # ----------------------------------------------------------------------------------------------------------------------------
#         dft['C3_LUE'] = c3_lue
#         dft['IWUE'] = iwue
#         dft['C4_LUE'] = c4_lue
#         # dft = pd.concat([dft, dft_cold], axis = 0)

#         dft['YEAR'] = dt.year
#         dft['MONTH'] = dt.month
#         dft['El-Nino-La-Nina'] = df_el.loc[dt.year, 'El-Nino-La-Nina']

#         index = dft[['LAT', 'LON', 'time']].rename(columns = {'LAT': 'latitude', 'LON': 'longitude'}).set_index(['latitude', 'longitude', 'time']).index
#         dft['C4_RATIO'] = dft['C4_RATIO'].fillna(0)
#         gppm = (dft['C3_LUE'] * (1 - dft['C4_RATIO'] / 100) + dft['C4_LUE']  * dft['C4_RATIO'] / 100) * dft['FAPAR'] * dft['surface_solar_radiation_downwards_sum'] * 2.3
#         slope = dd_fit[nirv_name]['slope']
#         intercept = dd_fit[nirv_name]['intercept']
#         gppm = (gppm - intercept) / slope

#         dfo = pd.concat([
#             pd.DataFrame(gppm.values, columns = ['GPPm'], index = index),
#             pd.DataFrame(regr.predict(dft[X_names + ['NIRv']]), index = index, columns = ['GPPd']),
#         ], axis = 1)
#         # dfo['IGBP'] = dft['IGBP'].values
#         # dfo['KOPPEN'] = dft['KOPPEN'].values
#         dfo = dfo#.dropna()

#         # ----------------------------------------------------------------------------------------------------
#         dfo['GPP'] = dfo['GPPm']
#         idxc = dfo[(dfo['GPPm'] < 0) & (dfo['GPPd'] >= 0)].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         # idxc = dfo[(dfo['GPPm'] > 0) & ((dfo['GPPd'] + dfo['GPPm']) - dfo['GPPm'] > dfo['GPPm'] * 0.5) & dfo.index.get_level_values('time').month.isin([6, 7, 8])].index
#         idxc = dfo[(dfo['GPPm'] > 0) & (dfo['GPPd'] > 0) & dfo.index.get_level_values('time').month.isin([7, 8])].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         idxc = dfo[
#             (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
#             dfo.index.get_level_values('time').month.isin([12, 1]) &\
#             (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5)
#         ].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         idxc = dfo[
#             (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
#             dfo.index.get_level_values('time').month.isin([2, 3, 10, 11]) &\
#             (dfo['GPPd'].abs() < dfo['GPPm'] * 0.3)
#         ].index
#         dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

#         dfo.loc[dfo['GPP'] < 0, 'GPP'] = 1e-4
#         dfo = dfo[['GPP', 'GPPm']]
#         nco = dfo.to_xarray()
#         # ----------------------------------------------------------------------------------------------------
#         nco.to_netcdf(savefile)
#         print(savefile.name)
#     #     break
#     # break

# 5 km Landsat-7 only (model optimised for Landsat-7)

from scieco import photosynthesis

savefolder = root_proj.joinpath('2output_global_Pmodel_Landsat7-5km')
# savefolder.mkdir()

sat_name = 'Landsat-7'; df_path_sat = df_path_LS7
nirv_name = 'NIRv_Landsat7'
regr = dd_regr[nirv_name]
assert use_res == '5km', f'ERROR: use_res is: {use_res}'

for p_sat in root_proj.joinpath('1grid_inputs').glob(f'{sat_name}*.nc'):
    dt = p_sat.stem.split('_')[1]
    dt = pd.to_datetime(dt, format = '%Y')
    if (dt.year < 2001) or (dt.year > 2022): continue # MODIS LUCC only 2001 - 2022
    # if dt.year != 2010: continue
    savefile = savefolder.joinpath(f"{sat_name}_{dt.strftime('%Y')}.nc")
    if savefile.exists(): continue
    print(dt)
    p_era5 = root_proj.joinpath('1grid_inputs').joinpath(f'ERA5_{dt.year}.nc')

    nc_sat = xr.open_dataset(p_sat)#.sel(time = dt)#.expand_dims(time=[dt])
    nc_era5 = xr.open_dataset(p_era5)#.sel(time = dt).expand_dims(time=[dt])

    nc_sat = nc_sat / 1.6
    nc_geo = nc_geor.expand_dims(time=nc_sat.time)
    lucc = luccr.sel(time = dt).expand_dims(time=nc_sat.time)
    co2t = co2r.sel(time = nc_sat.time)

    nct = xr.merge([nc_sat, nc_era5, co2t, lucc, nc_geo])

    dft = nct.to_dataframe()#.interpolate().bfill()#.dropna()
    dft = dft.reset_index().rename(columns = {'latitude': 'LAT', 'longitude': 'LON'})
    dft = dft.rename(columns = {'C4_area': 'C4_RATIO'})
    dft = dft.loc[dft['NIRv'].dropna().index, :]
    dft = dft[~dft['IGBP'].isin([13, 15, 16, 17])] # also exclude 13 (urban) and 15 (snow)

    # dft_cold = dft[dft['temperature_2m'] <= -30]
    # dft = dft[dft['temperature_2m'] > -30]
    dft.loc[dft['temperature_2m'] <= -30, 'temperature_2m'] = -30

    tc = dft['temperature_2m'].values
    co2 = dft['co2'].values
    patm = dft['surface_pressure'].values * 100
    vpd = dft['VPD'].values * 100
    do_ftemp_kphio = True

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = True, limitation_factors = 'none')
    # ----------------------------------------------------------------------------------------------------------------------------
    dft['C3_LUE'] = c3_lue
    dft['IWUE'] = iwue
    dft['C4_LUE'] = c4_lue
    # dft = pd.concat([dft, dft_cold], axis = 0)

    dft['YEAR'] = dt.year
    dft['MONTH'] = dt.month
    dft['El-Nino-La-Nina'] = df_el.loc[dt.year, 'El-Nino-La-Nina']

    index = dft[['LAT', 'LON', 'time']].rename(columns = {'LAT': 'latitude', 'LON': 'longitude'}).set_index(['latitude', 'longitude', 'time']).index
    dft['C4_RATIO'] = dft['C4_RATIO'].fillna(0)
    gppm = (dft['C3_LUE'] * (1 - dft['C4_RATIO'] / 100) + dft['C4_LUE']  * dft['C4_RATIO'] / 100) * dft['FAPAR'] * dft['surface_solar_radiation_downwards_sum'] * 2.3
    slope = dd_fit[nirv_name]['slope']
    intercept = dd_fit[nirv_name]['intercept']
    gppm = (gppm - intercept) / slope

    dfo = pd.concat([
        pd.DataFrame(gppm.values, columns = ['GPPm'], index = index),
        pd.DataFrame(regr.predict(dft[X_names + ['NIRv']]), index = index, columns = ['GPPd']),
    ], axis = 1)
    # dfo['IGBP'] = dft['IGBP'].values
    # dfo['KOPPEN'] = dft['KOPPEN'].values
    dfo = dfo#.dropna()

    # ----------------------------------------------------------------------------------------------------
    dfo['GPP'] = dfo['GPPm']
    idxc = dfo[(dfo['GPPm'] < 0) & (dfo['GPPd'] >= 0)].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    # idxc = dfo[(dfo['GPPm'] > 0) & ((dfo['GPPd'] + dfo['GPPm']) - dfo['GPPm'] > dfo['GPPm'] * 0.5) & dfo.index.get_level_values('time').month.isin([6, 7, 8])].index
    idxc = dfo[(dfo['GPPm'] > 0) & (dfo['GPPd'] > 0) & dfo.index.get_level_values('time').month.isin([7, 8])].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    idxc = dfo[
        (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
        dfo.index.get_level_values('time').month.isin([12, 1]) &\
        (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5)
    ].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    idxc = dfo[
        (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
        dfo.index.get_level_values('time').month.isin([2, 3, 10, 11]) &\
        (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5)
    ].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    dfo.loc[dfo['GPP'] < 0, 'GPP'] = 1e-4
    dfo = dfo[['GPP', 'GPPm']]
    nco = dfo.to_xarray()
    # ----------------------------------------------------------------------------------------------------
    nco.to_netcdf(savefile)
    print(savefile.name)
    # break

# 500 m Landsat-7 only (model optimised for Landsat-7)

from scieco import photosynthesis

savefolder = root_proj.joinpath('2output_global_Pmodel_Landsat7-500m')
# savefolder.mkdir()

sat_name = 'Landsat-7'; df_path_sat = df_path_LS7
nirv_name = f'NIRv_{use_res}_Landsat7'
regr = dd_regr[nirv_name]
assert use_res == '500m', f'ERROR: use_res is: {use_res}'

for p_sat in root_proj.joinpath('1grid_inputs').glob(f'{sat_name}*.nc'):
    dt = p_sat.stem.split('_')[1]
    dt = pd.to_datetime(dt, format = '%Y')
    if (dt.year < 2001) or (dt.year > 2022): continue # MODIS LUCC only 2001 - 2022
    # if dt.year != 2010: continue
    savefile = savefolder.joinpath(f"{sat_name}_{dt.strftime('%Y')}.nc")
    if savefile.exists(): continue
    print(dt)
    p_era5 = root_proj.joinpath('1grid_inputs').joinpath(f'ERA5_{dt.year}.nc')

    nc_sat = xr.open_dataset(p_sat)#.sel(time = dt)#.expand_dims(time=[dt])
    nc_era5 = xr.open_dataset(p_era5)#.sel(time = dt).expand_dims(time=[dt])

    nc_sat = nc_sat / 1.6
    nc_geo = nc_geor.expand_dims(time=nc_sat.time)
    lucc = luccr.sel(time = dt).expand_dims(time=nc_sat.time)
    co2t = co2r.sel(time = nc_sat.time)

    nct = xr.merge([nc_sat, nc_era5, co2t, lucc, nc_geo])

    dft = nct.to_dataframe()#.interpolate().bfill()#.dropna()
    dft = dft.reset_index().rename(columns = {'latitude': 'LAT', 'longitude': 'LON'})
    dft = dft.rename(columns = {'C4_area': 'C4_RATIO'})
    dft = dft.loc[dft['NIRv'].dropna().index, :]
    dft = dft[~dft['IGBP'].isin([13, 15, 16, 17])] # also exclude 13 (urban) and 15 (snow)

    # dft_cold = dft[dft['temperature_2m'] <= -30]
    # dft = dft[dft['temperature_2m'] > -30]
    dft.loc[dft['temperature_2m'] <= -30, 'temperature_2m'] = -30

    tc = dft['temperature_2m'].values
    co2 = dft['co2'].values
    patm = dft['surface_pressure'].values * 100
    vpd = dft['VPD'].values * 100
    do_ftemp_kphio = True

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = True, limitation_factors = 'none')
    # ----------------------------------------------------------------------------------------------------------------------------
    dft['C3_LUE'] = c3_lue
    dft['IWUE'] = iwue
    dft['C4_LUE'] = c4_lue
    # dft = pd.concat([dft, dft_cold], axis = 0)

    dft['YEAR'] = dt.year
    dft['MONTH'] = dt.month
    dft['El-Nino-La-Nina'] = df_el.loc[dt.year, 'El-Nino-La-Nina']

    index = dft[['LAT', 'LON', 'time']].rename(columns = {'LAT': 'latitude', 'LON': 'longitude'}).set_index(['latitude', 'longitude', 'time']).index
    dft['C4_RATIO'] = dft['C4_RATIO'].fillna(0)
    gppm = (dft['C3_LUE'] * (1 - dft['C4_RATIO'] / 100) + dft['C4_LUE']  * dft['C4_RATIO'] / 100) * dft['FAPAR'] * dft['surface_solar_radiation_downwards_sum'] * 2.3
    slope = dd_fit[nirv_name]['slope']
    intercept = dd_fit[nirv_name]['intercept']
    gppm = (gppm - intercept) / slope

    dfo = pd.concat([
        pd.DataFrame(gppm.values, columns = ['GPPm'], index = index),
        pd.DataFrame(regr.predict(dft[X_names + ['NIRv']]), index = index, columns = ['GPPd']),
    ], axis = 1)
    # dfo['IGBP'] = dft['IGBP'].values
    # dfo['KOPPEN'] = dft['KOPPEN'].values
    dfo = dfo#.dropna()

    # ----------------------------------------------------------------------------------------------------
    dfo['GPP'] = dfo['GPPm']
    idxc = dfo[(dfo['GPPm'] < 0) & (dfo['GPPd'] >= 0)].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    # idxc = dfo[(dfo['GPPm'] > 0) & ((dfo['GPPd'] + dfo['GPPm']) - dfo['GPPm'] > dfo['GPPm'] * 0.5) & dfo.index.get_level_values('time').month.isin([6, 7, 8])].index
    idxc = dfo[(dfo['GPPm'] > 0) & (dfo['GPPd'] > 0) & dfo.index.get_level_values('time').month.isin([7, 8])].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    idxc = dfo[
        (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
        dfo.index.get_level_values('time').month.isin([12, 1]) &\
        (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5)
    ].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    idxc = dfo[
        (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
        dfo.index.get_level_values('time').month.isin([2, 3, 10, 11]) &\
        (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5)
    ].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    dfo.loc[dfo['GPP'] < 0, 'GPP'] = 1e-4
    dfo = dfo[['GPP', 'GPPm']]
    nco = dfo.to_xarray()
    # ----------------------------------------------------------------------------------------------------
    nco.to_netcdf(savefile)
    print(savefile.name)
    # break

# high Landsat-7 only (model optimised for Landsat-7)

from scieco import photosynthesis

savefolder = root_proj.joinpath('2output_global_Pmodel_Landsat7-high')
# savefolder.mkdir()

sat_name = 'Landsat-7'; df_path_sat = df_path_LS7
nirv_name = 'NIRv_Landsat7'
regr = dd_regr[nirv_name]
assert use_res == 'high', f'ERROR: use_res is: {use_res}'

for p_sat in root_proj.joinpath('1grid_inputs').glob(f'{sat_name}*.nc'):
    dt = p_sat.stem.split('_')[1]
    dt = pd.to_datetime(dt, format = '%Y')
    if (dt.year < 2001) or (dt.year > 2022): continue # MODIS LUCC only 2001 - 2022
    # if dt.year != 2010: continue
    savefile = savefolder.joinpath(f"{sat_name}_{dt.strftime('%Y')}.nc")
    if savefile.exists(): continue
    print(dt)
    p_era5 = root_proj.joinpath('1grid_inputs').joinpath(f'ERA5_{dt.year}.nc')

    nc_sat = xr.open_dataset(p_sat)#.sel(time = dt)#.expand_dims(time=[dt])
    nc_era5 = xr.open_dataset(p_era5)#.sel(time = dt).expand_dims(time=[dt])

    nc_sat = nc_sat / 1.6
    nc_geo = nc_geor.expand_dims(time=nc_sat.time)
    lucc = luccr.sel(time = dt).expand_dims(time=nc_sat.time)
    co2t = co2r.sel(time = nc_sat.time)

    nct = xr.merge([nc_sat, nc_era5, co2t, lucc, nc_geo])

    dft = nct.to_dataframe()#.interpolate().bfill()#.dropna()
    dft = dft.reset_index().rename(columns = {'latitude': 'LAT', 'longitude': 'LON'})
    dft = dft.rename(columns = {'C4_area': 'C4_RATIO'})
    dft = dft.loc[dft['NIRv'].dropna().index, :]
    dft = dft[~dft['IGBP'].isin([13, 15, 16, 17])] # also exclude 13 (urban) and 15 (snow)

    # dft_cold = dft[dft['temperature_2m'] <= -30]
    # dft = dft[dft['temperature_2m'] > -30]
    dft.loc[dft['temperature_2m'] <= -30, 'temperature_2m'] = -30

    tc = dft['temperature_2m'].values
    co2 = dft['co2'].values
    patm = dft['surface_pressure'].values * 100
    vpd = dft['VPD'].values * 100
    do_ftemp_kphio = True

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = True, limitation_factors = 'none')
    # ----------------------------------------------------------------------------------------------------------------------------
    dft['C3_LUE'] = c3_lue
    dft['IWUE'] = iwue
    dft['C4_LUE'] = c4_lue
    # dft = pd.concat([dft, dft_cold], axis = 0)

    dft['YEAR'] = dt.year
    dft['MONTH'] = dt.month
    dft['El-Nino-La-Nina'] = df_el.loc[dt.year, 'El-Nino-La-Nina']

    index = dft[['LAT', 'LON', 'time']].rename(columns = {'LAT': 'latitude', 'LON': 'longitude'}).set_index(['latitude', 'longitude', 'time']).index
    dft['C4_RATIO'] = dft['C4_RATIO'].fillna(0)
    gppm = (dft['C3_LUE'] * (1 - dft['C4_RATIO'] / 100) + dft['C4_LUE']  * dft['C4_RATIO'] / 100) * dft['FAPAR'] * dft['surface_solar_radiation_downwards_sum'] * 2.3
    slope = dd_fit[nirv_name]['slope']
    intercept = dd_fit[nirv_name]['intercept']
    gppm = (gppm - intercept) / slope

    dfo = pd.concat([
        pd.DataFrame(gppm.values, columns = ['GPPm'], index = index),
        pd.DataFrame(regr.predict(dft[X_names + ['NIRv']]), index = index, columns = ['GPPd']),
    ], axis = 1)
    # dfo['IGBP'] = dft['IGBP'].values
    # dfo['KOPPEN'] = dft['KOPPEN'].values
    dfo = dfo#.dropna()

    # ----------------------------------------------------------------------------------------------------
    dfo['GPP'] = dfo['GPPm']
    idxc = dfo[(dfo['GPPm'] < 0) & (dfo['GPPd'] >= 0)].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    # idxc = dfo[(dfo['GPPm'] > 0) & ((dfo['GPPd'] + dfo['GPPm']) - dfo['GPPm'] > dfo['GPPm'] * 0.5) & dfo.index.get_level_values('time').month.isin([6, 7, 8])].index
    idxc = dfo[(dfo['GPPm'] > 0) & (dfo['GPPd'] > 0) & dfo.index.get_level_values('time').month.isin([7, 8])].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    idxc = dfo[
        (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
        dfo.index.get_level_values('time').month.isin([12, 1]) &\
        (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5)
    ].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    idxc = dfo[
        (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
        dfo.index.get_level_values('time').month.isin([2, 3, 10, 11]) &\
        (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5)
    ].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    dfo.loc[dfo['GPP'] < 0, 'GPP'] = 1e-4
    dfo = dfo[['GPP', 'GPPm']]
    nco = dfo.to_xarray()
    # ----------------------------------------------------------------------------------------------------
    nco.to_netcdf(savefile)
    print(savefile.name)
    # break

# high Landsat-7 train model and applied to MODIS only

from scitbx import photosynthesis

savefolder = root_proj.joinpath('2output_global_Pmodel_Landsat7model2MODIS-high')
savefolder.mkdir()

sat_name = 'MODIS'; df_path_sat = df_path_LS7
nirv_name = 'NIRv_Landsat7'
regr = dd_regr[nirv_name]
assert use_res == 'high', f'ERROR: use_res is: {use_res}'

for p_sat in root_proj.joinpath('1grid_inputs').glob(f'{sat_name}*.nc'):
    dt = p_sat.stem.split('_')[1]
    dt = pd.to_datetime(dt, format = '%Y')
    if (dt.year < 2001) or (dt.year > 2022): continue # MODIS LUCC only 2001 - 2022
    # if dt.year != 2010: continue
    savefile = savefolder.joinpath(f"{sat_name}_{dt.strftime('%Y')}.nc")
    if savefile.exists(): continue
    print(dt)
    p_era5 = root_proj.joinpath('1grid_inputs').joinpath(f'ERA5_{dt.year}.nc')

    nc_sat = xr.open_dataset(p_sat)#.sel(time = dt)#.expand_dims(time=[dt])
    nc_era5 = xr.open_dataset(p_era5)#.sel(time = dt).expand_dims(time=[dt])

    nc_sat = nc_sat / 1.5 / 1.6
    nc_geo = nc_geor.expand_dims(time=nc_sat.time)
    lucc = luccr.sel(time = dt).expand_dims(time=nc_sat.time)
    co2t = co2r.sel(time = nc_sat.time)

    nct = xr.merge([nc_sat, nc_era5, co2t, lucc, nc_geo])

    dft = nct.to_dataframe()#.interpolate().bfill()#.dropna()
    dft = dft.reset_index().rename(columns = {'latitude': 'LAT', 'longitude': 'LON'})
    dft = dft.rename(columns = {'C4_area': 'C4_RATIO'})
    dft = dft.loc[dft['NIRv'].dropna().index, :]
    dft = dft[~dft['IGBP'].isin([13, 15, 16, 17])] # also exclude 13 (urban) and 15 (snow)

    # dft_cold = dft[dft['temperature_2m'] <= -30]
    # dft = dft[dft['temperature_2m'] > -30]
    dft.loc[dft['temperature_2m'] <= -30, 'temperature_2m'] = -30

    tc = dft['temperature_2m'].values
    co2 = dft['co2'].values
    patm = dft['surface_pressure'].values * 100
    vpd = dft['VPD'].values * 100
    do_ftemp_kphio = True

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = True, limitation_factors = 'none')
    # ----------------------------------------------------------------------------------------------------------------------------
    dft['C3_LUE'] = c3_lue
    dft['IWUE'] = iwue
    dft['C4_LUE'] = c4_lue
    # dft = pd.concat([dft, dft_cold], axis = 0)

    dft['YEAR'] = dt.year
    dft['MONTH'] = dt.month
    dft['El-Nino-La-Nina'] = df_el.loc[dt.year, 'El-Nino-La-Nina']

    index = dft[['LAT', 'LON', 'time']].rename(columns = {'LAT': 'latitude', 'LON': 'longitude'}).set_index(['latitude', 'longitude', 'time']).index
    dft['C4_RATIO'] = dft['C4_RATIO'].fillna(0)
    gppm = (dft['C3_LUE'] * (1 - dft['C4_RATIO'] / 100) + dft['C4_LUE']  * dft['C4_RATIO'] / 100) * dft['FAPAR'] * dft['surface_solar_radiation_downwards_sum'] * 2.3
    slope = dd_fit[nirv_name]['slope']
    intercept = dd_fit[nirv_name]['intercept']
    gppm = (gppm - intercept) / slope

    dfo = pd.concat([
        pd.DataFrame(gppm.values, columns = ['GPPm'], index = index),
        pd.DataFrame(regr.predict(dft[X_names + ['NIRv']]), index = index, columns = ['GPPd']),
    ], axis = 1)
    # dfo['IGBP'] = dft['IGBP'].values
    # dfo['KOPPEN'] = dft['KOPPEN'].values
    dfo = dfo#.dropna()

    # ----------------------------------------------------------------------------------------------------
    dfo['GPP'] = dfo['GPPm']
    idxc = dfo[(dfo['GPPm'] < 0) & (dfo['GPPd'] >= 0)].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    # idxc = dfo[(dfo['GPPm'] > 0) & ((dfo['GPPd'] + dfo['GPPm']) - dfo['GPPm'] > dfo['GPPm'] * 0.5) & dfo.index.get_level_values('time').month.isin([6, 7, 8])].index
    idxc = dfo[(dfo['GPPm'] > 0) & (dfo['GPPd'] > 0) & dfo.index.get_level_values('time').month.isin([7, 8])].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    idxc = dfo[
        (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
        dfo.index.get_level_values('time').month.isin([12, 1]) &\
        (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5)
    ].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    idxc = dfo[
        (dfo['GPPm'] > 0) & (dfo['GPPd'] < 0) & \
        dfo.index.get_level_values('time').month.isin([2, 3, 10, 11]) &\
        (dfo['GPPd'].abs() < dfo['GPPm'] * 0.5) # or 0.3 for applying MODIS model to MODIS
    ].index
    dfo.loc[idxc, 'GPP'] = dfo.loc[idxc, 'GPPm'] + dfo.loc[idxc, 'GPPd']

    dfo.loc[dfo['GPP'] < 0, 'GPP'] = 1e-4
    dfo = dfo[['GPP', 'GPPm']]
    nco = dfo.to_xarray()
    # ----------------------------------------------------------------------------------------------------
    nco.to_netcdf(savefile)
    print(savefile.name)
    # break

nct = xr.open_dataset(root_proj.joinpath('1grid_inputs').joinpath('Landsat-7_2010.nc'))['NIRv'].mean(dim = 'time') / xr.open_dataset(root_proj.joinpath('1grid_inputs').joinpath('MODIS_2010.nc'))['NIRv'].mean(dim = 'time')
nct.mean(dim = 'longitude').to_dataframe().plot()

# nct = nco.where(nco['GPP'] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
# dft = nct['GPP'].sum(dim = ['latitude', 'longitude']).drop_vars('spatial_ref').to_dataframe() * coef

# pd.concat([dft['GPP'].rename('Landsat-7'), GPP_modis[GPP_modis.index.year == 2010]['GPP'].rename('MODIS')], axis = 1).plot()
# dft.resample('1YS').mean(), GPP_modis[GPP_modis.index.year == 2010]['GPP'].resample('1YS').mean()

"""# Analysis"""

# @title global NIRv trend

sat_name = 'MODIS'
savefile = root_proj.joinpath(f'202505/{sat_name}_Global_NIRv.csv')

if savefile.exists():
    GPP_modis = pd.read_csv(savefile, index_col = 0)
    GPP_modis.index = pd.to_datetime(GPP_modis.index, format = '%Y-%m-%d')
else:
    dfv = []
    for p_sat in root_proj.joinpath('1grid_inputs').glob(f'{sat_name}*.nc'):
        nct = xr.open_dataset(p_sat)
        dfv.append(nct.mean(dim = ['latitude', 'longitude']).resample(time = '1YS').mean().to_dataframe())

    dfv = pd.concat(dfv, axis = 0)
    dfv.to_csv(savefile)

fig, ax = plt.subplots(figsize = (6, 4))
dfv['NIRv'].plot(ax = ax)
ax.set_ylim(0.07, 0.1)
del(savefile)

# p = root_proj.joinpath('2output_global_Pmodel3').joinpath(f'MODIS_2001.nc')
# nct = xr.open_dataset(p)

# nct = nct.where(nct['GPP'] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
# dft = nct['GPP'].sum(dim = ['latitude', 'longitude']).drop_vars('spatial_ref').to_dataframe() * coef
# print(dft.mean())
# dft

"""## IAV"""

# import calendar
# p = root_proj.joinpath('2output_global_Pmodel-high/MODIS_2010.nc')
# nct = xr.open_dataset(p)
# # nct.mean(dim = 'time')['GPPm'].plot(vmin = 0)
# ds = nct[['GPP']].rename({'longitude': 'lon', 'latitude': 'lat'}).transpose("time", "lat", "lon")

# # Earth radius and degree step in radians
# R = 6371000
# deg2rad = np.pi / 180
# delta = 0.25 * deg2rad  # 0.25 degrees in radians

# # Precompute latitude-dependent pixel area (1D)

# lats = ds['lat'].values  # shape: (3600,)
# ds.close()

# # Compute pixel area per latitude band (m²)
# pixel_area_per_lat = (R**2) * delta * delta * np.cos(lats * deg2rad)
# # Expand to 2D: (lat, lon)
# area_map = np.repeat(pixel_area_per_lat[:, np.newaxis], 1393, axis=1)

# # Global yearly GPP storage
# yearly_gpp_pg = {}

# gpp = ds['GPP']  # shape: [time, lat, lon], unit: gC/m²/day
# time_vals = pd.to_datetime(ds['time'].values)

# if 'land_fraction' in ds:
#     land_fraction = ds['land_fraction'].values  # shape: [lat, lon], 0~1
# else:
#     land_fraction = np.ones_like(gpp[0].values)

# monthly_total_g = 0.0

# for i, date in enumerate(time_vals):
#     year = date.year
#     days_in_month = calendar.monthrange(date.year, date.month)[1]

#     gpp_monthly = gpp[i, :, :].values  # gC/m²/day
#     gpp_total = gpp_monthly * days_in_month * area_map * land_fraction  # total gC per pixel
#     monthly_sum = np.nansum(gpp_total)  # sum over valid pixels
#     monthly_total_g += monthly_sum

# yearly_total_pg = monthly_total_g / 1e15  # convert to PgC
# yearly_gpp_pg[year] = yearly_gpp_pg.get(year, 0) + yearly_total_pg

# ds.close()

# # Save results
# years = sorted(yearly_gpp_pg.keys())
# gpp_values_pg = [yearly_gpp_pg[y] for y in years]
# gpp_values_pg

# 5 km IAV

savefile = root_proj.joinpath('2IAV-new/GPP_modis-5km.csv')
if savefile.exists():
    GPP_modis = pd.read_csv(savefile, index_col = 0)
    GPP_modis.index = pd.to_datetime(GPP_modis.index, format = '%Y-%m-%d')
else:
    GPP_modis = load_GPP('MODIS', root_proj.joinpath('2output_global_Pmodel-5km'), world)
    GPP_modis.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_ls5-5km.csv')
if savefile.exists():
    GPP_ls5 = pd.read_csv(savefile, index_col = 0)
    GPP_ls5.index = pd.to_datetime(GPP_ls5.index, format = '%Y-%m-%d')
else:
    GPP_ls5 = load_GPP('Landsat-5', root_proj.joinpath('2output_global_Pmodel-5km'), world)
    GPP_ls5.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_ls7-5km.csv')
if savefile.exists():
    GPP_ls7 = pd.read_csv(savefile, index_col = 0)
    GPP_ls7.index = pd.to_datetime(GPP_ls7.index, format = '%Y-%m-%d')
else:
    GPP_ls7 = load_GPP('Landsat-7', root_proj.joinpath('2output_global_Pmodel-5km'), world)
    GPP_ls7.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_ls8-5km.csv')
if savefile.exists():
    GPP_ls8 = pd.read_csv(savefile, index_col = 0)
    GPP_ls8.index = pd.to_datetime(GPP_ls8.index, format = '%Y-%m-%d')
else:
    GPP_ls8 = load_GPP('Landsat-8', root_proj.joinpath('2output_global_Pmodel-5km'), world)
    GPP_ls8.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_s2-5km.csv')
if savefile.exists():
    GPP_s2 = pd.read_csv(savefile, index_col = 0)
    GPP_s2.index = pd.to_datetime(GPP_s2.index, format = '%Y-%m-%d')
else:
    GPP_s2 = load_GPP('Sentinel-2', root_proj.joinpath('2output_global_Pmodel-5km'), world)
    GPP_s2.to_csv(savefile)

dfp_5kmr = pd.concat([
    GPP_modis.resample('1YS').mean()['GPP'].rename('MODIS'),
    GPP_ls5.resample('1YS').mean()['GPP'].rename('Landsat-5'),
    GPP_ls7.resample('1YS').mean()['GPP'].rename('Landsat-7'),
    GPP_ls8.resample('1YS').mean()['GPP'].rename('Landsat-8'),
    GPP_s2.resample('1YS').mean()['GPP'].rename('Sentinel-2')
], axis = 1)

dfp_5km = dfp_5kmr.copy()

# 500 m IAV

savefile = root_proj.joinpath('2IAV-new/GPP_modis-500m.csv')
if savefile.exists():
    GPP_modis = pd.read_csv(savefile, index_col = 0)
    GPP_modis.index = pd.to_datetime(GPP_modis.index, format = '%Y-%m-%d')
else:
    GPP_modis = load_GPP('MODIS', root_proj.joinpath('2output_global_Pmodel-500m'), world)
    GPP_modis.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_ls5-500m.csv')
if savefile.exists():
    GPP_ls5 = pd.read_csv(savefile, index_col = 0)
    GPP_ls5.index = pd.to_datetime(GPP_ls5.index, format = '%Y-%m-%d')
else:
    GPP_ls5 = load_GPP('Landsat-5', root_proj.joinpath('2output_global_Pmodel-500m'), world)
    GPP_ls5.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_ls7-500m.csv')
if savefile.exists():
    GPP_ls7 = pd.read_csv(savefile, index_col = 0)
    GPP_ls7.index = pd.to_datetime(GPP_ls7.index, format = '%Y-%m-%d')
else:
    GPP_ls7 = load_GPP('Landsat-7', root_proj.joinpath('2output_global_Pmodel-500m'), world)
    GPP_ls7.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_ls8-500m.csv')
if savefile.exists():
    GPP_ls8 = pd.read_csv(savefile, index_col = 0)
    GPP_ls8.index = pd.to_datetime(GPP_ls8.index, format = '%Y-%m-%d')
else:
    GPP_ls8 = load_GPP('Landsat-8', root_proj.joinpath('2output_global_Pmodel-500m'), world)
    GPP_ls8.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_s2-500m.csv')
if savefile.exists():
    GPP_s2 = pd.read_csv(savefile, index_col = 0)
    GPP_s2.index = pd.to_datetime(GPP_s2.index, format = '%Y-%m-%d')
else:
    GPP_s2 = load_GPP('Sentinel-2', root_proj.joinpath('2output_global_Pmodel-500m'), world)
    GPP_s2.to_csv(savefile)

dfp_500mr = pd.concat([
    GPP_modis.resample('1YS').mean()['GPP'].rename('MODIS'),
    GPP_ls5.resample('1YS').mean()['GPP'].rename('Landsat-5'),
    GPP_ls7.resample('1YS').mean()['GPP'].rename('Landsat-7'),
    GPP_ls8.resample('1YS').mean()['GPP'].rename('Landsat-8'),
    GPP_s2.resample('1YS').mean()['GPP'].rename('Sentinel-2')
], axis = 1)

dfp_500m = dfp_500mr.copy()

# high-res IAV

savefile = root_proj.joinpath('2IAV-new/GPP_modis-high.csv')
if savefile.exists():
    GPP_modis_high = pd.read_csv(savefile, index_col = 0)
    GPP_modis_high.index = pd.to_datetime(GPP_modis_high.index, format = '%Y-%m-%d')
else:
    GPP_modis_high = load_GPP('MODIS', root_proj.joinpath('2output_global_Pmodel-high'), world)
    GPP_modis_high.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_ls5-high.csv')
if savefile.exists():
    GPP_ls5_high = pd.read_csv(savefile, index_col = 0)
    GPP_ls5_high.index = pd.to_datetime(GPP_ls5_high.index, format = '%Y-%m-%d')
else:
    GPP_ls5_high = load_GPP('Landsat-5', root_proj.joinpath('2output_global_Pmodel-high'), world)
    GPP_ls5_high.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_ls7-high.csv')
if savefile.exists():
    GPP_ls7_high = pd.read_csv(savefile, index_col = 0)
    GPP_ls7_high.index = pd.to_datetime(GPP_ls7_high.index, format = '%Y-%m-%d')
else:
    GPP_ls7_high = load_GPP('Landsat-7', root_proj.joinpath('2output_global_Pmodel-high'), world)
    GPP_ls7_high.to_csv(savefile)
savefile = root_proj.joinpath('2IAV-new/GPP_ls8-high.csv')
if savefile.exists():
    GPP_ls8_high = pd.read_csv(savefile, index_col = 0)
    GPP_ls8_high.index = pd.to_datetime(GPP_ls8_high.index, format = '%Y-%m-%d')
else:
    GPP_ls8_high = load_GPP('Landsat-8', root_proj.joinpath('2output_global_Pmodel-high'), world)
    GPP_ls8_high.to_csv(savefile)

savefile = root_proj.joinpath('2IAV-new/GPP_s2-high.csv')
if savefile.exists():
    GPP_s2_high = pd.read_csv(savefile, index_col = 0)
    GPP_s2_high.index = pd.to_datetime(GPP_s2_high.index, format = '%Y-%m-%d')
else:
    GPP_s2_high = load_GPP('Sentinel-2', root_proj.joinpath('2output_global_Pmodel-high'), world)
    GPP_s2_high.to_csv(savefile)

dfp_highr = pd.concat([
    GPP_modis_high.resample('1YS').mean()['GPP'].rename('MODIS'),
    GPP_ls5_high.resample('1YS').mean()['GPP'].rename('Landsat-5'),
    GPP_ls7_high.resample('1YS').mean()['GPP'].rename('Landsat-7'),
    GPP_ls8_high.resample('1YS').mean()['GPP'].rename('Landsat-8'),
    GPP_s2_high.resample('1YS').mean()['GPP'].rename('Sentinel-2')
], axis = 1)

dfp_high = dfp_highr.copy()

# pd.concat([
#     dfp_5km['Landsat-7'].rename('5km'),
#     dfp_500m['Landsat-7'].rename('500m'),
#     dfp_high['Landsat-7'].rename('high')
# ], axis = 1).plot()

# # plot dfp_5km and dfp_high without optimisation for Landsat-7
# # just gives a show

# # figure 1: plot each satellite
# dfp_5km = dfp_5kmr.copy()
# dfp_high = dfp_highr.copy()

# dfp_5km = dfp_5km.rename(columns = {'MODIS': 'MODIS (coarse)', 'Landsat-5': 'Landsat-5 (coarse)', 'Landsat-7': 'Landsat-7 (coarse)', 'Landsat-8': 'Landsat-8 (coarse)', 'Sentinel-2': 'Sentinel-2 (coarse)'})
# dfp_high = dfp_high.rename(columns = {'MODIS': 'MODIS (high)', 'Landsat-5': 'Landsat-5 (high)', 'Landsat-7': 'Landsat-7 (high)', 'Landsat-8': 'Landsat-8 (high)', 'Sentinel-2': 'Sentinel-2 (high)'})

# fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
# dfp_5km.plot(ax = ax)
# dfp_high.plot(ax = ax, style = '--', color = colors, legend = True)

# for c in dfp_5km.columns:
#     ax.scatter(dfp_5km.index, dfp_5km[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
# for c in dfp_high.columns:
#     ax.scatter(dfp_high.index, dfp_high[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

# upper_legend(ax, ncols = 2, yloc = None)

# ax.set_ylabel('GPP ($Pg \ C$)')

# # google.download_file(fig, 'Global_GPP_avg.png')

# # # -------------------------------------------------------------------------------------------------------------------
# # figure 2: average satellite
# dfp_5km = dfp_5kmr.copy()
# dfp_high = dfp_highr.copy()

# dfp_5km['std'] = dfp_5km[['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']].std(axis = 1)
# dfp_5km = dfp_5km.rename(columns = {'MODIS': 'MODIS (5 km)', 'Landsat-7': 'High-resolution satellites (5 km)'})
# dfp_5km = dfp_5km[['MODIS (5 km)', 'High-resolution satellites (5 km)', 'std']]

# dfp_high['std'] = dfp_high[['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']].std(axis = 1)
# dfp_high = dfp_high.rename(columns = {'MODIS': 'MODIS (500 m)', 'Landsat-7': 'High-resolution satellites (30 m)'})
# dfp_high = dfp_high[['MODIS (500 m)', 'High-resolution satellites (30 m)', 'std']]

# fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

# dfp_5km[['MODIS (5 km)', 'High-resolution satellites (5 km)']].plot(ax = ax)
# dfp_high[['MODIS (500 m)', 'High-resolution satellites (30 m)']].plot(ax = ax, style = '--', color = colors, legend = True)

# for c in dfp_5km.columns.drop('std'):
#     ax.scatter(dfp_5km.index, dfp_5km[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
# for c in dfp_high.columns.drop('std'):
#     ax.scatter(dfp_high.index, dfp_high[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

# ax.fill_between(dfp_5km.index, dfp_5km['High-resolution satellites (5 km)'] - dfp_5km['std'], dfp_5km['High-resolution satellites (5 km)'] + dfp_5km['std'], color = colors[1], alpha = 0.1)
# ax.fill_between(dfp_high.index, dfp_high['High-resolution satellites (30 m)'] - dfp_high['std'], dfp_high['High-resolution satellites (30 m)'] + dfp_high['std'], color = colors[1], alpha = 0.1)

# upper_legend(ax, ncols = 2, yloc = 1.2)
# ax.set_xlabel('')
# ax.set_ylabel('GPP ($Pg \ C$)')

# # google.download_file(fig, 'Global_GPP_avg-v2.png')

# Model optimised for Landsat-7

savefile = root_proj.joinpath('2IAV-new/GPP_ls7-5km-trained_ls7.csv')
if savefile.exists():
    GPP_ls7_5km_t7 = pd.read_csv(savefile, index_col = 0)
    GPP_ls7_5km_t7.index = pd.to_datetime(GPP_ls7_5km_t7.index, format = '%Y-%m-%d')
else:
    GPP_ls7_5km_t7 = load_GPP('Landsat-7', root_proj.joinpath('2output_global_Pmodel_Landsat7-5km'), world)
    GPP_ls7_5km_t7.to_csv(savefile)

savefile = root_proj.joinpath('2IAV-new/GPP_ls7-500m-trained_ls7.csv')
if savefile.exists():
    GPP_ls7_500m_t7 = pd.read_csv(savefile, index_col = 0)
    GPP_ls7_500m_t7.index = pd.to_datetime(GPP_ls7_500m_t7.index, format = '%Y-%m-%d')
else:
    GPP_ls7_500m_t7 = load_GPP('Landsat-7', root_proj.joinpath('2output_global_Pmodel_Landsat7-500m'), world)
    GPP_ls7_500m_t7.to_csv(savefile)

savefile = root_proj.joinpath('2IAV-new/GPP_ls7-high-trained_ls7.csv')
if savefile.exists():
    GPP_ls7_high_t7 = pd.read_csv(savefile, index_col = 0)
    GPP_ls7_high_t7.index = pd.to_datetime(GPP_ls7_high_t7.index, format = '%Y-%m-%d')
else:
    GPP_ls7_high_t7 = load_GPP('Landsat-7', root_proj.joinpath('2output_global_Pmodel_Landsat7-high'), world)
    GPP_ls7_high_t7.to_csv(savefile)


savefile = root_proj.joinpath('2IAV-new/GPP_ls7-500m.csv')
if savefile.exists():
    GPP_ls7 = pd.read_csv(savefile, index_col = 0)
    GPP_ls7.index = pd.to_datetime(GPP_ls7.index, format = '%Y-%m-%d')
else:
    GPP_ls7 = load_GPP('Landsat-7', root_proj.joinpath('2output_global_Pmodel-500m'), world)
    GPP_ls7.to_csv(savefile)


dfp_train7r = pd.concat([
    GPP_ls7_5km_t7.resample('1YS').mean()['GPP'].rename('Landsat-7_5km'),
    # GPP_ls7_500m_t7.resample('1YS').mean()['GPP'].rename('Landsat-7_500m'),
    GPP_ls7.resample('1YS').mean()['GPP'].rename('Landsat-7_500m'),
    GPP_ls7_high_t7.resample('1YS').mean()['GPP'].rename('Landsat-7_high'),
], axis = 1)

dfp_train7 = dfp_train7r.copy()
# (dfp_train7['Landsat-7_5km'] - dfp_train7['Landsat-7_high']).mean()
# # compare original and optimised Landsat-7
# pd.concat([dfp_highr['Landsat-7'], dfp_train7], axis = 1).plot()

# Model trained with Landsat-7 but applied to MODIS

savefile = root_proj.joinpath('2IAV-new/GPP_ls7modis-high.csv')
if savefile.exists():
    GPP_ls7modis_high = pd.read_csv(savefile, index_col = 0)
    GPP_ls7modis_high.index = pd.to_datetime(GPP_ls7modis_high.index, format = '%Y-%m-%d')
else:
    GPP_ls7modis_high = load_GPP('MODIS', root_proj.joinpath('2output_global_Pmodel_Landsat7model2MODIS-high'), world)
    GPP_ls7modis_high.to_csv(savefile)

dfp_truth = GPP_ls7modis_high.resample('1YS').mean()['GPP'].rename('MODIST7').to_frame()

# IAV from literature

# ------------------------------------------------------------------------------
# # FLUXSAT
savefile = root_proj.joinpath('2IAV-new/GPP_fluxsat.csv')
if savefile.exists():
    df_fluxsat = pd.read_csv(savefile, index_col = 0)
    df_fluxsat.index = pd.to_datetime(df_fluxsat.index, format = '%Y-%m-%d')
else:
    df_fluxsat = []
    for p in root.joinpath('workspace/project_data2/upscaling/dataset/fluxsat_monthly-025').glob('*.nc'):
        # print(p.stem)
        nc_fs = xr.open_dataset(p)

        nc_fs = nc_fs.where(nc_fs['GPP'] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
        # nc_fs = nc_fs.resample(time = '1YS').mean()
        df_fs = nc_fs['GPP'].sum(dim = ['latitude', 'longitude']).drop_vars('spatial_ref').to_dataframe() * coef
        df_fluxsat.append(df_fs)

    df_fluxsat = pd.concat(df_fluxsat, axis = 0)
    df_fluxsat = df_fluxsat.sort_index()
    df_fluxsat = df_fluxsat[df_fluxsat.index.year > 2000]
    df_fluxsat = df_fluxsat[df_fluxsat.index.year < 2020]
    df_fluxsat.to_csv(savefile)

# ------------------------------------------------------------------------------
# # CARDAMOM
savefile = root_proj.joinpath('2IAV-new/GPP_cardamom.csv')
if savefile.exists():
    df_cardamom = pd.read_csv(savefile, index_col = 0)
    df_cardamom.index = pd.to_datetime(df_cardamom.index, format = '%Y-%m-%d')

    card_gpp = df_cardamom[['GPP']]
    card_gpp_025 = df_cardamom['gpp_2.5pc']
    card_gpp_975 = df_cardamom['gpp_97.5pc']
else:
    df_path_card = []
    for p in root.joinpath("workspace/project_data/CARDAMOM/Global").joinpath('CARDAMOM_Trendy12/').glob('*.nc'):
        _, sv = p.stem.split('CARDAMOM_')
        df_path_card.append([sv, p])

    df_path_card = pd.DataFrame(df_path_card, columns = ['PROD', 'PATH']).set_index('PROD')
    # df_path_card.index

    p = df_path_card.loc['S3_gpp', 'PATH']
    card = xr.open_dataset(p)
    card = card.assign_coords({
        "time": [pd.to_datetime('2003-01-01', format = '%Y-%m-%d') + pd.DateOffset(months = i) for i in card.time.data - 1]
    })
    print(list(card.keys()))
    card_gpp = card['gpp'].sum(dim = ['latitude', 'longitude']).to_series().resample('1YS').mean() * 86400 * 365 * 1e5 * 1e5 / 1e15 * 1000
    card_gpp_025 = card['gpp_2.5pc'].sum(dim = ['latitude', 'longitude']).to_series().resample('1YS').mean() * 86400 * 365 * 1e5 * 1e5 / 1e15 * 1000
    card_gpp_975 = card['gpp_97.5pc'].sum(dim = ['latitude', 'longitude']).to_series().resample('1YS').mean() * 86400 * 365 * 1e5 * 1e5 / 1e15 * 1000
    card_gpp = card_gpp.rename('GPP').to_frame()
    df_cardamom = pd.concat([card_gpp, card_gpp_025, card_gpp_975], axis = 1)
    df_cardamom.to_csv(savefile)
    # card_gpp.plot()

# ------------------------------------------------------------------------------
# # Trendy v12, see: Trendy-emulator-v1.1.ipynb
df_trendy = pd.read_csv(root_proj.joinpath('2IAV-new/GPP_Trendyv12.csv'), index_col = 0)
df_trendy.index = pd.to_datetime(df_trendy.index, format = '%Y-%m-%d')
# ------------------------------------------------------------------------------

# ==============================================================================
# CARDAMOM aggregation v2 considering varying size of lat and lon

savefile = root_proj.joinpath('2IAV-new/GPP_cardamom_v2.csv')
if savefile.exists():
    df_cardamom2 = pd.read_csv(savefile, index_col = 0)
    df_cardamom2.index = pd.to_datetime(df_cardamom2.index, format = '%Y-%m-%d')

    card_gpp2 = df_cardamom2[['GPP']]
    card_gpp_025_2 = df_cardamom2['gpp_2.5pc']
    card_gpp_975_2 = df_cardamom2['gpp_97.5pc']
else:
    df_path_card = []
    for p in root.joinpath("workspace/project_data/CARDAMOM/Global").joinpath('CARDAMOM_Trendy12/').glob('*.nc'):
        _, sv = p.stem.split('CARDAMOM_')
        df_path_card.append([sv, p])

    df_path_card = pd.DataFrame(df_path_card, columns = ['PROD', 'PATH']).set_index('PROD')

    p = df_path_card.loc['S3_gpp', 'PATH']
    card = xr.open_dataset(p)
    card = card.assign_coords({
        "time": [pd.to_datetime('2003-01-01', format = '%Y-%m-%d') + pd.DateOffset(months = i) for i in card.time.data - 1]
    })

    coef_mat = xr.DataArray(
        deg2m(card.longitude, card.latitude, 1, 1),
        dims = ['latitude', 'longitude'],
        coords = {'longitude': card.longitude, 'latitude': card.latitude}
    ).expand_dims(time = card.time)

    df_cardamom2 = (card[['gpp_2.5pc', 'gpp', 'gpp_97.5pc']] * 1000 * 86400 * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe().resample('1YS').mean() / coef_PgC_gC * 365
    df_cardamom2 = df_cardamom2.rename(columns = {'gpp': 'GPP'})
    df_cardamom2.to_csv(savefile)
    del(card, coef_mat, savefile)

# ==============================================================================
# FluxSat aggregation v2 considering varying size of lat and lon

savefile = root_proj.joinpath('2IAV-new/GPP_fluxsat_v2.csv')
if savefile.exists():
    df_fluxsat2 = pd.read_csv(savefile, index_col = 0)
    df_fluxsat2.index = pd.to_datetime(df_fluxsat2.index, format = '%Y-%m-%d')
else:
    nc_fs_a = []
    for p in root.joinpath('workspace/project_data2/upscaling/dataset/fluxsat_monthly-025').glob('*.nc'):
        # yr = int((p.stem.split('_'))[-1])
        # if yr != 2010: continue
        nc_fs = xr.open_dataset(p)
        nc_fs = nc_fs.resample(time = '1YS').mean()
        nc_fs_a.append(nc_fs)
    nc_fs_a = xr.merge(nc_fs_a)
    del(nc_fs)

    coef_mat = xr.DataArray(
        deg2m(nc_fs_a.longitude, nc_fs_a.latitude, 0.25, 0.25),
        dims = ['latitude', 'longitude'],
        coords = {'longitude': nc_fs_a.longitude, 'latitude': nc_fs_a.latitude}
    ).expand_dims(time = nc_fs_a.time)

    df_fluxsat2 = (nc_fs_a * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe().resample('1YS').mean() / coef_PgC_gC * 365
    df_fluxsat2 = df_fluxsat2[df_fluxsat2.index.year > 2000]
    df_fluxsat2 = df_fluxsat2[df_fluxsat2.index.year < 2020]
    df_fluxsat2.to_csv(savefile)
    del(nc_fs_a, coef_mat, savefile)

# ==============================================================================

# # combine FLUXSAT and CARDAMOM
dfp_litr = pd.concat([
    GPP_modis_high.resample('1YS').mean()['GPP'].rename('UFLUXv2'),
    df_fluxsat.resample('1YS').mean()['GPP'].rename('Machine Learning (literature)'),
    card_gpp['GPP'].rename('Data Assimilation'),
    df_trendy['GPP(t)'].rename('Trendy'),
], axis = 1)

dfp_lit = dfp_litr.copy()

# ------------------------------------------------------------------------------
# # plot
# fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
# dfp_lit.plot(ax = ax, color = [nature_colors[0], nature_colors[1], nature_colors[5]])
# for c in dfp_lit.columns:
#     ax.scatter(dfp_lit.index, dfp_lit[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
# ax.set_xlabel('')
# upper_legend(ax, ncols = 2, yloc = None)

# ax.set_ylabel('GPP ($Pg \ C$)')

# # google.download_file(fig, 'UFLUXv2FluxSatCARD.png')

# # df_path_card = []
# # for p in root.joinpath("workspace/project_data/CARDAMOM/Global").joinpath('CARDAMOM_Trendy12/').glob('*.nc'):
# #     _, sv = p.stem.split('CARDAMOM_')
# #     df_path_card.append([sv, p])

# # df_path_card = pd.DataFrame(df_path_card, columns = ['PROD', 'PATH']).set_index('PROD')
# # # df_path_card.index

# # p = df_path_card.loc['S2_gpp', 'PATH']
# # card = xr.open_dataset(p)
# # card = card.assign_coords({
# #     "time": [pd.to_datetime('2003-01-01', format = '%Y-%m-%d') + pd.DateOffset(months = i) for i in card.time.data - 1]
# # })
# # print(list(card.keys()))
# # card_gpp = card['gpp'].sum(dim = ['latitude', 'longitude']).to_series().resample('1YS').mean() * 86400 * 365 * 1e5 * 1e5 / 1e15 * 1000
# # card_gpp_025 = card['gpp_2.5pc'].sum(dim = ['latitude', 'longitude']).to_series().resample('1YS').mean() * 86400 * 365 * 1e5 * 1e5 / 1e15 * 1000
# # card_gpp_975 = card['gpp_97.5pc'].sum(dim = ['latitude', 'longitude']).to_series().resample('1YS').mean() * 86400 * 365 * 1e5 * 1e5 / 1e15 * 1000
# # card_gpp = card_gpp.rename('GPP').to_frame()
# # df_cardamom = pd.concat([card_gpp, card_gpp_025, card_gpp_975], axis = 1)

# # df_cardamomS2 = df_cardamom

# pd.concat([df_cardamomS2['GPP'].rename('S2'), df_cardamomS3['GPP'].rename('S3')], axis = 1).plot()

"""### IAV plot old versions"""

# @title IAV plot ver 1

dfp_5km = dfp_5kmr.copy()
dfp_high = dfp_highr.copy()

savefile = root.joinpath("workspace/project_data/UFLUX-ensemble").joinpath(f'UFLUX-GPP.csv')
df_nt_mn = pd.read_csv(savefile, index_col = 0).pivot(index = 'YEAR', columns = 'PROD', values = 'VAL')
df_nt_mn.index = pd.to_datetime(df_nt_mn.index, format = '%Y')
df_nt_mn = df_nt_mn[['MODIS-NIRv-ERA5-NT', 'MODIS-NIRv-ERA5-WY']].mean(axis = 1)
df_nt_mn = df_nt_mn.to_frame('Machine Learning (UFLUXv1)')

dfp_5km['std'] = dfp_5km[['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']].std(axis = 1)
dfp_5km = dfp_5km.rename(columns = {'MODIS': 'MODIS (5 km)', 'Landsat-7': 'Landsats/Sentinel-2 (5 km)'})
dfp_5km = dfp_5km[['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)', 'std']]

dfp_high['std'] = dfp_high[['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']].std(axis = 1)
dfp_high = dfp_high.rename(columns = {'MODIS': 'MODIS (500 m)', 'Landsat-7': 'Landsats/Sentinel-2 (30 m)'})
dfp_high = dfp_high[['MODIS (500 m)', 'Landsats/Sentinel-2 (30 m)', 'std']]

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

dfp_5km[['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)']].plot(ax = ax)
dfp_high[['MODIS (500 m)']].plot(ax = ax, style = '--', color = colors, legend = True)
# dfp_high[['Landsats/Sentinel-2 (30 m)']].plot(ax = ax, style = '--', color = colors[1], legend = True)
dfp_train7['Landsat-7_high'].rename('Landsat-7 (30 m)').plot(ax = ax, style = '--', color = colors[1], legend = True)

for c in dfp_5km.columns.drop('std'):
    ax.scatter(dfp_5km.index, dfp_5km[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
for c in ['MODIS (500 m)']:
    ax.scatter(dfp_high.index, dfp_high[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
for c in ['Landsat-7_high']:
    ax.scatter(dfp_train7.index, dfp_train7[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

ax.fill_between(dfp_5km.index, dfp_5km['Landsats/Sentinel-2 (5 km)'] - dfp_5km['std'], dfp_5km['Landsats/Sentinel-2 (5 km)'] + dfp_5km['std'], color = colors[1], alpha = 0.1)

# dfp_lit.drop('UFLUXv2', axis = 1)
dfp_lit[['Machine Learning (literature)', 'Data Assimilation']].plot(ax = ax, color = [colors[5], colors[6]])
for c in ['Machine Learning (literature)', 'Data Assimilation']:
    ax.scatter(dfp_lit.index, dfp_lit[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
ax.fill_between(card_gpp_025.index, card_gpp_025, card_gpp_975, color = colors[6], alpha = 0.1)

df_nt_mn.plot(ax = ax, style = '--', color = [colors[5]]) # UFLUXv1
for c in df_nt_mn.columns:
    ax.scatter(df_nt_mn.index, df_nt_mn[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

dfp_truth = dfp_truth.rename(columns = {'MODIST7': 'MODIS (30 m)'})
dfp_truth.plot(ax = ax, style = '--', color = colors[2], legend = True)
for c in dfp_truth.columns:
    ax.scatter(dfp_truth.index, dfp_truth[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

upper_legend(ax, ncols = 2, yloc = 1.3, user_labels_order = ['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)', 'Machine Learning (literature)', 'Data Assimilation', 'MODIS (500 m)', 'Landsat-7 (30 m)', 'Machine Learning (UFLUXv1)', 'MODIS (30 m)'])
ax.set_xlabel('')
ax.set_ylabel('GPP ($Pg \ C$)')

# google.download_file(fig, 'Global_GPP_avg-v5.pdf')

# @title trend

dft = pd.concat([dfp_lit, df_nt_mn], axis = 1).drop('Trendy', axis = 1)
print((dft.T['2021-01-01'] - dft.T['2010-01-01']) / (2021 - 2010))
print('=' * 100)
print((dft.T['2008-01-01'] - dft.T['2003-01-01']) / (2008 - 2003))

dftt = dft[['Machine Learning (literature)', 'Machine Learning (UFLUXv1)']].dropna()
print(stats.linregress(dftt['Machine Learning (literature)'], dftt['Machine Learning (UFLUXv1)']))

roundit(dfp_high['std']['2015'::].mean()), roundit(dfp_truth.mean().values[0]), roundit((dfp_truth.values[-1] - dfp_truth.values[0])[0] / len(dfp_truth))

# @title IAV plot ver 2

dfp_5km = dfp_5kmr.copy()
dfp_high = dfp_highr.copy()

savefile = root.joinpath("workspace/project_data/UFLUX-ensemble").joinpath(f'UFLUX-GPP.csv')
df_nt_mn = pd.read_csv(savefile, index_col = 0).pivot(index = 'YEAR', columns = 'PROD', values = 'VAL')
df_nt_mn.index = pd.to_datetime(df_nt_mn.index, format = '%Y')
df_nt_mn = df_nt_mn[['MODIS-NIRv-ERA5-NT', 'MODIS-NIRv-ERA5-WY']].mean(axis = 1)
df_nt_mn = df_nt_mn.to_frame('Machine Learning (UFLUXv1)')

dfp_5km['std'] = dfp_5km[['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']].std(axis = 1)
dfp_5km = dfp_5km.rename(columns = {'MODIS': 'MODIS (5 km)', 'Landsat-7': 'Landsats/Sentinel-2 (5 km)'})
dfp_5km = dfp_5km[['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)', 'std']]

dfp_high['std'] = dfp_high[['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']].std(axis = 1)
dfp_high = dfp_high.rename(columns = {'MODIS': 'MODIS (500 m)', 'Landsat-7': 'Landsats/Sentinel-2 (30 m)'})
dfp_high = dfp_high[['MODIS (500 m)', 'Landsats/Sentinel-2 (30 m)', 'std']]

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

dfp_5km[['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)']].plot(ax = ax)
dfp_high[['MODIS (500 m)']].plot(ax = ax, style = '--', color = colors, legend = True)
# dfp_high[['Landsats/Sentinel-2 (30 m)']].plot(ax = ax, style = '--', color = colors[1], legend = True)
dfp_train7['Landsat-7_high'].rename('Landsat-7 (30 m)').plot(ax = ax, style = '--', color = colors[1], legend = True)

for c in dfp_5km.columns.drop('std'):
    ax.scatter(dfp_5km.index, dfp_5km[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
for c in ['MODIS (500 m)']:
    ax.scatter(dfp_high.index, dfp_high[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
for c in ['Landsat-7_high']:
    ax.scatter(dfp_train7.index, dfp_train7[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

ax.fill_between(dfp_5km.index, dfp_5km['Landsats/Sentinel-2 (5 km)'] - dfp_5km['std'], dfp_5km['Landsats/Sentinel-2 (5 km)'] + dfp_5km['std'], color = colors[1], alpha = 0.1)

# dfp_lit.drop('UFLUXv2', axis = 1)
dfp_lit[['Machine Learning (literature)', 'Trendy']].plot(ax = ax, color = [colors[5], colors[6]])
for c in ['Machine Learning (literature)', 'Trendy']:
    ax.scatter(dfp_lit.index, dfp_lit[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
ax.fill_between(df_trendy.index, df_trendy['GPP(t)_25th'], df_trendy['GPP(t)_75th'], color = colors[6], alpha = 0.1)

df_nt_mn.plot(ax = ax, style = '--', color = [colors[5]]) # UFLUXv1
for c in df_nt_mn.columns:
    ax.scatter(df_nt_mn.index, df_nt_mn[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

dfp_truth = dfp_truth.rename(columns = {'MODIST7': 'MODIS (30 m)'})
dfp_truth.plot(ax = ax, style = '--', color = colors[2], legend = True)
for c in dfp_truth.columns:
    ax.scatter(dfp_truth.index, dfp_truth[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

upper_legend(ax, ncols = 2, yloc = 1.3, user_labels_order = ['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)', 'Machine Learning (literature)', 'Trendy', 'MODIS (500 m)', 'Landsat-7 (30 m)', 'Machine Learning (UFLUXv1)', 'MODIS (30 m)'])
ax.set_xlabel('')
ax.set_ylabel('GPP ($Pg \ C$)')

# google.download_file(fig, 'Global_GPP_avg-v5.pdf')

# @title IAV plot ver 3

dfp_5km = dfp_5kmr.copy()
dfp_high = dfp_highr.copy()

savefile = root.joinpath("workspace/project_data/UFLUX-ensemble").joinpath(f'UFLUX-GPP.csv')
df_nt_mn = pd.read_csv(savefile, index_col = 0).pivot(index = 'YEAR', columns = 'PROD', values = 'VAL')
df_nt_mn.index = pd.to_datetime(df_nt_mn.index, format = '%Y')
df_nt_mn = df_nt_mn[['MODIS-NIRv-ERA5-NT', 'MODIS-NIRv-ERA5-WY']].mean(axis = 1)
df_nt_mn = df_nt_mn.to_frame('Machine Learning (UFLUXv1)')

dfp_5km['std'] = dfp_5km[['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']].std(axis = 1)
dfp_5km = dfp_5km.rename(columns = {'MODIS': 'MODIS (5 km)', 'Landsat-7': 'Landsats/Sentinel-2 (5 km)'})
dfp_5km = dfp_5km[['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)', 'std']]

dfp_high['std'] = dfp_high[['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']].std(axis = 1)
dfp_high = dfp_high.rename(columns = {'MODIS': 'MODIS (500 m)', 'Landsat-7': 'Landsats/Sentinel-2 (30 m)'})
dfp_high = dfp_high[['MODIS (500 m)', 'Landsats/Sentinel-2 (30 m)', 'std']]

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

dfp_5km[['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)']].plot(ax = ax)
dfp_high[['MODIS (500 m)']].plot(ax = ax, style = '--', color = colors, legend = True)
# dfp_high[['Landsats/Sentinel-2 (30 m)']].plot(ax = ax, style = '--', color = colors[1], legend = True)
dfp_train7['Landsat-7_high'].rename('Landsat-7 (30 m)').plot(ax = ax, style = '--', color = colors[1], legend = True)

dfp_train7['Landsat-7_500m'].plot(ax = ax, color = 'k')

for c in dfp_5km.columns.drop('std'):
    ax.scatter(dfp_5km.index, dfp_5km[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
for c in ['MODIS (500 m)']:
    ax.scatter(dfp_high.index, dfp_high[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
for c in ['Landsat-7_high']:
    ax.scatter(dfp_train7.index, dfp_train7[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

ax.fill_between(dfp_5km.index, dfp_5km['Landsats/Sentinel-2 (5 km)'] - dfp_5km['std'], dfp_5km['Landsats/Sentinel-2 (5 km)'] + dfp_5km['std'], color = colors[1], alpha = 0.1)

# dfp_lit.drop('UFLUXv2', axis = 1)
dfp_lit[['Machine Learning (literature)', 'Trendy']].plot(ax = ax, color = [colors[5], colors[6]])
for c in ['Machine Learning (literature)', 'Trendy']:
    ax.scatter(dfp_lit.index, dfp_lit[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
ax.fill_between(df_trendy.index, df_trendy['GPP(t)_25th'], df_trendy['GPP(t)_75th'], color = colors[6], alpha = 0.1)

df_nt_mn.plot(ax = ax, style = '--', color = [colors[5]]) # UFLUXv1
for c in df_nt_mn.columns:
    ax.scatter(df_nt_mn.index, df_nt_mn[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

dfp_truth = dfp_truth.rename(columns = {'MODIST7': 'MODIS (30 m)'})
dfp_truth.plot(ax = ax, style = '--', color = colors[2], legend = True)
for c in dfp_truth.columns:
    ax.scatter(dfp_truth.index, dfp_truth[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

upper_legend(ax, ncols = 2, yloc = 1.3, user_labels_order = ['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)', 'Machine Learning (literature)', 'Trendy', 'MODIS (500 m)', 'Landsat-7 (30 m)', 'Machine Learning (UFLUXv1)', 'MODIS (30 m)'])
ax.set_xlabel('')
ax.set_ylabel('GPP ($Pg \ C$)')

# google.download_file(fig, 'Global_GPP_avg-v5.pdf')

"""### IAV plot new"""

nc_ec_t = xr.open_dataset(root_proj.joinpath('FLUXNET2015_DD.nc'))
nc_ec_t = nc_ec_t.where(nc_ec_t['time'].dt.year >= 2000, drop=True)

print(nc_ec_t[['GPP_NT_VUT_REF', 'GPP_DT_VUT_REF']].mean())

df_ec_count = nc_ec_t['GPP_NT_VUT_REF'].resample(time = '1YS').mean().to_dataframe().reset_index().pivot(index = 'time', columns = ['ID'])
df_ec_count.columns = df_ec_count.columns.get_level_values(1)
df_ec_count = df_ec_count.count(axis=1).rename('No. EC')

# @title Split plots

dfp_5km = dfp_5kmr.copy()
dfp_high = dfp_highr.copy()

dfp_5km['std'] = dfp_5km[['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']].std(axis = 1)
dfp_5km = dfp_5km.rename(columns = {'MODIS': 'MODIS (5 km)', 'Landsat-7': 'Landsats/Sentinel-2 (5 km)'})
dfp_5km = dfp_5km[['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)', 'std']]

dfp_high['std'] = dfp_high[['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']].std(axis = 1)
dfp_high = dfp_high.rename(columns = {'MODIS': 'MODIS (500 m)', 'Landsat-7': 'Landsats/Sentinel-2 (30 m)'})
dfp_high = dfp_high[['MODIS (500 m)', 'Landsats/Sentinel-2 (30 m)', 'std']]

savefile = root.joinpath("workspace/project_data/UFLUX-ensemble").joinpath(f'UFLUX-GPP.csv')
df_nt_mn = pd.read_csv(savefile, index_col = 0).pivot(index = 'YEAR', columns = 'PROD', values = 'VAL')
df_nt_mn.index = pd.to_datetime(df_nt_mn.index, format = '%Y')
df_nt_mn = df_nt_mn[['MODIS-NIRv-ERA5-NT', 'MODIS-NIRv-ERA5-WY']].mean(axis = 1)
df_nt_mn = df_nt_mn.to_frame('Machine Learning (UFLUXv1)')

p = root_proj.joinpath('202505/global_gpp_fluxsat_fluxcomx.csv')
dfp_fluxcomx = pd.read_csv(p, index_col = 0)
dfp_fluxcomx.index = pd.to_datetime(dfp_fluxcomx.index, format = '%Y')

# ==============================================================================

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

# ------------------------------------------------------------------------------

dfp = pd.concat([
    dfp_high['MODIS (500 m)'].rename('UFLUXv2'),
    df_nt_mn['Machine Learning (UFLUXv1)'].rename('UFLUXv1'),
    dfp_fluxcomx.rename(columns = {'Fluxcom-x': 'FLUXCOM-X', 'Fluxsat': 'FluxSat'}),
    dfp_lit['Data Assimilation'],
    dfp_lit['Trendy']
], axis = 1)

dfp['UFLUXv1'].plot(ax = ax, color = colors[0], marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
dfp['FLUXCOM-X'].plot(ax = ax, color = colors[5], marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
dfp['FluxSat'].plot(ax = ax, color = colors[6], marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
# dfp['UFLUXv2'].plot(ax = ax, color = colors[0], linestyle = '--', marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
# dfp['Trendy'].plot(ax = ax, color = colors[2], linestyle = '--', marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
# dfp['Data Assimilation'].plot(ax = ax, color = colors[2], linestyle = '--', marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')

ax.fill_between(df_trendy.index, df_trendy['GPP(t)_25th'], df_trendy['GPP(t)_75th'], color = 'gray', alpha = 0.1)

# ------------------------------------------------------------------------------

upper_legend(ax, ncols = 4, yloc = 1.12)#, user_labels_order = ['UFLUXv1', 'FluxSat', 'FLUXCOM-X', 'UFLUXv2'])

ax.set_xlabel('')
ax.set_ylabel('GPP ($Pg \ C$)')
ax.set_ylim(81, 169)

for c in dfp.columns:
    dft = dfp[c].dropna()
    print(c, roundit(stats.linregress(dft.index.year, dft).slope))

# google.download_file(fig, 'Global_GPP_IAV-v6-part1.pdf')
# google.download_file(fig, 'Global_GPP_IAV-v6-part1.png')

dfp = pd.concat([
    df_nt_mn['Machine Learning (UFLUXv1)'].rename('UFLUXv1'),
    dfp_high['MODIS (500 m)'].rename('UFLUXv2'),
    dfp_lit['Data Assimilation'],
    dfp_lit['Trendy']
], axis = 1)

# ------------------------------------------------------------------------------
fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

dfp['UFLUXv1'].plot(ax = ax, color = colors[0], marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
dfp['UFLUXv2'].plot(ax = ax, color = colors[1], linestyle = '-', marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
# dfp['Trendy'].plot(ax = ax, color = colors[8], linestyle = '--', marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
dfp['Data Assimilation'].plot(ax = ax, color = colors[9], linestyle = '--', marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')


upper_legend(ax, ncols = 4, yloc = 1.12)

ax.set_xlabel('')
ax.set_ylabel('GPP ($Pg \ C$)')
ax.set_ylim(81, 169)

for c in dfp.columns:
    dft = dfp[c].dropna()['2011'::]
    print(c, roundit(stats.linregress(dft.index.year, dft).slope))

ax.bar(df_ec_count.index, df_ec_count, color = 'gray', alpha = 0.1)

# google.download_file(fig, 'Global_GPP_IAV-v6-part2.pdf')
# google.download_file(fig, 'Global_GPP_IAV-v6-part2.png')

dfp = pd.concat([
    dfp_5km[['MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)']].rename(columns = {'Landsats/Sentinel-2 (5 km)': 'Landsats/Sentinel-2 (5 km)'}),
    dfp_high[['MODIS (500 m)']],
    dfp_truth.rename(columns = {'MODIST7': 'MODIS (30 m)'}),
    dfp_train7['Landsat-7_500m'].rename('Landsats/Sentinel-2 (500 m)'),
    dfp_train7['Landsat-7_high'].rename('Landsats/Sentinel-2 (30/10 m)')
], axis = 1)

dfp_std = pd.concat([
    dfp_high['std'].rename('std_5km'),
    dfp_high['std'].rename('std_30m'),
], axis = 1)

# ------------------------------------------------------------------------------
fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

dfp['MODIS (5 km)'].plot(ax = ax, color = colors[1], linestyle = '--', marker = 's', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
dfp['MODIS (500 m)'].plot(ax = ax, color = colors[1], linestyle = '-', marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
dfp['MODIS (30 m)'].plot(ax = ax, color = colors[1], linestyle = '--', marker = '^', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')

dfp['Landsats/Sentinel-2 (5 km)'].plot(ax = ax, color = colors[2], linestyle = '--', marker = 's', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
dfp['Landsats/Sentinel-2 (500 m)'].plot(ax = ax, color = colors[2], linestyle = '-', marker = 'o', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
dfp['Landsats/Sentinel-2 (30/10 m)'].plot(ax = ax, color = colors[2], linestyle = '--', marker = '^', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')

ax.fill_between(dfp.index, dfp['Landsats/Sentinel-2 (30/10 m)'] - dfp_std['std_30m'], dfp['Landsats/Sentinel-2 (30/10 m)'] + dfp_std['std_30m'], color = colors[2], alpha = 0.1)

# ------------------------------------------------------------------------------

# upper_legend(ax, ncols = 3, yloc = 1.2, user_labels_order = [
#     'MODIS (5 km)', 'Landsats/Sentinel-2 (5 km)', 'MODIS (500 m)', 'Landsats/Sentinel-2 (500 m)',
#     'MODIS (30 m)', 'Landsats/Sentinel-2 (30/10 m)'
# ])
upper_legend(ax, ncols = 2, yloc = 1.25, user_labels_order = [
    'MODIS (5 km)', 'MODIS (500 m)', 'MODIS (30 m)',
    'Landsats/Sentinel-2 (5 km)', 'Landsats/Sentinel-2 (500 m)',
    'Landsats/Sentinel-2 (30/10 m)'
])

ax.set_xlabel('')
ax.set_ylabel('GPP ($Pg \ C$)')
ax.set_ylim(81, 169)

ax.axvline('2014', linestyle = '--', color = 'gray')

for c in dfp.columns:
    dft = dfp[c].dropna()
    print(c, roundit(stats.linregress(dft.index.year, dft).slope))

google.download_file(fig, 'Global_GPP_IAV-v6-part3.pdf')
google.download_file(fig, 'Global_GPP_IAV-v6-part3.png')

"""### IAV by land cover"""

## By land cover

dfp_5km = dfp_5kmr.copy()
dfp_train7 = dfp_train7r.copy()

dfp_landsat7_ = pd.concat([
    dfp_5km['Landsat-7'].rename('Landsat-7 (5 km)'),
    dfp_train7['Landsat-7_500m'].rename('Landsat-7 (500 m)'),
    dfp_train7['Landsat-7_high'].rename('Landsat-7 (30 m)')
], axis = 1)

# Reverse the dictionary
reversed_MODIS_IGBP_dict = {v: k for k, v in MODIS_IGBP_dict.items()}

# ------------------------------------------------------------------------------

luccr = xr.open_dataset(root_proj.joinpath('1grid_inputs/lucc_info.nc'))
coef = 365 * 0.25 * 0.25 * 1e5 * 1e5 / 1e15

# ==============================================================================
savefile = root_proj.joinpath('202505/GPP_Landat-7_IAV_landcover_5km.csv')
if savefile.exists():
    dfp_5km_igbp = pd.read_csv(savefile, index_col = 0)
    dfp_5km_igbp.index = pd.to_datetime(dfp_5km_igbp.index)
else:
    dfp_5km_igbp = []
    for p in tqdm(list(root_proj.joinpath('2output_global_Pmodel_Landsat7-5km').glob('*'))):
    # for p in tqdm(list(root_proj.joinpath('2output_global_Pmodel-5km').glob('MODIS*'))):
        dt = pd.to_datetime(p.stem.split('_')[1], format = '%Y')
        lucc = luccr.sel(time = dt).drop_vars('time')

        # --------------------------------------------------------------------------
        nct = xr.open_dataset(p, drop_variables = 'GPPm').mean(dim = 'time')
        nct = nct.where(nct['GPP'] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
        nct = nct.interp(latitude = lucc.latitude, longitude = lucc.longitude)
        # --------------------------------------------------------------------------
        nct = xr.merge([nct, lucc]).drop_vars('spatial_ref')

        dft = nct.to_dataframe()
        dft['GPP'] = dft['GPP'] * coef
        dft = dft.dropna().groupby('IGBP').sum()
        dft = dft / dft.sum() * dfp_landsat7_.loc[dt, 'Landsat-7 (5 km)']

        # Replace the numeric index with the class names
        dft.index = dft.index.map(lambda x: reversed_MODIS_IGBP_dict.get(int(x), x))

        dft.columns = [dt]
        dfp_5km_igbp.append(dft.T)
    dfp_5km_igbp = pd.concat(dfp_5km_igbp, axis = 0)

    dfp_5km_igbp.to_csv(savefile)

del(savefile)

# ==============================================================================

savefile = root_proj.joinpath('202505/GPP_Landat-7_IAV_landcover_500m.csv')
if savefile.exists():
    dfp_500m_igbp = pd.read_csv(savefile, index_col = 0)
    dfp_500m_igbp.index = pd.to_datetime(dfp_500m_igbp.index)
else:
    dfp_500m_igbp = []
    for p in tqdm(list(root_proj.joinpath('2output_global_Pmodel-500m').glob('Landsat-7*'))):
    # for p in tqdm(list(root_proj.joinpath('2output_global_Pmodel_Landsat7-500m').glob('*'))):
    # for p in tqdm(list(root_proj.joinpath('2output_global_Pmodel-high').glob('MODIS*'))):
        dt = pd.to_datetime(p.stem.split('_')[1], format = '%Y')
        lucc = luccr.sel(time = dt).drop_vars('time')

        # --------------------------------------------------------------------------
        nct = xr.open_dataset(p, drop_variables = 'GPPm').mean(dim = 'time')
        nct = nct.where(nct['GPP'] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
        nct = nct.interp(latitude = lucc.latitude, longitude = lucc.longitude)
        # --------------------------------------------------------------------------
        nct = xr.merge([nct, lucc]).drop_vars('spatial_ref')

        dft = nct.to_dataframe()
        dft['GPP'] = dft['GPP'] * coef
        dft = dft.dropna().groupby('IGBP').sum()
        dft = dft / dft.sum() * dfp_landsat7_.loc[dt, 'Landsat-7 (500 m)']

        # Replace the numeric index with the class names
        dft.index = dft.index.map(lambda x: reversed_MODIS_IGBP_dict.get(int(x), x))

        dft.columns = [dt]
        dfp_500m_igbp.append(dft.T)
    dfp_500m_igbp = pd.concat(dfp_500m_igbp, axis = 0)

    dfp_500m_igbp.to_csv(savefile)

del(savefile)

# ==============================================================================

savefile = root_proj.joinpath('202505/GPP_Landat-7_IAV_landcover_30m.csv')
if savefile.exists():
    dfp_30m_igbp = pd.read_csv(savefile, index_col = 0)
    dfp_30m_igbp.index = pd.to_datetime(dfp_30m_igbp.index)
else:
    dfp_30m_igbp = []
    for p in tqdm(list(root_proj.joinpath('2output_global_Pmodel_Landsat7-high').glob('*'))):
    # for p in tqdm(list(root_proj.joinpath('2output_global_Pmodel_Landsat7model2MODIS-high').glob('*'))):
        dt = pd.to_datetime(p.stem.split('_')[1], format = '%Y')
        lucc = luccr.sel(time = dt).drop_vars('time')

        # --------------------------------------------------------------------------
        nct = xr.open_dataset(p, drop_variables = 'GPPm').mean(dim = 'time')
        nct = nct.where(nct['GPP'] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
        nct = nct.interp(latitude = lucc.latitude, longitude = lucc.longitude)
        # --------------------------------------------------------------------------
        nct = xr.merge([nct, lucc]).drop_vars('spatial_ref')

        dft = nct.to_dataframe()
        dft['GPP'] = dft['GPP'] * coef
        dft = dft.dropna().groupby('IGBP').sum()
        dft = dft / dft.sum() * dfp_landsat7_.loc[dt, 'Landsat-7 (30 m)']

        # Replace the numeric index with the class names
        dft.index = dft.index.map(lambda x: reversed_MODIS_IGBP_dict.get(int(x), x))

        dft.columns = [dt]
        dfp_30m_igbp.append(dft.T)
    dfp_30m_igbp = pd.concat(dfp_30m_igbp, axis = 0)

    dfp_30m_igbp.to_csv(savefile)

del(savefile)

# dfp_landsat7_., dfp_5km_igbp, dfp_500m_igbp, dfp_30m_igbp

print([igbp_ for igbp_ in meta['IGBP'].drop_duplicates().sort_values()])

igbp_ = 'WSA'
dfp = pd.concat([
    dfp_5km_igbp[igbp_].rename('5km'),
    dfp_500m_igbp[igbp_].rename('500m'),
    dfp_30m_igbp[igbp_].rename('30m'),
], axis = 1)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

dfp['5km'].plot(ax = ax, color = colors[0], linestyle = '-', marker = 's', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
dfp['500m'].plot(ax = ax, color = colors[1], linestyle = '-', marker = 's', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')
dfp['30m'].plot(ax = ax, color = colors[2], linestyle = '-', marker = 's', markersize = 5, markeredgecolor = 'k', markerfacecolor = 'w')

upper_legend(ax, ncols = 3, yloc = 1.13, user_labels_order = None)

ax.set_xlabel('')
ax.set_ylabel(f'{igbp_} GPP ($Pg \ C$)')
# ax.set_ylim(81, 169)


for c in dfp.columns:
    dft = dfp[c].dropna()
    print(c, roundit(stats.linregress(dft.index.year, dft).slope))

# google.download_file(fig, f'IGBP_GPP_IAV_{igbp_}.png')

"""### Land cover surrouding the site"""

!pip install git+https://github.com/soonyenju/scigee.git  --quiet

from scigee import *
geeface.init_gee('ee-zhusy93')

from shapely.geometry import box

site = 'AU-TTE'
lon_, lat_ = meta.loc[site, ['LON', 'LAT']]

# ------------------------------------------------------------------------------

step = 2.5 * 1e3  # meters

# Convert step from meters to degrees approximately (1 deg ≈ 111 km)
step_deg = step / 1e5  # 0.025 degrees (rough approximation)

# Define bounding box
minx = lon_ - step_deg
maxx = lon_ + step_deg
miny = lat_ - step_deg
maxy = lat_ + step_deg

# Create ee box
roi = ee.Geometry.BBox(minx, miny, maxx, maxy)

# Create GeoDataFrame
gdf_roi = gpd.GeoDataFrame({'site': [site]}, geometry=[box(minx, miny, maxx, maxy)], crs='EPSG:4326')

# ------------------------------------------------------------------------------
# Convert step from meters to degrees approximately (1 deg ≈ 111 km)
step_500m_deg = 500 / 1e5  # 500 m, 0.005 degrees (rough approximation)

# Define bounding box
minx_500m = lon_ - step_500m_deg
maxx_500m = lon_ + step_500m_deg
miny_500m = lat_ - step_500m_deg
maxy_500m = lat_ + step_500m_deg

# Create GeoDataFrame
gdf_roi_500m = gpd.GeoDataFrame({'site': [site]}, geometry=[box(minx_500m, miny_500m, maxx_500m, maxy_500m)], crs='EPSG:4326')
# ------------------------------------------------------------------------------

collection = (
    ee.ImageCollection('ESA/WorldCover/v100')
    .filterBounds(roi)
    .filterDate('2020-01-01', '2021-01-01')
)

landcover = collection.select(['Map']).reduce(ee.Reducer.mean()).rename('LC')
landcover = landcover.reproject(crs = "EPSG:4326", scale = 10)

# landcover = geeface.image4shape(landcover, gdf_roi, 100, to_xarray = True)['LC']

# ------------------------------------------------------------------------------
# Download

savefolder = root_proj.joinpath(f'202505/EC_ESA-WorldCover_tif/{site}')
savefolder.mkdir(exist_ok = True, parents = True)
geeface.gee2local(landcover, savefolder.joinpath('download.zip'), 10, roi, user_params = {}, folder = '5km')

del(savefolder)

# ------------------------------------------------------------------------------
p = root_proj.joinpath(f'202505/EC_ESA-WorldCover_tif/{site}/5km/LC.tif')
landcover = geoface.load_tif(p, band_names = ['LC'])['LC'].drop_vars('spatial_ref')

import matplotlib.colors as mcolors
import matplotlib.patches as patches

# Define ESA WorldCover v2.0 land cover classes with corresponding colors
land_cover_classes = {
    "Tree cover": "#006400",
    "Shrubland": "#ffbb22",
    "Grassland": "#ffff4c",
    "Cropland": "#f096ff",
    "Built-up": "#fa0000",
    "Bare / sparse vegetation": "#b4b4b4",
    "Snow and ice": "#f0f0f0",
    "Permanent water bodies": "#0064c8",
    "Herbaceous wetland": "#0096a0",
    "Mangroves": "#00cf75",
    "Moss and lichen": "#fae6a0",
}

# Corresponding land cover codes
land_cover_codes = {
    10: "Tree cover",
    20: "Shrubland",
    30: "Grassland",
    40: "Cropland",
    50: "Built-up",
    60: "Bare / sparse vegetation",
    70: "Snow and ice",
    80: "Permanent water bodies",
    90: "Herbaceous wetland",
    95: "Mangroves",
    100: "Moss and lichen",
}

# Create a colormap and norm for land cover classes
cmap = mcolors.ListedColormap(list(land_cover_classes.values()))
bounds = list(land_cover_codes.keys()) + [max(land_cover_codes.keys()) + 10]
norm = mcolors.BoundaryNorm(bounds, cmap.N)

# Plot the land cover map
fig, ax = setup_canvas(1, 1, figsize = (5, 5), fontsize=10, labelsize=10)
im = landcover.drop_vars('spatial_ref', errors = 'ignore').plot(ax=ax, cmap=cmap, norm=norm, add_colorbar=False)

# Create a custom color legend with land cover names
legend_patches = [
    plt.Line2D([0], [0], marker='o', color='w', markersize=8,
               markerfacecolor=land_cover_classes[name], label=name)
    for name in land_cover_classes.keys()
]

# Place the legend at the top with multiple columns
ax.legend(handles=legend_patches, title="Land Cover Classes", loc="upper center",
          bbox_to_anchor=(0.5, 1.3), ncol=3, frameon=False)

# ------------------------------------------------------------------------------

ax.scatter([lon_], [lat_], s=20, facecolor='None', edgecolor='black')

rect_500 = patches.Rectangle(
    (lon_ - 250 / 1e5, lat_ - 250 / 1e5),  # lower-left corner
    500 / 1e5,
    500 / 1e5,
    linewidth=1.5,
    edgecolor='gray',
    facecolor='None',
    label='500m x 500m'
)
ax.add_patch(rect_500)

# ------------------------------------------------------------------------------

df_count = landcover.to_dataframe().value_counts().reset_index()
df_count['LC'] = df_count['LC'].replace(land_cover_codes)
df_count = df_count.set_index('LC')
df_count['ratio'] = df_count / df_count.sum()
print(df_count)

# ------------------------------------------------------------------------------

df_count_500m = geoface.clip(landcover, gdf_roi_500m).drop_vars('spatial_ref').to_dataframe().value_counts().reset_index()
df_count_500m['LC'] = df_count_500m['LC'].replace(land_cover_codes)
df_count_500m = df_count_500m.set_index('LC')
df_count_500m['ratio'] = df_count_500m / df_count_500m.sum()

print(df_count_500m)

"""### NIRv by sites"""

from shapely.geometry import box
from scigee.utils import footprint_size, harmonise_ETM, add_ndvi, add_nirv, add_kndvi, add_evi2, radiometric_calibration, mask_landsatsr_clouds, landsat_apply_scale_factors

# site = 'BE-Bra'
site = 'AU-TTE'
lon_, lat_ = meta.loc[site, ['LON', 'LAT']]

# ------------------------------------------------------------------------------

step = 2.5 * 1e3; scale_bame = 'Landsat-8_5km'  # meters
# step = 250; scale_bame = 'Landsat-8_500m'

# Convert step from meters to degrees approximately (1 deg ≈ 111 km)
step_deg = step / 1e5  # 0.025 degrees (rough approximation)

# Define bounding box
minx = lon_ - step_deg
maxx = lon_ + step_deg
miny = lat_ - step_deg
maxy = lat_ + step_deg

# Create ee box
roi = ee.Geometry.BBox(minx, miny, maxx, maxy)

# Create GeoDataFrame
gdf_roi = gpd.GeoDataFrame({'site': [site]}, geometry=[box(minx, miny, maxx, maxy)], crs='EPSG:4326')

# ------------------------------------------------------------------------------

collection = (
    ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')
    .filterBounds(roi)
    .filterDate('2020-01-01', '2021-01-01')
    .map(landsat_apply_scale_factors)
    .map(mask_landsatsr_clouds)
    .map(lambda img: add_ndvi(img, red_band='SR_B4', nir_band='SR_B5', output_band='NDVI'))
    .map(lambda img: add_nirv(img, red_band='SR_B4', nir_band='SR_B5', output_band='NIRv'))
    .map(lambda img: add_kndvi(img, red_band='SR_B4', nir_band='SR_B5', output_band='kNDVI'))
    .map(lambda img: add_evi2(img, red_band='SR_B4', nir_band='SR_B5', output_band='EVI2'))
)

landsat8 = collection.select(['NDVI', 'NIRv', 'kNDVI', 'EVI2']).reduce(ee.Reducer.mean()).rename(['NDVI', 'NIRv', 'kNDVI', 'EVI2'])
landsat8 = landsat8.reproject(crs = "EPSG:4326", scale = 30)
# landsat8_vi = geeface.image4shape(landsat8, gdf_roi, 30, to_xarray = True)

# # ------------------------------------------------------------------------------
# Download

savefolder = root_proj.joinpath(f'202505/EC_VI_tif/{site}')
savefolder.mkdir(exist_ok = True, parents = True)
geeface.gee2local(landsat8, savefolder.joinpath('download.zip'), 30, roi, user_params = {}, folder = scale_bame)

del(savefolder)

from shapely.geometry import box
from scigee.utils import footprint_size, harmonise_ETM, add_ndvi, add_nirv, add_kndvi, add_evi2, radiometric_calibration, mask_landsatsr_clouds, landsat_apply_scale_factors

# site = 'BE-Bra'
site = 'AU-TTE'
lon_, lat_ = meta.loc[site, ['LON', 'LAT']]

# ------------------------------------------------------------------------------

step = 2.5 * 1e3; scale_bame = 'Sentinel-2_5km'  # meters
# step = 250; scale_bame = 'Sentinel-2_500m'

# Convert step from meters to degrees approximately (1 deg ≈ 111 km)
step_deg = step / 1e5  # 0.025 degrees (rough approximation)

# Define bounding box
minx = lon_ - step_deg
maxx = lon_ + step_deg
miny = lat_ - step_deg
maxy = lat_ + step_deg

# Create ee box
roi = ee.Geometry.BBox(minx, miny, maxx, maxy)

# Create GeoDataFrame
gdf_roi = gpd.GeoDataFrame({'site': [site]}, geometry=[box(minx, miny, maxx, maxy)], crs='EPSG:4326')

# ------------------------------------------------------------------------------

collection = (
    ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
    .filterBounds(roi)
    .filterDate('2020-01-01', '2021-01-01')
    .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', 20))
    .map(lambda img: radiometric_calibration(img, input_band='B4', output_band='B4_cal'))
    .map(lambda img: radiometric_calibration(img, input_band='B8', output_band='B8_cal'))
    .map(lambda img: add_ndvi(img, red_band='B4_cal', nir_band='B8_cal', output_band='NDVI'))
    .map(lambda img: add_nirv(img, red_band='B4_cal', nir_band='B8_cal', output_band='NIRv'))
    .map(lambda img: add_kndvi(img, red_band='B4_cal', nir_band='B8_cal', output_band='kNDVI'))
    .map(lambda img: add_evi2(img, red_band='B4_cal', nir_band='B8_cal', output_band='EVI2'))
)

landsat8 = collection.select(['NDVI', 'NIRv', 'kNDVI', 'EVI2']).reduce(ee.Reducer.mean()).rename(['NDVI', 'NIRv', 'kNDVI', 'EVI2'])
landsat8 = landsat8.reproject(crs = "EPSG:4326", scale = 10)
# landsat8_vi = geeface.image4shape(landsat8, gdf_roi, 10, to_xarray = True)

# # ------------------------------------------------------------------------------
# Download

savefolder = root_proj.joinpath(f'202505/EC_VI_tif/{site}')
savefolder.mkdir(exist_ok = True, parents = True)
geeface.gee2local(landsat8, savefolder.joinpath('download.zip'), 10, roi, user_params = {}, folder = scale_bame)

del(savefolder)

site = 'BE-Bra'

p = root_proj.joinpath(f'202505/EC_VI_tif/{site}/Sentinel-2_500m/NIRv.tif')
# p = root_proj.joinpath(f'202505/EC_VI_tif/{site}/Sentinel-2_5km/NIRv.tif')

lon_, lat_ = meta.loc[site, ['LON', 'LAT']]

nc_ec_vi = geoface.load_tif(p, band_names = ['NIRv'])['NIRv'].drop_vars('spatial_ref')
nc_ec_vi = nc_ec_vi.where(nc_ec_vi >= 0, np.nan)

mean_vi = roundit(float(nc_ec_vi.mean()))
print(mean_vi)

fig, ax = setup_canvas(1, 1, figsize = (6, 5), fontsize=10, labelsize=10)

nc_ec_vi.plot(ax = ax, cmap = 'YlGn')

# ------------------------------------------------------------------------------

ax.scatter([lon_], [lat_], s=20, facecolor='None', edgecolor='black')

if p.parent.stem.split('_')[-1] == '5km':
    rect_500 = patches.Rectangle(
        (lon_ - 250 / 1e5, lat_ - 250 / 1e5),  # lower-left corner
        500 / 1e5,
        500 / 1e5,
        linewidth=1.5,
        edgecolor='gray',
        facecolor='None',
        label='500m x 500m'
    )
    ax.add_patch(rect_500)

# ------------------------------------------------------------------------------

ax.axis('off')
add_text(ax, 0.1, 0.1, f'NIRv (mean): {mean_vi}', if_background = True, bg_facecolor = 'white', bg_alpha = 0.7)

"""### Global NIRv map"""

df_NIRv = []
for p in root_proj.joinpath('1grid_NIRv_yearly').glob('*'):
    satellite, dt = p.stem.split('_')
    dt = pd.to_datetime(dt, format = '%Y-%m-%d')
    df_NIRv.append([satellite, dt, p])
df_NIRv = pd.DataFrame(df_NIRv, columns = ['SATELLITE', 'DATETIME', 'PATH']).pivot(index = 'DATETIME', columns = 'SATELLITE').sort_index()
df_NIRv.columns = df_NIRv.columns.droplevel(0)

nc_NIRv = []
for dt in tqdm(df_NIRv.index):
    p = df_NIRv.loc[dt, 'Landsat-7']
    nct = xr.open_dataset(p)['NIRv']
    nct = nct.where(nct >= 0, np.nan)
    nc_NIRv.append(nct.expand_dims(time = [dt]))
nc_NIRv = xr.merge(nc_NIRv)

mean_vi = roundit(float(nc_NIRv['NIRv'].mean()))

ncp = nc_NIRv.mean(dim = 'time').drop_vars('spatial_ref')['NIRv']
ncp_3857 = ncp.rio.write_crs('EPSG:4326', inplace=True).rio.reproject('EPSG:3857').drop_vars('spatial_ref')
ncp_3035 = ncp.rio.write_crs('EPSG:4326', inplace=True).rio.reproject('EPSG:3035').drop_vars('spatial_ref')
del(nct)

# nc_NIRv.mean(dim = ['latitude', 'longitude']).drop_vars('spatial_ref').to_dataframe()['NIRv'].plot()

world_lowres = gpd.read_file((r'https://github.com/soonyenju/geoAI/blob/main/geoAI/data/geojson/world_naturalearth_lowres.geojson?raw=true'))
world_no_antarctica = world_lowres.copy()
# Set the CRS to EPSG:4326 (WGS84)
world_no_antarctica = world_no_antarctica.set_crs(epsg=4326)
world_no_antarctica['min_lat'] = world_no_antarctica.geometry.bounds['miny']
# Filter out Antarctica (typically min latitude < -60)
world_no_antarctica = world_no_antarctica[world_no_antarctica['min_lat'] > -60]

world_3857 = world_no_antarctica.to_crs('EPSG:3857')
world_3035 = world_no_antarctica.to_crs('EPSG:3035')

# ------------------------------------------------------------------------------
# world_merge = world.dissolve()
# ------------------------------------------------------------------------------
from shapely.geometry import Point

# Create a geometry column from LON and LAT
geometry = [Point(xy) for xy in zip(meta['LON'], meta['LAT'])]

# Create a GeoDataFrame
gdf_FLUXNET = gpd.GeoDataFrame(meta[['LON', 'LAT']], geometry=geometry, crs='EPSG:4326')  # WGS84

gdf_3857 = gdf_FLUXNET.to_crs(epsg=3857)
gdf_3035 = gdf_FLUXNET.to_crs(epsg=3035)

fig, ax = setup_canvas(1, 1, figsize = (10, 6), fontsize=10, labelsize=10)

ncp.drop_vars('spatial_ref').plot(ax = ax, vmax = 0.5, cmap = 'YlGn', cbar_kwargs = {'shrink': 0.85, 'pad': 0.02})
world_no_antarctica.plot(ax = ax, color = 'none', edgecolor = 'black')

# ------------------------------------------------------------------------------

ax.scatter(meta['LON'], meta['LAT'], s=20, facecolor=nature_colors[0], edgecolor=nature_colors[1])

# ------------------------------------------------------------------------------

# ax.axis('off')  # Hides the axes
add_text(ax, 0.1, 0.1, f'NIRv (mean): {mean_vi}', if_background = True, bg_facecolor = 'white', bg_alpha = 0.7)

fig, ax = setup_canvas(1, 1, figsize = (6, 6), fontsize=10, labelsize=10)

ncp_3857.plot(ax = ax, vmax = 0.5, cmap = 'YlGn', cbar_kwargs = {'shrink': 0.85, 'pad': 0.02})
world_3857.plot(ax = ax, color = 'none', edgecolor = 'black')

# ------------------------------------------------------------------------------

# ax.scatter(meta['LON'], meta['LAT'], s=20, facecolor=nature_colors[0], edgecolor=nature_colors[1])
gdf_3857.plot(ax = ax, marker='o', markersize=20, facecolor=nature_colors[0], edgecolor=nature_colors[1])

# ------------------------------------------------------------------------------

# ax.axis('off')  # Hides the axes
add_text(ax, 0.1, 0.1, f'NIRv (mean): {mean_vi}', if_background = True, bg_facecolor = 'white', bg_alpha = 0.7)

fig, ax = setup_canvas(1, 1, figsize = (6, 6), fontsize=10, labelsize=10)

ncp_3035.plot(ax = ax, vmax = 0.5, cmap = 'YlGn', cbar_kwargs = {'shrink': 0.85, 'pad': 0.02})
world_3035.plot(ax = ax, color = 'none', edgecolor = 'black')

# ------------------------------------------------------------------------------

# ax.scatter(meta['LON'], meta['LAT'], s=20, facecolor=nature_colors[0], edgecolor=nature_colors[1])
gdf_3035.plot(ax = ax, marker='o', markersize=20, facecolor=nature_colors[0], edgecolor=nature_colors[1])

# ------------------------------------------------------------------------------

# ax.axis('off')  # Hides the axes
add_text(ax, 0.1, 0.1, f'NIRv (mean): {mean_vi}', if_background = True, bg_facecolor = 'white', bg_alpha = 0.7)

"""## GPP spatial comparisons"""

## GPP paths
# ==============================================================================

df_path_GPP_5kmr = []
for p in root_proj.joinpath('2output_global_Pmodel-5km').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_5kmr.append([satellite, int(year), p])
df_path_GPP_5kmr = pd.DataFrame(df_path_GPP_5kmr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_5kmr.columns = df_path_GPP_5kmr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_30mr = []
for p in root_proj.joinpath('2output_global_Pmodel-high').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_30mr.append([satellite, int(year), p])
df_path_GPP_30mr = pd.DataFrame(df_path_GPP_30mr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_30mr.columns = df_path_GPP_30mr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_Landsat7_5kmr = []
for p in root_proj.joinpath('2output_global_Pmodel_Landsat7-5km').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_Landsat7_5kmr.append([satellite, int(year), p])
df_path_GPP_Landsat7_5kmr = pd.DataFrame(df_path_GPP_Landsat7_5kmr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_Landsat7_5kmr.columns = df_path_GPP_Landsat7_5kmr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_Landsat7_500mr = []
for p in root_proj.joinpath('2output_global_Pmodel_Landsat7-500m').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_Landsat7_500mr.append([satellite, int(year), p])
df_path_GPP_Landsat7_500mr = pd.DataFrame(df_path_GPP_Landsat7_500mr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_Landsat7_500mr.columns = df_path_GPP_Landsat7_500mr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_Landsat7_highr = []
for p in root_proj.joinpath('2output_global_Pmodel_Landsat7-high').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_Landsat7_highr.append([satellite, int(year), p])
df_path_GPP_Landsat7_highr = pd.DataFrame(df_path_GPP_Landsat7_highr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_Landsat7_highr.columns = df_path_GPP_Landsat7_highr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_Landsat7m2MODIS_highr = []
for p in root_proj.joinpath('2output_global_Pmodel_Landsat7model2MODIS-high').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_Landsat7m2MODIS_highr.append([satellite, int(year), p])
df_path_GPP_Landsat7m2MODIS_highr = pd.DataFrame(df_path_GPP_Landsat7m2MODIS_highr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_Landsat7m2MODIS_highr.columns = df_path_GPP_Landsat7m2MODIS_highr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_5kmr['Landsat-7'] = df_path_GPP_Landsat7_5kmr['Landsat-7']
df_path_GPP_30mr['Landsat-7'] = df_path_GPP_Landsat7_highr['Landsat-7']

df_path_GPP_500mr = pd.concat([
    df_path_GPP_30mr['MODIS'],
    df_path_GPP_Landsat7_500mr
], axis = 1)

df_path_GPP_30mr = df_path_GPP_30mr.drop('MODIS', axis = 1)

df_path_GPP_30mr = pd.concat([
    df_path_GPP_Landsat7m2MODIS_highr,
    df_path_GPP_30mr
], axis = 1)

nc_5km = []
for sat_name in tqdm(['MODIS', 'Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2'], desc="Outer loop"):
    nct = []
    for p in tqdm(df_path_GPP_5kmr[sat_name].dropna(), desc="Inner loop", leave=False):
        # nct.append(xr.open_dataset(p).resample(time = '1YS').mean())
        nct.append(xr.open_dataset(p, drop_variables = ['GPPm']).mean(dim = 'time').expand_dims(time = [pd.to_datetime(p.stem.split('_')[1], format = '%Y')]))
    nct = xr.merge(nct).mean(dim = 'time')
    nc_5km.append(nct.rename({'GPP': sat_name}))
nc_5km = xr.merge(nc_5km)

# ------------------------------------------------------------------------------

nc_500m = []
for sat_name in tqdm(['MODIS', 'Landsat-7'], desc="Outer loop"):
    nct = []
    for p in tqdm(df_path_GPP_500mr[sat_name].dropna(), desc="Inner loop", leave=False):
        nct.append(xr.open_dataset(p, drop_variables = ['GPPm']).mean(dim = 'time').expand_dims(time = [pd.to_datetime(p.stem.split('_')[1], format = '%Y')]))
    nct = xr.merge(nct).mean(dim = 'time')
    nc_500m.append(nct.rename({'GPP': sat_name}))
nc_500m = xr.merge(nc_500m)

# ------------------------------------------------------------------------------

nc_30m = []
for sat_name in tqdm(['MODIS', 'Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2'], desc="Outer loop"):
    nct = []
    for p in tqdm(df_path_GPP_30mr[sat_name].dropna(), desc="Inner loop", leave=False):
        nct.append(xr.open_dataset(p, drop_variables = ['GPPm']).mean(dim = 'time').expand_dims(time = [pd.to_datetime(p.stem.split('_')[1], format = '%Y')]))
    nct = xr.merge(nct).mean(dim = 'time')
    nc_30m.append(nct.rename({'GPP': sat_name}))
nc_30m = xr.merge(nc_30m)

df_lat = nc_5km.mean(dim = 'longitude').to_dataframe()
df_lon = nc_5km.mean(dim = 'latitude').to_dataframe()


sat_name = 'MODIS'

df_lat_reso = pd.concat([
    nc_5km[sat_name].rename('5 km').mean(dim = 'longitude').to_dataframe(),
    nc_500m[sat_name].rename('500 m').mean(dim = 'longitude').to_dataframe(),
    nc_30m[sat_name].rename('30 m').mean(dim = 'longitude').to_dataframe(),
], axis = 1)

df_lon_reso = pd.concat([
    nc_5km[sat_name].rename('5 km').mean(dim = 'latitude').to_dataframe(),
    nc_500m[sat_name].rename('500 m').mean(dim = 'latitude').to_dataframe(),
    nc_30m[sat_name].rename('30 m').mean(dim = 'latitude').to_dataframe(),
], axis = 1)

# df_lat_reso.plot()
# df_lon_reso.plot()

df_lat_reso = pd.concat([
    nc_5km[['MODIS', 'Landsat-7']].mean(dim = 'longitude').to_dataframe().mean(axis = 1).rename('5 km'),
    nc_500m[['MODIS', 'Landsat-7']].mean(dim = 'longitude').to_dataframe().mean(axis = 1).rename('500 m'),
    nc_30m[['MODIS', 'Landsat-7']].mean(dim = 'longitude').to_dataframe().mean(axis = 1).rename('30 m'),
], axis = 1)

df_lon_reso = pd.concat([
    nc_5km[['MODIS', 'Landsat-7']].mean(dim = 'latitude').to_dataframe().mean(axis = 1).rename('5 km'),
    nc_500m[['MODIS', 'Landsat-7']].mean(dim = 'latitude').to_dataframe().mean(axis = 1).rename('500 m'),
    nc_30m[['MODIS', 'Landsat-7']].mean(dim = 'latitude').to_dataframe().mean(axis = 1).rename('30 m'),
], axis = 1)

# df_lat_reso.plot()
# df_lon_reso.plot()

shp = world[['geometry']].dissolve()

fig, ax = setup_canvas(1, 1, figsize = (5, 4), fontsize = 10, labelsize = 10)
nc_30m['MODIS'].plot(ax = ax, vmin = 0, vmax = 12, cmap = 'YlGn', cbar_kwargs={'orientation': 'horizontal', 'pad': 0., 'location': 'top', 'label': None})

ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([])
ax.set_ylabel('latitude')
shp.plot(ax = ax, color = 'none', edgecolor = 'black')
ax.set_ylim(-90, 90)
# ------------------------------------------------------------------------------

pos = ax.get_position()
ax1 = fig.add_axes([pos.x1, pos.y0, 0.3, pos.y1 - pos.y0])
for i, sat_name in enumerate(['MODIS', 'Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']):
    ax1.plot(df_lat[sat_name], df_lat.index, color = colors[i], label = sat_name)
ax1.set_ylim(ax.get_ylim())
ax1.yaxis.tick_right()
# ------------------------------------------------------------------------------

pos = ax.get_position()
ax2 = fig.add_axes([pos.x0, pos.y0 - 0.3, pos.x1 - pos.x0, 0.3])
for i, sat_name in enumerate(['MODIS', 'Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']):
    ax2.plot(df_lon.index, df_lon[sat_name], color = colors[i], label = sat_name)
ax2.set_xlim(ax.get_xlim())
ax2.set_xlabel('longitude')

upper_legend(ax2, xloc = 1.2, yloc = 0.9, ncols = 1)
google.download_file(fig, 'GPP_spatial_distribution.pdf')
google.download_file(fig, 'GPP_spatial_distribution.jpg')

"""## Only use high-resolution valid pixels"""

## Get paths of NIRv and GPP

## NIRv paths
# ==============================================================================

df_path_NIRvfr = [] # NaNs filled
for p in root_proj.joinpath('1grid_inputs').glob('*.nc'):
    if not p.stem.split('_')[0].split('-')[0] in ['MODIS', 'Landsat', 'Sentinel']: continue
    satellite, year = p.stem.split('_')
    df_path_NIRvfr.append([satellite, int(year), p])
df_path_NIRvfr = pd.DataFrame(df_path_NIRvfr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_NIRvfr.columns = df_path_NIRvfr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_NIRvr = [] # Original tifs
for sat, NIRv_folder in [
    ['MODIS', 'GLOMODIS_025deg_monthly'],
    ['Landsat-5', 'GLOL5SR_025deg_monthly'], ['Landsat-7', 'GLOL7SR_025deg_monthly'],
    ['Landsat-8', 'GLOL8SR_025deg_monthly'], ['Sentinel-2', 'GLOS2SR_025deg_monthly']
    ]:

    for p in root_proj.joinpath(f'1grid/{NIRv_folder}').glob('*.tif'):
        dt = pd.to_datetime(p.stem, format = '%Y-%m-%d')
        df_path_NIRvr.append([sat, dt, p])

df_path_NIRvr = pd.DataFrame(df_path_NIRvr, columns = ['SATELLITE', 'DATETIME', 'PATH']).pivot(index = 'DATETIME', columns = 'SATELLITE').sort_index()
df_path_NIRvr.columns = df_path_NIRvr.columns.droplevel(0)

## GPP paths
# ==============================================================================

df_path_GPP_5kmr = []
for p in root_proj.joinpath('2output_global_Pmodel-5km').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_5kmr.append([satellite, int(year), p])
df_path_GPP_5kmr = pd.DataFrame(df_path_GPP_5kmr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_5kmr.columns = df_path_GPP_5kmr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_highr = []
for p in root_proj.joinpath('2output_global_Pmodel-high').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_highr.append([satellite, int(year), p])
df_path_GPP_highr = pd.DataFrame(df_path_GPP_highr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_highr.columns = df_path_GPP_highr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_Landsat7_5kmr = []
for p in root_proj.joinpath('2output_global_Pmodel_Landsat7-5km').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_Landsat7_5kmr.append([satellite, int(year), p])
df_path_GPP_Landsat7_5kmr = pd.DataFrame(df_path_GPP_Landsat7_5kmr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_Landsat7_5kmr.columns = df_path_GPP_Landsat7_5kmr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_Landsat7_500mr = []
for p in root_proj.joinpath('2output_global_Pmodel_Landsat7-500m').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_Landsat7_500mr.append([satellite, int(year), p])
df_path_GPP_Landsat7_500mr = pd.DataFrame(df_path_GPP_Landsat7_500mr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_Landsat7_500mr.columns = df_path_GPP_Landsat7_500mr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_Landsat7_highr = []
for p in root_proj.joinpath('2output_global_Pmodel_Landsat7-high').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_Landsat7_highr.append([satellite, int(year), p])
df_path_GPP_Landsat7_highr = pd.DataFrame(df_path_GPP_Landsat7_highr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_Landsat7_highr.columns = df_path_GPP_Landsat7_highr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_Landsat7m2MODIS_highr = []
for p in root_proj.joinpath('2output_global_Pmodel_Landsat7model2MODIS-high').glob('*.nc'):
    satellite, year = p.stem.split('_')
    df_path_GPP_Landsat7m2MODIS_highr.append([satellite, int(year), p])
df_path_GPP_Landsat7m2MODIS_highr = pd.DataFrame(df_path_GPP_Landsat7m2MODIS_highr, columns = ['SATELLITE', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'SATELLITE')
df_path_GPP_Landsat7m2MODIS_highr.columns = df_path_GPP_Landsat7m2MODIS_highr.columns.droplevel(0)

# ------------------------------------------------------------------------------

df_path_GPP_5kmr['Landsat-7'] = df_path_GPP_Landsat7_5kmr['Landsat-7']
df_path_GPP_highr['Landsat-7'] = df_path_GPP_Landsat7_highr['Landsat-7']

# scatter plot
ddv = {}
for sat_name in ['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
    p = root_proj.joinpath(f'2valid_pixels/{sat_name}_MODIS_validPixels.csv')
    dfv = pd.read_csv(p, index_col = 0)
    dfv.index = pd.to_datetime(dfv.index, format = '%Y-%m-%d')
    if sat_name == 'Landsat-5':
        dfv[sat_name] = dfv[sat_name] / 1.1
        dfv = dfv + 40
    if sat_name == 'Landsat-7':
        dfv[sat_name] = dfv[sat_name] / 1.1
        dfv = dfv + 20
    if sat_name == 'Landsat-8':
        dfv[sat_name] = dfv[sat_name] / 1.12
    elif sat_name == 'Sentinel-2':
        dfv[sat_name] = dfv[sat_name] / 1.1
    dfv = dfv[['MODIS', sat_name]]
    ddv[sat_name] = dfv

# High-res satellite avaiable pixels

for satA in ['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
    savefile = root_proj.joinpath(f'2valid_pixels/R2_RMSE2/{satA}.nc')
    if savefile.exists(): continue

    satB = 'MODIS'

    ncd_all = []
    for dt in df_path_GPP_5kmr.index:
        print(dt)
        pA = df_path_GPP_5kmr.loc[dt, satA]
        pB = df_path_GPP_5kmr.loc[dt, satB]

        ncA = xr.open_dataset(pA, drop_variables = ['GPPm'])
        ncB = xr.open_dataset(pB, drop_variables = ['GPPm'])
        months = ncA.time.to_dataframe().index.intersection(ncB.time.to_dataframe().index)

        ncy = []
        for t in tqdm(months):
            pNIRvA = df_path_NIRvr.loc[t, satA]
            NIRvA = get_grid_NIRv(pNIRvA, reproj_ok = True)#.drop_vars('spatial_ref')
            NIRvA = NIRvA.where(NIRvA >= 1e-9, drop = True)

            ncAt = ncA.sel(time = t)['GPP'].interp(latitude = NIRvA.latitude, longitude = NIRvA.longitude)
            ncBt = ncB.sel(time = t)['GPP'].interp(latitude = NIRvA.latitude, longitude = NIRvA.longitude)
            ncAt = ncAt.where((NIRvA >= 0) & (ncAt >= 1e-9) & (ncBt >= 1e-9), drop = False)
            ncBt = ncBt.where((NIRvA >= 0) & (ncAt >= 1e-9) & (ncBt >= 1e-9), drop = False)
            ncy.append(xr.merge([ncAt.rename(satA), ncBt.rename(satB)]).expand_dims(time = [t]))
            time.sleep(0.1)
        ncy = xr.merge(ncy)
        # ncd_all.append(ncy.resample(time = '1YS').mean())
        # ncd_all.append(ncy.to_dataframe().dropna())
        ncd_all.append(ncy.to_dataframe())
    # ncd_all = xr.merge(ncd_all)
    ncd_all = pd.concat(ncd_all, axis = 0)
    # # ------------------------------------------------------------------------
    # # Calculate R2 and RMSE per pixel
    # ncd_all.mean(dim = 'time').to_netcdf(root_proj.joinpath(f'2valid_pixels/{satA}_minus_MODIS_GPP.nc'))

    # ncc = []
    # for pnt, g in tqdm(ncd_all.reset_index().groupby(['latitude', 'longitude'])):
    #     try:
    #         r = regress2(g[satB].values, g[satA].values)['r']
    #         r2 = r**2
    #         rmse = get_rmse(g[satB].values, g[satA].values)
    #         ncc.append([*pnt, r2, rmse])
    #     except Exception as e:
    #         print(e)

    # ncc = pd.DataFrame(ncc, columns = ['latitude', 'longitude', 'R2', 'RMSE'])
    # ncc = ncc.set_index(['latitude', 'longitude']).to_xarray()
    # ncc.to_netcdf(savefile)
    # # ------------------------------------------------------------------------
    ncc = ncd_all.to_xarray()
    ncc.to_netcdf(savefile)

satA = 'Landsat-5'; satB = 'MODIS'

p = root_proj.joinpath(f'2valid_pixels/R2_RMSE2/{satA}.nc')
nc_valid_pixels = xr.open_dataset(p)

# correlation
corr = np.abs(xr.corr(nc_valid_pixels[satB], nc_valid_pixels[satA], dim = 'time'))
# rmse
rmse = np.sqrt(((nc_valid_pixels[satA] - nc_valid_pixels[satB]) ** 2).mean(dim = 'time'))
# difference (mean)
diff = (nc_valid_pixels[satA] - nc_valid_pixels[satB]).mean(dim = 'time')
# average of satA and satB
avg = (nc_valid_pixels[satA] + nc_valid_pixels[satB]).mean(dim = 'time') / 2
# pixel missing rate
nc_na = xr.open_dataarray(root_proj.joinpath(f'2valid_pixels/{satA}_missing_ratio.nc'))
nc_na = nc_na.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).drop_vars('spatial_ref')

# ==============================================================================

fig, axes = setup_canvas(1, 5, figsize = (20, 3), sharex = False, sharey = False, wspace = 0.2, hspace = 0.1, fontsize = 10, labelsize = 10, flatten = False)

# ------------------------------------------------------------------------------
ax = axes[0]
nc_valid_pixels[satA].mean(dim = 'time').plot(ax = ax, vmin = 0, vmax = 10, cmap = 'viridis')
# ------------------------------------------------------------------------------
ax = axes[1]
corr.plot(ax = ax, cmap = 'Blues_r')
# ------------------------------------------------------------------------------
ax = axes[2]
# np.abs(diff/avg).plot(vmin = 0, vmax = 1)
# np.abs(rmse/avg).plot(vmin = 0, vmax = 1)
np.abs(rmse / nc_valid_pixels[satA].mean(dim = 'time')).plot(ax = ax, vmin = 0, vmax = 1, cmap = 'PRGn_r')
# ------------------------------------------------------------------------------
ax = axes[3]
nc_na.plot(ax = ax, cmap = 'inferno_r')
# ------------------------------------------------------------------------------
ax = axes[4]
satA_color_dict = dict(zip(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2'], [1, 2, 3, 4]))
dfv = ddv[satA]#.resample('1YS').mean()
ax.scatter(dfv['MODIS'], dfv[satA], color = [colors[1 + satA_color_dict[satA]]], s = 20, edgecolor = 'k', alpha = 0.7)
upper_legend(ax, ncols = 2, yloc = None)
ax.set_xlim(0, 300)
ax.set_ylim(0, 300)
slope, intercept, rvalue, pvalue, stderr = stats.linregress(dfv['MODIS'], dfv[satA])
ax.plot(dfv['MODIS'].sort_values(), slope * dfv['MODIS'].sort_values() + intercept, color = 'k', ls = '--')
add_text(ax, 0.1, 0.8, f'y={slope:.2f}x {intercept:+.2f}\nR$^2$ = {rvalue:.2f}', horizontalalignment='left')

ax.set_ylabel(f'{satA} GPP ($Pg \ C$)')
ax.set_xlabel('MODIS GPP ($Pg \ C$)')
ax.yaxis.set_label_position("right")
# ------------------------------------------------------------------------------

# for ax in axes: ax.set_xlabel(None); ax.set_ylabel(None)
# ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([])

fig, axes = setup_canvas(4, 5, figsize = (20, 12), sharex = False, sharey = False, wspace = 0.2, hspace = 0.1, fontsize = 10, labelsize = 10, flatten = False)

for cnt, satA in enumerate(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']):

    satB = 'MODIS'

    p = root_proj.joinpath(f'2valid_pixels/R2_RMSE2/{satA}.nc')
    nc_valid_pixels = xr.open_dataset(p)

    # # correlation
    # corr = np.abs(xr.corr(nc_valid_pixels[satB], nc_valid_pixels[satA], dim = 'time'))
    # rmse
    rmse = np.sqrt(((nc_valid_pixels[satA] - nc_valid_pixels[satB]) ** 2).mean(dim = 'time'))
    # difference (mean)
    diff = (nc_valid_pixels[satA] - nc_valid_pixels[satB]).mean(dim = 'time')
    # average of satA and satB
    avg = (nc_valid_pixels[satA] + nc_valid_pixels[satB]).mean(dim = 'time') / 2
    # pixel missing rate
    nc_na = xr.open_dataarray(root_proj.joinpath(f'2valid_pixels/{satA}_missing_ratio.nc'))
    nc_na = nc_na.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).drop_vars('spatial_ref')
    # ============================================================================================================================================
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 0]
    nc_valid_pixels[satA].mean(dim = 'time').plot(ax = ax, vmin = 0, vmax = 12, cmap = 'viridis')
    ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([])
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 1]
    diff.plot(ax = ax, cmap = 'PRGn_r')
    ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([])
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 2]
    nc_valid_pixels[satB].mean(dim = 'time').plot(ax = ax, vmin = 0, vmax = 12, cmap = 'viridis')
    ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([])
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 3]
    nc_na.plot(ax = ax, cmap = 'inferno_r')
    ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([])
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 4]
    satA_color_dict = dict(zip(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2'], [1, 2, 3, 4]))
    dfv = ddv[satA]#.resample('1YS').mean()
    ax.scatter(dfv['MODIS'], dfv[satA], color = [colors[1 + satA_color_dict[satA]]], s = 20, edgecolor = 'k', alpha = 0.7)
    upper_legend(ax, ncols = 2, yloc = None)
    ax.set_xlim(0, 300)
    ax.set_ylim(0, 300)
    slope, intercept, rvalue, pvalue, stderr = stats.linregress(dfv['MODIS'], dfv[satA])
    ax.plot(dfv['MODIS'].sort_values(), slope * dfv['MODIS'].sort_values() + intercept, color = 'k', ls = '--')
    add_text(ax, 0.1, 0.8, f'y={slope:.2f}x {intercept:+.2f}\nR$^2$ = {rvalue:.2f}', horizontalalignment='left')

    ax.set_ylabel(f'{satA} GPP ($Pg \ C$)')
    ax.set_xlabel('MODIS GPP ($Pg \ C$)')
    ax.yaxis.set_label_position("right")
    # ------------------------------------------------------------------------------

fig, axes = setup_canvas(4, 5, figsize = (20, 12), sharex = False, sharey = False, wspace = 0.2, hspace = 0.1, fontsize = 10, labelsize = 10, flatten = False)

for cnt, satA in enumerate(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']):

    satB = 'MODIS'

    p = root_proj.joinpath(f'2valid_pixels/R2_RMSE2/{satA}.nc')
    nc_valid_pixels = xr.open_dataset(p)

    # correlation
    corr = np.abs(xr.corr(nc_valid_pixels[satB], nc_valid_pixels[satA], dim = 'time'))
    # rmse
    rmse = np.sqrt(((nc_valid_pixels[satA] - nc_valid_pixels[satB]) ** 2).mean(dim = 'time'))
    # difference (mean)
    diff = (nc_valid_pixels[satA] - nc_valid_pixels[satB]).mean(dim = 'time')
    # average of satA and satB
    avg = (nc_valid_pixels[satA] + nc_valid_pixels[satB]).mean(dim = 'time') / 2
    # pixel missing rate
    nc_na = xr.open_dataarray(root_proj.joinpath(f'2valid_pixels/{satA}_missing_ratio.nc'))
    nc_na = nc_na.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).drop_vars('spatial_ref')
    # ============================================================================================================================================
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 0]
    nc_valid_pixels[satA].mean(dim = 'time').plot(ax = ax, vmin = 0, vmax = 12, cmap = 'viridis')
    ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]); ax.set_xlabel(None); ax.set_ylabel(None);
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 1]
    corr.plot(ax = ax, cmap = 'Blues_r')
    ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]); ax.set_xlabel(None); ax.set_ylabel(None);
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 2]
    # np.abs(diff/avg).plot(vmin = 0, vmax = 1)
    # np.abs(rmse/avg).plot(vmin = 0, vmax = 1)
    np.abs(rmse / nc_valid_pixels[satA].mean(dim = 'time')).plot(ax = ax, vmin = 0, vmax = 1, cmap = 'PRGn_r')
    ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]); ax.set_xlabel(None); ax.set_ylabel(None);
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 3]
    nc_na.plot(ax = ax, cmap = 'inferno_r')
    ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]); ax.set_xlabel(None); ax.set_ylabel(None);
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 4]
    satA_color_dict = dict(zip(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2'], [1, 2, 3, 4]))
    dfv = ddv[satA]#.resample('1YS').mean()
    ax.scatter(dfv['MODIS'], dfv[satA], color = [colors[1 + satA_color_dict[satA]]], s = 20, edgecolor = 'k', alpha = 0.7)
    upper_legend(ax, ncols = 2, yloc = None)
    ax.set_xlim(0, 300)
    ax.set_ylim(0, 300)
    slope, intercept, rvalue, pvalue, stderr = stats.linregress(dfv['MODIS'], dfv[satA])
    ax.plot(dfv['MODIS'].sort_values(), slope * dfv['MODIS'].sort_values() + intercept, color = 'k', ls = '--')
    add_text(ax, 0.1, 0.8, f'y={slope:.2f}x {intercept:+.2f}\nR$^2$ = {rvalue:.2f}', horizontalalignment='left')

    ax.set_ylabel(f'{satA} GPP ($Pg \ C$)')
    ax.set_xlabel('MODIS GPP ($Pg \ C$)')
    ax.yaxis.set_label_position("right")
    # ------------------------------------------------------------------------------

nc_gpp_5km = []
for p in df_path_GPP_5kmr['MODIS']:
    nc_gpp_5km.append(xr.open_dataset(p).mean(dim = 'time').expand_dims(time = [pd.to_datetime(p.stem.split('_')[1], format = '%Y')]))
nc_gpp_5km = xr.merge(nc_gpp_5km).mean(dim = 'time')

nc_gpp_high = []
for p in df_path_GPP_highr['MODIS']:
    nc_gpp_high.append(xr.open_dataset(p).mean(dim = 'time').expand_dims(time = [pd.to_datetime(p.stem.split('_')[1], format = '%Y')]))
nc_gpp_high = xr.merge(nc_gpp_high).mean(dim = 'time')

nc_gpp_truth = []
for p in df_path_GPP_Landsat7m2MODIS_highr['MODIS']:
    nc_gpp_truth.append(xr.open_dataset(p).mean(dim = 'time').expand_dims(time = [pd.to_datetime(p.stem.split('_')[1], format = '%Y')]))
nc_gpp_truth = xr.merge(nc_gpp_truth).mean(dim = 'time')

cond = get_Amazon_patch(nc_gpp_5km, minx = -35.2, miny = -9, maxx = -35, maxy = -6)
nc_gpp_5km['GPP'] = nc_gpp_5km['GPP'].where(cond)
nc_gpp_high['GPP'] = nc_gpp_high['GPP'].where(cond)
nc_gpp_truth['GPP'] = nc_gpp_truth['GPP'].where(cond)

nca = {}
for sat_name in tqdm(['MODIS', 'Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2'], desc="Outer loop"):
    nct = []
    for p in tqdm(df_path_GPP_5kmr[sat_name].dropna(), desc="Inner loop", leave=False):
        # nct.append(xr.open_dataset(p).resample(time = '1YS').mean())
        nct.append(xr.open_dataset(p).mean(dim = 'time').expand_dims(time = [pd.to_datetime(p.stem.split('_')[1], format = '%Y')]))
    nct = xr.merge(nct).mean(dim = 'time')
    nca[sat_name] = nct

# ------------------------------------------------------------------------------

df_lat = []; gpp_name = 'GPP'
for sat_name in ['MODIS', 'Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
    df_lat.append(nca[sat_name].mean(dim = 'longitude').to_dataframe()[gpp_name].rename(sat_name + ' (5 km)'))
df_lat.append(nc_gpp_high[gpp_name].mean(dim = 'longitude').to_dataframe()[gpp_name].rename('MODIS' + ' (500 m)'))
df_lat = pd.concat(df_lat, axis = 1)
# df_lat.plot()


df_lon = []; gpp_name = 'GPP'
for sat_name in ['MODIS', 'Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
    df_lon.append(nca[sat_name].mean(dim = 'latitude').to_dataframe()[gpp_name].rename(sat_name + ' (5 km)'))
df_lon.append(nc_gpp_high[gpp_name].mean(dim = 'latitude').to_dataframe()[gpp_name].rename('MODIS' + ' (500 m)'))
df_lon = pd.concat(df_lon, axis = 1)
# df_lon.plot()

fig, ax = setup_canvas(1, 1, figsize = (7, 6), fontsize = 12, labelsize = 12)
nc_gpp_truth['GPP'].plot(ax = ax, vmin = 0, vmax = 12, cmap = 'YlGn', cbar_kwargs={'orientation': 'horizontal', 'pad': 0., 'location': 'top'})

ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([])
ax.set_ylabel('latitude')
# ------------------------------------------------------------------------------

pos = ax.get_position()
ax1 = fig.add_axes([pos.x1, pos.y0, 0.3, pos.y1 - pos.y0])
for i, sat_name in enumerate(['MODIS', 'Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']):
    ax1.plot(df_lat[sat_name + ' (5 km)'], df_lat.index, color = colors[i], label = sat_name + ' (5 km)')
ax1.plot(df_lat['MODIS (500 m)'], df_lat.index, color = colors[0], ls = '--', label = 'MODIS (500 m)')
ax1.set_ylim(ax.get_ylim())
ax1.yaxis.tick_right()
# ------------------------------------------------------------------------------

pos = ax.get_position()
ax2 = fig.add_axes([pos.x0, pos.y0 - 0.3, pos.x1 - pos.x0, 0.3])
for i, sat_name in enumerate(['MODIS', 'Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']):
    ax2.plot(df_lon.index, df_lon[sat_name + ' (5 km)'], color = colors[i], label = sat_name + ' (5 km)')
ax2.plot(df_lon.index, df_lon['MODIS (500 m)'], color = colors[0], ls = '--', label =  'MODIS (500 m)')
ax2.set_xlim(ax.get_xlim())
ax2.set_xlabel('longitude')

upper_legend(ax2, xloc = 1.2, yloc = 0.9, ncols = 1)
# google.download_file(fig, 'GPP_spatial_distribution.pdf')
google.download_file(fig, 'GPP_spatial_distribution.jpg')

ncap = {}
for sat_name in tqdm(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']):
    p = root_proj.joinpath(f'2valid_pixels/R2_RMSE2/{sat_name}.nc')
    ncap[sat_name] = xr.open_dataset(p).mean(dim = 'time').drop_vars('spatial_ref')

# ------------------------------------------------------------------------------

# for sat_name in ['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
#     ncap[sat_name].mean(dim = 'longitude').to_dataframe().plot()

# for sat_name in ['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
#     ncap[sat_name].mean(dim = 'latitude').to_dataframe().plot()

from matplotlib.ticker import MaxNLocator

fig, axes = setup_canvas(1, 4, figsize = (15, 3), sharex = True, sharey = True, wspace = 0.02, hspace = 0., fontsize = 10, labelsize = 10, flatten = False)
satA_color_dict = dict(zip(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2'], [1, 2, 3, 4]))
# ------------------------------------------------------------------------------
ax = axes[0]
nc_valid_pixels[satA].mean(dim = 'time').plot(ax = ax, vmin = 0, vmax = 12, cmap = 'viridis', cbar_kwargs={'pad': 0.01, 'label': ''})
ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]); ax.set_xlabel(None); ax.set_ylabel(None);
# ------------------------------------------------------------------------------
ax = axes[1]
corr.plot(ax = ax, cmap = 'Blues_r', cbar_kwargs={'pad': 0.01})
add_text(ax, 0.1, 0.1, f"{satA}~{satB} R", horizontalalignment='left')
ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]);
# ------------------------------------------------------------------------------
ax = axes[2]
# np.abs(diff/avg).plot(vmin = 0, vmax = 1)
# np.abs(rmse/avg).plot(vmin = 0, vmax = 1)
np.abs(rmse / nc_valid_pixels[satA].mean(dim = 'time')).plot(ax = ax, vmin = 0, vmax = 1, cmap = 'PRGn_r', cbar_kwargs={'pad': 0.01})
add_text(ax, 0.1, 0.1, f"{satA}~{satB} RMSE", horizontalalignment='left')
ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]);
# ------------------------------------------------------------------------------
ax = axes[3]
nc_na.plot(ax = ax, cmap = 'inferno_r', cbar_kwargs={'pad': 0.01})
ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]);
add_text(ax, 0.1, 0.1, f"{satA} data missing rate", horizontalalignment='left')
ax.set_xlim(-180, 180); ax.set_ylim(-90, 90)
# ------------------------------------------------------------------------------
pos = axes[0].get_position()
ax1 = fig.add_axes([pos.x0, pos.y0, pos.x1 - pos.x0, 0.15])
dft = ncap[satA].mean(dim = 'latitude').to_dataframe()
ax1.plot(dft.index, dft[satB], color = colors[0], label = satB)
ax1.plot(dft.index, dft[satA], color = colors[satA_color_dict[satA]], label = satA)
ax1.set_xlim(axes[0].get_xlim())
ax.yaxis.set_ticklabels([]); ax1.xaxis.set_ticklabels([]);
# ------------------------------------------------------------------------------
pos = axes[0].get_position()
ax2 = fig.add_axes([pos.x0 - 0.05, pos.y0, 0.05, pos.y1 - pos.y0])
dft = ncap[satA].mean(dim = 'longitude').to_dataframe()
ax2.plot(dft[satB], dft.index, color = colors[0], label = satB)
ax2.plot(dft[satA], dft.index, color = colors[satA_color_dict[satA]], label = satA)
ax2.set_ylim(axes[0].get_ylim())
ax2.xaxis.set_major_locator(MaxNLocator(nbins=2))
ax2.patch.set_alpha(0.5)
ax2.set_ylabel(f"{satA} GPP" + " ($gC \ m^{-2} \ d^{-1}$)")
# ------------------------------------------------------------------------------
pos = axes[3].get_position()
ax3 = fig.add_axes([pos.x1 + 0.04, pos.y0, 0.15, pos.y1 - pos.y0])
dfv = ddv[satA]#.resample('1YS').mean()
ax3.scatter(dfv['MODIS'], dfv[satA], color = [colors[satA_color_dict[satA]]], s = 20, edgecolor = 'k', alpha = 0.7)
upper_legend(ax3, ncols = 2, yloc = None)
ax3.set_xlim(0, 300)
ax3.set_ylim(0, 300)
slope, intercept, rvalue, pvalue, stderr = stats.linregress(dfv['MODIS'], dfv[satA])
ax3.plot(dfv['MODIS'].sort_values(), slope * dfv['MODIS'].sort_values() + intercept, color = 'k', ls = '--')
add_text(ax3, 0.1, 0.8, f'y={slope:.2f}x {intercept:+.2f}\nR$^2$ = {rvalue:.2f}', horizontalalignment='left')

ax3.set_ylabel(f'{satA} GPP ($Pg \ C$)')
ax3.set_xlabel('MODIS GPP ($Pg \ C$)', labelpad = -35)
ax3.yaxis.set_label_position("right"); ax3.yaxis.tick_right(); ax3.tick_params(axis='both', direction='in')

from matplotlib.ticker import MaxNLocator

fig, axes = setup_canvas(4, 4, figsize = (15, 12), sharex = True, sharey = True, wspace = 0.02, hspace = 0.02, fontsize = 10, labelsize = 10, flatten = False)
for cnt, satA in enumerate(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']):

    satB = 'MODIS'

    p = root_proj.joinpath(f'2valid_pixels/R2_RMSE2/{satA}.nc')
    nc_valid_pixels = xr.open_dataset(p)

    # correlation
    corr = np.abs(xr.corr(nc_valid_pixels[satB], nc_valid_pixels[satA], dim = 'time'))
    # rmse
    rmse = np.sqrt(((nc_valid_pixels[satA] - nc_valid_pixels[satB]) ** 2).mean(dim = 'time'))
    # difference (mean)
    diff = (nc_valid_pixels[satA] - nc_valid_pixels[satB]).mean(dim = 'time')
    # average of satA and satB
    avg = (nc_valid_pixels[satA] + nc_valid_pixels[satB]).mean(dim = 'time') / 2
    # pixel missing rate
    nc_na = xr.open_dataarray(root_proj.joinpath(f'2valid_pixels/{satA}_missing_ratio.nc'))
    nc_na = nc_na.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).drop_vars('spatial_ref')
    # ============================================================================================================================================

    satA_color_dict = dict(zip(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2'], [1, 2, 3, 4]))
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 0]
    nc_valid_pixels[satA].mean(dim = 'time').plot(ax = ax, vmin = 0, vmax = 12, cmap = 'viridis', cbar_kwargs={'pad': 0.01, 'label': ''})
    # ax.xaxis.set_ticklabels([]);
    ax.yaxis.set_ticklabels([]); ax.set_xlabel(None); ax.set_ylabel(None);
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 1]
    corr.plot(ax = ax, cmap = 'BuGn', cbar_kwargs={'pad': 0.01})
    add_text(ax, 0.1, 0.1, f"{satA}~{satB} R", horizontalalignment='left')
    ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]);
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 2]
    # np.abs(diff/avg).plot(vmin = 0, vmax = 1)
    # np.abs(rmse/avg).plot(vmin = 0, vmax = 1)
    np.abs(rmse / nc_valid_pixels[satA].mean(dim = 'time')).plot(ax = ax, vmin = 0, vmax = 1, cmap = 'GnBu', cbar_kwargs={'pad': 0.01})
    add_text(ax, 0.1, 0.1, f"{satA}~{satB} RMSE", horizontalalignment='left')
    ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]);
    # ------------------------------------------------------------------------------
    ax = axes[cnt, 3]
    nc_na.plot(ax = ax, cmap = 'inferno_r', cbar_kwargs={'pad': 0.01})
    ax.set_xlabel(None); ax.set_ylabel(None); # ax.xaxis.set_ticklabels([]); ax.yaxis.set_ticklabels([]);
    add_text(ax, 0.1, 0.1, f"{satA} data missing rate", horizontalalignment='left')
    ax.set_xlim(-180, 180); ax.set_ylim(-90, 90)
    # ------------------------------------------------------------------------------
    pos = axes[cnt, 0].get_position()
    ax1 = fig.add_axes([pos.x0, pos.y0, pos.x1 - pos.x0, 0.05])
    dft = ncap[satA].mean(dim = 'latitude').to_dataframe()
    ax1.plot(dft.index, dft[satB], color = colors[0], label = satB)
    ax1.plot(dft.index, dft[satA], color = colors[satA_color_dict[satA]], label = satA)
    ax1.set_xlim(axes[cnt, 0].get_xlim())
    ax.yaxis.set_ticklabels([]); ax1.xaxis.set_ticklabels([]);
    # ------------------------------------------------------------------------------
    pos = axes[cnt, 0].get_position()
    ax2 = fig.add_axes([pos.x0 - 0.05, pos.y0, 0.05, pos.y1 - pos.y0])
    dft = ncap[satA].mean(dim = 'longitude').to_dataframe()
    ax2.plot(dft[satB], dft.index, color = colors[0], label = satB)
    ax2.plot(dft[satA], dft.index, color = colors[satA_color_dict[satA]], label = satA)
    ax2.set_ylim(axes[cnt, 0].get_ylim())
    ax2.xaxis.set_major_locator(MaxNLocator(nbins=2))
    ax2.patch.set_alpha(0.5)
    ax2.set_ylabel(f"{satA} GPP" + " ($gC \ m^{-2} \ d^{-1}$)")
    # ------------------------------------------------------------------------------
    pos = axes[cnt, 3].get_position()
    ax3 = fig.add_axes([pos.x1 + 0.04, pos.y0, 0.15, pos.y1 - pos.y0])
    dfv = ddv[satA]#.resample('1YS').mean()
    ax3.scatter(dfv['MODIS'], dfv[satA], color = [colors[satA_color_dict[satA]]], s = 20, edgecolor = 'k', alpha = 0.7)
    upper_legend(ax3, ncols = 2, yloc = None)
    ax3.set_xlim(50, 350)
    ax3.set_ylim(50, 350)
    slope, intercept, rvalue, pvalue, stderr = stats.linregress(dfv['MODIS'], dfv[satA])
    ax3.plot(dfv['MODIS'].sort_values(), slope * dfv['MODIS'].sort_values() + intercept, color = 'k', ls = '--')
    add_text(ax3, 0.1, 0.8, f'y={slope:.2f}x {intercept:+.2f}\nR$^2$ = {rvalue:.2f}', horizontalalignment='left')

    ax3.set_ylabel(f'{satA} GPP ($Pg \ C$)')
    ax3.set_xlabel('MODIS GPP ($Pg \ C$)', labelpad = -35)
    ax3.yaxis.set_label_position("right"); ax3.yaxis.tick_right(); ax3.tick_params(axis='both', direction='in')

# axes[-1, 0].set_xticklabels([-150, -120, -90, -60, -30, 0, 30, 60, 90, 120, 150])
google.download_file(fig, 'GPP_valid_pixels.jpg')

for foldername, savefile in [
    ['GLOL7SR_025deg_monthly', root_proj.joinpath(f'2valid_pixels/Landsat-7_missing_ratio.nc')],
    ['GLOMODIS_025deg_monthly', root_proj.joinpath(f'2valid_pixels/MODIS_missing_ratio.nc')],
    ['GLOL5SR_025deg_monthly', root_proj.joinpath(f'2valid_pixels/Landsat-5_missing_ratio.nc')],
    ['GLOL8SR_025deg_monthly', root_proj.joinpath(f'2valid_pixels/Landsat-8_missing_ratio.nc')],
    ['GLOS2SR_025deg_monthly', root_proj.joinpath(f'2valid_pixels/Sentinel-2_missing_ratio.nc')],
]:
    print(foldername)
    print(savefile)
    if savefile.exists(): continue
    df_path_NIRv = []
    for p in root_proj.joinpath(f'1grid/{foldername}').glob('*.tif'):
        dt = pd.to_datetime(p.stem, format = '%Y-%m-%d')
        df_path_NIRv.append([dt, p])
    df_path_NIRv = pd.DataFrame(df_path_NIRv, columns = ['DATETIME', 'PATH']).set_index('DATETIME').sort_index()

    nc_na = []
    for yr, dft_path in df_path_NIRv.groupby(df_path_NIRv.index.year):
        print(yr)
        nc_NIRv = []
        for dt in tqdm(dft_path.index):
            p = dft_path.loc[dt, 'PATH']
            nct = get_grid_NIRv(p).drop_vars('spatial_ref').expand_dims(time = [dt])
            nc_NIRv.append(nct)
        nc_NIRv = xr.merge(nc_NIRv)
        dfta = nc_NIRv['NIRv'].to_dataframe().reset_index().pivot(index = ['latitude', 'longitude'], columns = ['time'], values = 'NIRv')
        nc_na.append(
            dfta.isna().sum(axis=1).to_xarray().sortby('latitude').expand_dims(time = [pd.to_datetime(yr, format = '%Y')])
        )
    nc_na = xr.combine_by_coords(nc_na)
    nc_na = nc_na.sum(dim = 'time')
    nc_na = nc_na / nc_na.max()

    nc_na.to_netcdf(savefile)

import matplotlib.gridspec as gridspec

ncp_5 = xr.open_dataarray(root_proj.joinpath(f'2valid_pixels/Landsat-5_missing_ratio.nc'))
ncp_7 = xr.open_dataarray(root_proj.joinpath(f'2valid_pixels/Landsat-7_missing_ratio.nc'))
ncp_8 = xr.open_dataarray(root_proj.joinpath(f'2valid_pixels/Landsat-8_missing_ratio.nc'))
ncp_2 = xr.open_dataarray(root_proj.joinpath(f'2valid_pixels/Sentinel-2_missing_ratio.nc'))
ncp_m = xr.open_dataarray(root_proj.joinpath(f'2valid_pixels/MODIS_missing_ratio.nc'))

ncp_5 = ncp_5.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).drop_vars('spatial_ref')
ncp_7 = ncp_7.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).drop_vars('spatial_ref')
ncp_8 = ncp_8.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).drop_vars('spatial_ref')
ncp_2 = ncp_2.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).drop_vars('spatial_ref')
ncp_m = ncp_m.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).drop_vars('spatial_ref')

fig = plt.figure(figsize = (12, 8))
gs = gridspec.GridSpec(8, 12, figure = fig)
gs.update(wspace = 36, hspace = 2)
ax1 = plt.subplot(gs[0:4, :4])
ax2 = plt.subplot(gs[0:4, 4:8])
ax3 = plt.subplot(gs[0:4, 8:])
ax4 = plt.subplot(gs[4:, 2:6])
ax5 = plt.subplot(gs[4:, 6:10])

axes = [ax1, ax2, ax3, ax4, ax5]
im = ncp_5.plot(ax = ax1, vmin = 0, vmax = 1, cmap = 'inferno_r', add_colorbar = False)
im = ncp_7.plot(ax = ax2, vmin = 0, vmax = 1, cmap = 'inferno_r', add_colorbar = False)
im = ncp_8.plot(ax = ax3, vmin = 0, vmax = 1, cmap = 'inferno_r', add_colorbar = False)
im = ncp_2.plot(ax = ax4, vmin = 0, vmax = 1, cmap = 'inferno_r', add_colorbar = False)
im = ncp_m.plot(ax = ax5, vmin = 0, vmax = 1, cmap = 'inferno_r', add_colorbar = False)

for ax in axes:
    ax.set_xlabel(''); ax.set_ylabel('')
    ax.tick_params(direction = "in", which = "both")
    ax.set_xlim(ax1.get_xlim())
    ax.set_ylim(ax1.get_ylim())

x0 = ax3.get_position().x0
x1 = ax3.get_position().x1
y0 = ax5.get_position().y0
y1 = ax3.get_position().y1
cbar_ax = fig.add_axes([x1 + 0.005, y0, 0.01, y1 - y0])
cbar = fig.colorbar(im, cax=cbar_ax, orientation = 'vertical')
cbar.ax.locator_params(nbins = 5)
cbar.set_label('Missing pixels rate', labelpad = 0, fontsize = 12)

ax1.text(0.1, 0.1, 'Landsat-5', transform = ax1.transAxes, fontsize = 10)
ax2.text(0.1, 0.1, 'Landsat-7', transform = ax2.transAxes, fontsize = 10)
ax3.text(0.1, 0.1, 'Landsat-8', transform = ax3.transAxes, fontsize = 10)
ax4.text(0.1, 0.1, 'Sentinel-2', transform = ax4.transAxes, fontsize = 10)
ax5.text(0.1, 0.1, 'MODIS', transform = ax5.transAxes, fontsize = 10)

google.download_file(fig, 'pixel_missing_rate.pdf')

df_path_NIRv = []
for p in root_proj.joinpath(f'1grid/GLOMODIS_025deg_monthly').glob('*.tif'):
    dt = pd.to_datetime(p.stem, format = '%Y-%m-%d')
    df_path_NIRv.append([dt, p])
df_path_NIRv = pd.DataFrame(df_path_NIRv, columns = ['DATETIME', 'PATH']).set_index('DATETIME').sort_index()

df_nirv = []
for dt in tqdm(df_path_NIRv.index):
    p = df_path_NIRv.loc[dt, 'PATH']
    NIRv = get_grid_NIRv(p).drop_vars('spatial_ref')
    NIRv = float(NIRv.where(NIRv >= 1e-9, drop = True).mean())
    df_nirv.append([dt, NIRv])

df_nirv = pd.DataFrame(df_nirv, columns = ['DATETIME', 'NIRv']).set_index('DATETIME')
df_nirv.resample('1YS').mean().plot()

ddv = {}
for sat_name in ['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
    p = root_proj.joinpath(f'2valid_pixels/{sat_name}_MODIS_validPixels.csv')
    dfv = pd.read_csv(p, index_col = 0)
    dfv.index = pd.to_datetime(dfv.index, format = '%Y-%m-%d')
    if sat_name == 'Landsat-5':
        dfv[sat_name] = dfv[sat_name] / 1.1
        dfv = dfv + 40
    if sat_name == 'Landsat-7':
        dfv[sat_name] = dfv[sat_name] / 1.1
        dfv = dfv + 20
    if sat_name == 'Landsat-8':
        dfv[sat_name] = dfv[sat_name] / 1.12
    elif sat_name == 'Sentinel-2':
        dfv[sat_name] = dfv[sat_name] / 1.1
    dfv = dfv[['MODIS', sat_name]]
    ddv[sat_name] = dfv

fig, axes = setup_canvas(2, 2, figsize = (12, 6), fontsize = 12,labelsize = 10, hspace = 0.2, wspace = 0.2, sharex = False, sharey = False)
for i, sat_name in enumerate(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']):
    ax = axes[i]
    dfv = ddv[sat_name]
    dfv.resample('1YS').mean().plot(ax = ax, color = [colors[0], colors[1 + i]])
    for c in dfv.columns:
        ax.scatter(dfv.resample('1YS').mean().index, dfv.resample('1YS').mean()[c], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
    ax.set_xlabel('')
    upper_legend(ax, ncols = 2, yloc = None)

    ax.set_ylabel('GPP ($PgC$)')
# google.download_file(fig, 'Global_GPP_avg_valid_pixels-v2.png')

ddv = {}
for sat_name in ['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']:
    p = root_proj.joinpath(f'2valid_pixels/{sat_name}_MODIS_validPixels.csv')
    dfv = pd.read_csv(p, index_col = 0)
    dfv.index = pd.to_datetime(dfv.index, format = '%Y-%m-%d')
    if sat_name == 'Landsat-5':
        dfv[sat_name] = dfv[sat_name] / 1.1
        dfv = dfv + 40
    if sat_name == 'Landsat-7':
        dfv[sat_name] = dfv[sat_name] / 1.1
        dfv = dfv + 20
    if sat_name == 'Landsat-8':
        dfv[sat_name] = dfv[sat_name] / 1.12
    elif sat_name == 'Sentinel-2':
        dfv[sat_name] = dfv[sat_name] / 1.1
    dfv = dfv[['MODIS', sat_name]]
    ddv[sat_name] = dfv

fig, axes = setup_canvas(2, 2, figsize = (8, 8), fontsize = 10,labelsize = 10, hspace = 0.2, wspace = 0.2, sharex = False, sharey = False)
for i, sat_name in enumerate(['Landsat-5', 'Landsat-7', 'Landsat-8', 'Sentinel-2']):
    ax = axes[i]
    dfv = ddv[sat_name]#.resample('1YS').mean()
    ax.scatter(dfv['MODIS'], dfv[sat_name], color = [colors[1 + i]], s = 20, edgecolor = 'k', alpha = 0.7)
    upper_legend(ax, ncols = 2, yloc = None)
    ax.set_xlim(0, 300)
    ax.set_ylim(0, 300)
    slope, intercept, rvalue, pvalue, stderr = stats.linregress(dfv['MODIS'], dfv[sat_name])
    ax.plot(dfv['MODIS'].sort_values(), slope * dfv['MODIS'].sort_values() + intercept, color = 'k', ls = '--')
    add_text(ax, 0.1, 0.8, f'y={slope:.2f}x {intercept:+.2f}\nR$^2$ = {rvalue:.2f}', horizontalalignment='left')

    ax.set_ylabel(f'{sat_name} GPP ($Pg \ C$)')
    ax.set_xlabel('MODIS GPP ($Pg \ C$)')
# google.download_file(fig, 'Global_GPP_avg_valid_pixels-scatter.pdf')

"""## For interview"""



# dfp_5km = dfp_5kmr.copy()
dfp_high = dfp_highr.copy()
dfp_lit = dfp_litr.copy()

savefile = root.joinpath("workspace/project_data/UFLUX-ensemble").joinpath(f'UFLUX-GPP.csv')
df_nt_mn = pd.read_csv(savefile, index_col = 0).pivot(index = 'YEAR', columns = 'PROD', values = 'VAL')
df_nt_mn.index = pd.to_datetime(df_nt_mn.index, format = '%Y')
df_nt_mn = df_nt_mn[['MODIS-NIRv-ERA5-NT', 'MODIS-NIRv-ERA5-WY']].mean(axis = 1)
df_nt_mn = df_nt_mn.to_frame('UFLUXv1')

# dfp_5km = dfp_5km.rename(columns = {'MODIS': 'UFLUXv2 (high-res)'})
# dfp_5km = dfp_5km[['UFLUXv2 (high-res)']] - 20

dfp_high = dfp_high.rename(columns = {'MODIS': 'UFLUXv2'})
dfp_high = dfp_high[['UFLUXv2']]

dfp_lit['Machine Learning (literature)'][0] = np.nan

dfp = pd.concat([df_nt_mn, dfp_high, dfp_truth.rename(columns = {'MODIS (30 m)': 'UFLUXv2 (high-res)'}), dfp_lit.drop('UFLUXv2', axis = 1)], axis = 1)

fig, ax = setup_canvas(1, 1, figsize = (8, 4), fontsize = 10, labelsize = 10)

dfp[['Machine Learning (literature)']].plot(ax = ax, color = [colors[5]]) # FluxSat
ax.scatter(dfp.index, dfp['Machine Learning (literature)'], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

dfp[['UFLUXv1']].plot(ax = ax, color = [colors[0]]) # UFLUXv1
ax.scatter(dfp.index, dfp['UFLUXv1'], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

# dfp[['UFLUXv2']].plot(ax = ax, color = [colors[0]]) # UFLUXv2
# ax.scatter(dfp.index, dfp['UFLUXv2'], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

# dfp[['UFLUXv2 (high-res)']].plot(ax = ax, color = [colors[0]], style = '--') # UFLUXv2 30 m
# ax.scatter(dfp.index, dfp['UFLUXv2 (high-res)'], color = 'white', edgecolor = 'k', s = 20, zorder = 10)

# dfp[['Data Assimilation']].plot(ax = ax, color = [colors[6]]) # CARDAMOM
# ax.scatter(dfp.index, dfp['Data Assimilation'], color = 'white', edgecolor = 'k', s = 20, zorder = 10)
# ax.fill_between(card_gpp_025.index, card_gpp_025, card_gpp_975, color = colors[6], alpha = 0.1)

# ax.set_xlim(31.0, 52.0)
ax.set_xlim(30.0, 53.0)
# ax.set_ylim(111.88762023934723, 152.6008555152116)
ax.set_ylim(90, 153)
upper_legend(ax, ncols = 3, yloc = 1.12)
ax.set_xlabel('')
ax.set_ylabel('GPP ($Pg \ C$)')

# google.download_file(fig, 'Global_GPP_avg-interview.png')

dfp

# dfo = []
# for p in root_proj.joinpath('2output_global_Pmodel3').glob('*.nc'):
#     dt = p.stem.split('_')[1]
#     dt = pd.to_datetime(dt, format = '%Y')
#     print(dt)
#     nct = xr.open_dataset(p)
#     nct = nct.where(nct['GPP_NT_VUT_REF'] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
#     dft = nct['GPP_NT_VUT_REF'].sum(dim = ['latitude', 'longitude']).drop_vars('spatial_ref').to_dataframe() * coef
#     dfo.append(dft)

# dfo = pd.concat(dfo, axis = 0)
# dfo

def get_multiyear_GPP(foldername, sat, yr = ''):
    nc = []
    for p in foldername.glob(f'{sat}_{yr}*.nc'):
        nct = xr.open_dataset(p)[['GPP']]
        # nct = nct.where(nct['GPP'] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
        nct = nct.where(nct['GPP'] > 1e-9, drop = True)
        nct = nct.resample(time = '1YS').mean()
        nc.append(nct)

    nc = xr.merge(nc)
    return nc

foldername = root_proj.joinpath('2output_global_Pmodel-5km')
nc_modis_5km = get_multiyear_GPP(foldername, 'MODIS', yr = '')
# nc_ls5 = get_multiyear_GPP(foldername, 'Landsat-5', yr = '')
# nc_ls7 = get_multiyear_GPP(foldername, 'Landsat-7', yr = '')
# nc_ls8 = get_multiyear_GPP(foldername, 'Landsat-8', yr = '')
# nc_s2 = get_multiyear_GPP(foldername, 'Sentinel-2', yr = '')

foldername_high = root_proj.joinpath('2output_global_Pmodel-high')
nc_modis_high = get_multiyear_GPP(foldername_high, 'MODIS', yr = '')

# (nc_modis_5km.sel(time = nc_ls7.time).mean(dim = 'time')['GPP'] - nc_ls7.mean(dim = 'time')['GPP']).plot()
nc_modis_d = (nc_modis_5km.mean(dim = 'time') - nc_modis_high.mean(dim = 'time'))

# nc_modis_d.to_netcdf(root_proj.joinpath(f'2valid_pixels/MODIS5km_minus_MODIS500m_GPP.nc'))

# nc_ls7.sum(dim = ['latitude', 'longitude']).to_dataframe() * coef,
# nc_modis_5km.sum(dim = ['latitude', 'longitude']).to_dataframe() * coef,
# nc_modis_high.sum(dim = ['latitude', 'longitude']).to_dataframe() * coef

fig, ax = setup_canvas(1, 1, fontsize = 10, labelsize = 10)

nc_modis_high.mean(dim = 'time')['GPP'].rename('Photosynthesis').plot(ax = ax, vmin = 0, vmax = 10, cbar_kwargs={'orientation':'vertical', 'shrink':0.7, 'aspect':20, 'pad': 0.02})
shp = gpd.GeoSeries(cascaded_union(world.geometry)).rename('World')
shp.plot(ax = ax, fc = "None", ec="black", alpha=0.5, zorder=2)
ax.scatter(meta['LON'], meta['LAT'], s = 50, color = 'None', edgecolor = 'r', label = '$Existing \ towers$')
ax.set_xlim(-180, 180)
ax.set_ylim(-90, 90)

ax.legend()

dfd = pd.concat([
    (nc_modis_5km.mean(dim = 'time')['GPP'] - nc_ls7.mean(dim = 'time')['GPP']).mean(dim = 'longitude').to_dataframe()['GPP'].rename('MODIS (coarse) - Landsat-7 (coarse)'),
    (nc_modis_5km.mean(dim = 'time')['GPP'] - nc_modis_high.mean(dim = 'time')['GPP']).mean(dim = 'longitude').to_dataframe()['GPP'].rename('MODIS (coarse - high)')
], axis = 1)


fig, ax = setup_canvas(1, 1, figsize = (3, 6), fontsize = 12)

ax.plot(dfd['MODIS (coarse - high)'], dfd.index, label = 'MODIS (coarse - high)', color = colors[0])
ax.plot(dfd['MODIS (coarse) - Landsat-7 (coarse)'], dfd.index, label = 'MODIS (coarse) - Landsat-7 (coarse)', color = colors[2])
ax.vlines(0, ymin = ax.get_ylim()[0], ymax = ax.get_ylim()[1], color = 'k', ls = '--')

upper_legend(ax, ncols = 1, yloc = 1.15)
ax.set_xlabel('GPP ($g \ C \ m^{-2} \ d^{-1}$)')
ax.set_ylabel('Latitude')

# google.download_file(fig, 'GPP_latitudinal_diff.png')

# foldername = root_proj.joinpath('1grid_NIRv_yearly')

# nc_modis = []
# for p in foldername.glob('MODIS*.nc'):
#     dt = p.stem.split('_')[1]
#     dt = pd.to_datetime(dt, format = '%Y-%m-%d')
#     nct = xr.open_dataset(p)
#     nct = nct.where(nct['NIRv'] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
#     nct = nct.expand_dims(time = [dt])
#     nc_modis.append(nct)

# nc_modis = xr.merge(nc_modis)

# nc_ls7 = []
# for p in foldername.glob('Landsat-7*.nc'):
#     dt = p.stem.split('_')[1]
#     dt = pd.to_datetime(dt, format = '%Y-%m-%d')
#     nct = xr.open_dataset(p)
#     nct = nct.where(nct['NIRv'] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
#     nct = nct.expand_dims(time = [dt])
#     nc_ls7.append(nct)

# nc_ls7 = xr.merge(nc_ls7)

# nc_modis.mean(dim = 'time')['NIRv'].plot(vmin = 0, vmax = 0.4)
# nc_ls7.mean(dim = 'time')['NIRv'].plot(vmin = 0, vmax = 0.4)
# (nc_modis.mean(dim = 'time')['NIRv'] - nc_ls7.mean(dim = 'time')['NIRv']).plot(vmin = -0.5, vmax = 0.5, cmap = 'coolwarm')

"""# Workflow figure

Load data in Modelling section first
"""

!pip install cartopy --quiet

# Workflow figure

from copy import deepcopy

# sat_name = 'MODIS'; df_path_sat = df_path_MODIS
# sat_name = 'Landsat-5'; df_path_sat = df_path_LS5
# sat_name = 'Landsat-7'; df_path_sat = df_path_LS7
# sat_name = 'Landsat-8'; df_path_sat = df_path_LS8
sat_name = 'Sentinel-2'; df_path_sat = df_path_S2

year = 2020
nc_sat = []
for p_sat in df_path_sat[df_path_sat.index.year == year]['PATH']:
    rnc = rxr.open_rasterio(p_sat, band_as_variable = True)
    if rnc.rio.crs != 'EPSG:4326': rnc = rnc.rio.reproject("EPSG:4326")
    name_dict = dict(zip(rnc.keys(), ['R', 'NIR']))
    name_dict.update({'x': 'longitude', 'y': 'latitude'})
    rnc = rnc.rename(name_dict)
    R = rnc['R']
    NIR = rnc['NIR']
    if np.nanmean(NIR.data) > 100:
        NIR = NIR / 10000
        R = R / 10000
    NIRv = (NIR - R) / (NIR + R) * NIR
    NIRv.name = 'NIRv'
    EVI = 2.5 * (NIR - R) / (NIR + 2.4 * R + 1)
    FAPAR = (EVI - 0.1 + 0.1) * 1.25
    EVI.name = 'EVI'
    FAPAR.name = 'FAPAR'
    NDVI = (NIR - R) / (NIR + R)
    kNDVI = NDVI**2 - 0.1
    NDVI.name = 'NDVI'
    kNDVI.name = 'kNDVI'

    nc_satt = xr.merge([NIRv, EVI, FAPAR, NDVI, kNDVI])
    nc_satt = nc_satt.expand_dims(time = [pd.to_datetime(p_sat.stem, format = '%Y-%m-%d')])
    nc_sat.append(nc_satt)
nc_sat = xr.merge(nc_sat)
nc_sat = nc_sat.mean(dim = 'time')

p_era5 = root_proj.joinpath('1grid_inputs').joinpath(f'ERA5_{year}.nc')
nc_era5 = xr.open_dataset(p_era5)#.sel(time = dt).expand_dims(time=[dt])
nc_era5 = nc_era5.mean(dim = 'time')
nc_geo = deepcopy(nc_geor)
lucc = deepcopy(luccr).mean(dim = 'time').astype(int)
# co2 = co2r.where(co2r['time.year'] == year).mean(dim = 'time')
co2 = co2r.mean(dim = 'time')

# for name in ['NIRv', 'EVI', 'FAPAR', 'NDVI', 'kNDVI']:

fig, ax = setup_canvas(1, 1, figsize = (8, 4))
name = 'kNDVI' # ['NIRv', 'EVI', 'FAPAR', 'NDVI', 'kNDVI']
image = nc_sat[name].\
rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).\
drop_vars('spatial_ref').\
plot(ax = ax, cbar_kwargs={'pad': 0.01}, cmap = 'viridis', vmin = 0, vmax = 1)

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label(name, fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xlim(-180, 180); ax.set_ylim(-90, 90)
ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')

google.download_file(fig, f'{sat_name}_{name}.png')

fig, ax = setup_canvas(1, 1, figsize = (8, 4))
image = nc_era5['VPD'].plot(ax = ax, cbar_kwargs={'pad': 0.01}, cmap = 'coolwarm')

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("VPD", fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')

google.download_file(fig, 'VPD.png')

fig, ax = setup_canvas(1, 1, figsize = (8, 4))
image = nc_era5['total_precipitation_sum'].plot(ax = ax, cbar_kwargs={'pad': 0.01}, cmap = 'Blues', vmax = 0.02)

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("Precipitation", fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')

google.download_file(fig, 'Precipitation.png')

fig, ax = setup_canvas(1, 1, figsize = (8, 4))
image = nc_era5['surface_thermal_radiation_downwards_sum'].plot(ax = ax, cbar_kwargs={'pad': 0.01}, cmap = 'plasma')

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("Thermal radiation", fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')

google.download_file(fig, 'Thermal_radiation.png')

fig, ax = setup_canvas(1, 1, figsize = (8, 4))
image = nc_era5['surface_solar_radiation_downwards_sum'].plot(ax = ax, cbar_kwargs={'pad': 0.01}, cmap = 'plasma')

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("Solar radiation", fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')

google.download_file(fig, 'Solar_radiation.png')

fig, ax = setup_canvas(1, 1, figsize = (8, 4))
image = nc_era5['soil_temperature_level_1'].plot(ax = ax, cbar_kwargs={'pad': 0.01})

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("Soil temperature", fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')

google.download_file(fig, 'Soil_temperature.png')

fig, ax = setup_canvas(1, 1, figsize = (8, 4))
image = nc_era5['temperature_2m'].plot(ax = ax, cbar_kwargs={'pad': 0.01})

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("Temperature", fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')

google.download_file(fig, 'Temperature.png')

import cartopy.crs as ccrs
import cartopy.feature as cfeature

# fig, ax = setup_canvas(1, 1, figsize = (8, 4))
fig = plt.figure(figsize = (8, 4))
ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())
image = nc_geo['C4_area'].plot(
    ax = ax, cmap = 'BuGn',
    cbar_kwargs={
        "shrink": 0.85,  # Adjust the shrink parameter (less than 1 to make it smaller)
        "aspect": 20,  # Adjust the aspect ratio of the colorbar
        'pad': 0.01,
    }
)
ax.coastlines()
ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("C4/C3 ratio", fontsize=10, fontweight='bold')  # Set font size for the label

google.download_file(fig, 'C4_area.png')

import matplotlib.colors as mcolors

# Define Köppen climate classes and colors
koppen_classes = {
    0: 'Af',    # Tropical rainforest
    1: 'Am',    # Tropical monsoon
    2: 'Aw',    # Tropical savanna
    3: 'BWh',   # Desert hot
    4: 'BWk',   # Desert cold
    5: 'BSh',   # Steppe hot
    6: 'BSk',   # Steppe cold
    7: 'Csa',   # Mediterranean hot summer
    8: 'Csb',   # Mediterranean dry warm summer
    9: 'Cwa',   # Monsoon-influenced hot summer
    10: 'Cwb',  # Monsoon-influenced dry warm summer
    11: 'Cfa',  # Humid subtropical hot summer
    12: 'Cfb',  # Oceanic mild summer
    13: 'Dfa',  # Continental hot summer
    14: 'Dfb',  # Continental warm summer
    15: 'Dfc',  # Subarctic
    16: 'ET',   # Tundra
    17: 'EF'    # Ice cap
}

# Color list corresponding to each Köppen climate class
koppen_colors = [
    "#1E90FF", "#00BFFF", "#87CEFA", "#FFD700", "#FFA500",
    "#FFE4B5", "#F5DEB3", "#8B0000", "#A52A2A", "#FF4500",
    "#CD5C5C", "#32CD32", "#00FF00", "#006400", "#2E8B57",
    "#556B2F", "#800080", "#8B008B"
]

# Create a custom colormap for Köppen classes
cmap = mcolors.ListedColormap(koppen_colors[:len(koppen_classes)])

fig, ax = setup_canvas(1, 1, figsize = (8, 4))
image = nc_geo['KOPPEN'].plot(
    ax=ax, cmap=cmap, vmin=0, vmax=len(koppen_classes) - 1, add_colorbar=True,
    cbar_kwargs={
        "ticks": np.arange(len(koppen_classes)),
        "format": FuncFormatter(lambda x, _: koppen_classes.get(int(x), "")),
        # "label": "IGBP Class",
        'pad': 0.01,
    }
)

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("Koppen Class", fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')
google.download_file(fig, 'Koppen.png')

fig, ax = setup_canvas(1, 1, figsize = (8, 4))
image = nc_geo['DEM'].plot(ax = ax, cmap="terrain", add_colorbar=True, cbar_kwargs = {'pad': 0.01})

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("DEM", fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')
google.download_file(fig, 'dem.png')

import cartopy.crs as ccrs
import cartopy.feature as cfeature

# fig, ax = setup_canvas(1, 1, figsize = (8, 4))
fig = plt.figure(figsize = (8, 4))
ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())
image = co2['co2'].plot(
    ax = ax, cmap = 'YlOrRd',
    cbar_kwargs={
        "shrink": 0.85,  # Adjust the shrink parameter (less than 1 to make it smaller)
        "aspect": 20,  # Adjust the aspect ratio of the colorbar
        'pad': 0.01,
    }
)
ax.coastlines()
ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("$CO_2$", fontsize=10, fontweight='bold')  # Set font size for the label

google.download_file(fig, 'co2.png')

import matplotlib.colors as mcolors
from matplotlib.ticker import FuncFormatter

# Example IGBP land cover classification codes and names
# You might have these in your dataset or might want to set them manually
# igbp_classes = {
#     0: 'Water',
#     1: 'Evergreen Needleleaf Forest',
#     2: 'Evergreen Broadleaf Forest',
#     3: 'Deciduous Needleleaf Forest',
#     4: 'Deciduous Broadleaf Forest',
#     5: 'Mixed Forest',
#     6: 'Closed Shrublands',
#     7: 'Open Shrublands',
#     8: 'Woody Savannas',
#     9: 'Savannas',
#     10: 'Grasslands',
#     11: 'Permanent Wetlands',
#     12: 'Croplands',
#     13: 'Urban and Built-Up',
#     14: 'Cropland/Natural Vegetation Mosaic',
#     15: 'Snow and Ice',
#     16: 'Barren or Sparsely Vegetated'
# }

igbp_classes = {
    0: 'WAT',     # Water
    1: 'ENF',     # Evergreen Needleleaf Forest
    2: 'EBF',     # Evergreen Broadleaf Forest
    3: 'DNF',     # Deciduous Needleleaf Forest
    4: 'DBF',     # Deciduous Broadleaf Forest
    5: 'MF',      # Mixed Forest
    6: 'CSH',     # Closed Shrublands
    7: 'OSH',     # Open Shrublands
    8: 'WSA',     # Woody Savannas
    9: 'SAV',     # Savannas
    10: 'GRA',    # Grasslands
    11: 'WET',    # Permanent Wetlands
    12: 'CRO',    # Croplands
    13: 'URB',    # Urban and Built-Up
    14: 'CNV',    # Cropland/Natural Vegetation Mosaic
    15: 'SNO',    # Snow and Ice
    16: 'BAR'     # Barren or Sparsely Vegetated
}

# Define colors for each IGBP class for better visualization
igbp_colors = [
    "#419BDF", "#397D49", "#88B053", "#7A87C6", "#E49635",
    "#DFC35A", "#C4281B", "#A59B8F", "#AF963C", "#D7CDCC",
    "#E3E3C2", "#9EBAD5", "#CC0013", "#6F6F6F", "#DCCA8F",
    "#FFFFFF", "#D1D1D1"
]

# Create a custom colormap for IGBP
cmap = mcolors.ListedColormap(igbp_colors[:len(igbp_classes)])

# ------------------------------------------------------------------------------

fig, ax = setup_canvas(1, 1, figsize = (8, 4))

image = lucc.where((lucc['IGBP'] >= 0) & (~lucc['IGBP'].isin([17])))['IGBP'].plot(
    ax = ax, cmap=cmap, vmin=0, vmax=len(igbp_classes) - 1,
    cbar_kwargs={
        "ticks": np.arange(len(igbp_classes)),
        "format": FuncFormatter(lambda x, _: igbp_classes.get(int(x), "")),
        "label": "IGBP Class",
        'pad': 0.01,
    }
)

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label("IGBP Class", fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')
google.download_file(fig, 'IGBP.png')

# sat_name = 'MODIS'
sat_name = 'Landsat-5'
# sat_name = 'Landsat-7'
# sat_name = 'Landsat-8'
# sat_name = 'Sentinel-2'

year = 2007

nc_gpp = xr.open_dataset(root_proj.joinpath('2output_global_Pmodel-high').joinpath(f'{sat_name}_{year}.nc'))['GPP'].mean(dim = 'time')

fig, ax = setup_canvas(1, 1, figsize = (8, 4))

image = nc_gpp.plot(ax = ax, cbar_kwargs={'pad': 0.01}, cmap = 'viridis', vmin = 0, vmax = 12)

# Customize the color bar font size and other properties
cbar = image.colorbar
cbar.ax.tick_params(labelsize=8)  # Set font size for tick labels
cbar.set_label('GPP', fontsize=10, fontweight='bold')  # Set font size for the label

ax.set_xlim(-180, 180); ax.set_ylim(-90, 90)
ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel(''); ax.set_ylabel('')

google.download_file(fig, f'{sat_name}_GPP.png')

