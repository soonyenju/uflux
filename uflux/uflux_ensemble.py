# -*- coding: utf-8 -*-
"""UFLUX-ensemble.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GqQCkn6vzJZcv3VOpoUxsCJVHv980SI0
"""

# prepare gee client and google drive

# Mount Google Drive
!pip install scitbx --quiet
from scitbx import google
root = google.mount_drive()

# pakacage install
!pip install sciml --quiet
!pip install -U xgboost --quiet
!pip install cartopy --quiet

# load project directory
from sciml import pipelines
from scitbx.easy_import import *
from scitbx.stutils import *
from scitbx.meteo import temperature_to_vpd

root_proj = root.joinpath("workspace/project_data/UFLUX-ensemble")

root_proj0 = root.joinpath("workspace/project_data/MODIS_GPP_RET")
root_proj1 = root.joinpath("workspace/project_data2/upscaling")
root_proj2 = root.joinpath("workspace/project_data2/ICOS")
root_proj3 = root.joinpath("workspace/project_data2/upscaling/LUCC")

meta = pd.read_excel(root.joinpath("fmt_fluxdata/fluxnet_meta_212.xlsx"), index_col = 1)
meta = meta.replace('Wet', 'WET')
meta['LAT'] = meta['LAT'].astype('float')
meta['LON'] = meta['LON'].astype('float')

meta_eu = pd.read_csv(root_proj2.joinpath('ICOS_meta.csv'), index_col = 0)
meta_eu.loc['SJ-Blv', 'Site Name'] = 'Bayelva Spitsbergen'
meta_eu.loc['SJ-Blv', 'Site Responsible'] = 'Julia Boike'
meta_eu.loc['SJ-Blv', 'Site Latitude'] = '78.921631'
meta_eu.loc['SJ-Blv', 'Site Longitude'] = '11.83108521'
meta_eu.loc['SJ-Blv', 'Site Name'] = 'SNO'

meta_eu['Site Latitude'] = meta_eu['Site Latitude'].astype(float)
meta_eu['Site Longitude'] = meta_eu['Site Longitude'].astype(float)

"""# Functions"""

# Model type I and II regression, including RMA (reduced major axis regression)

"""
Credit: UMaine MISC Lab; emmanuel.boss@maine.edu
http://misclab.umeoce.maine.edu/
https://github.com/OceanOptics
------------------------------------------------------------------------------
MIT License

Copyright (c) [year] [fullname]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""

import statsmodels.api as sm
import numpy as np


def regress2(_x, _y, _method_type_1 = "ordinary least square",
             _method_type_2 = "reduced major axis",
             _weight_x = [], _weight_y = [], _need_intercept = True):
    # Regression Type II based on statsmodels
    # Type II regressions are recommended if there is variability on both x and y
    # It's computing the linear regression type I for (x,y) and (y,x)
    # and then average relationship with one of the type II methods
    #
    # INPUT:
    #   _x <np.array>
    #   _y <np.array>
    #   _method_type_1 <str> method to use for regression type I:
    #     ordinary least square or OLS <default>
    #     weighted least square or WLS
    #     robust linear model or RLM
    #   _method_type_2 <str> method to use for regression type II:
    #     major axis
    #     reduced major axis <default> (also known as geometric mean)
    #     arithmetic mean
    #   _need_intercept <bool>
    #     True <default> add a constant to relation (y = a x + b)
    #     False force relation by 0 (y = a x)
    #   _weight_x <np.array> containing the weigth of x
    #   _weigth_y <np.array> containing the weigth of y
    #
    # OUTPUT:
    #   slope
    #   intercept
    #   r
    #   std_slope
    #   std_intercept
    #   predict
    #
    # REQUIRE:
    #   numpy
    #   statsmodels
    #
    # The code is based on the matlab function of MBARI.
    # AUTHOR: Nils Haentjens
    # REFERENCE: https://www.mbari.org/products/research-software/matlab-scripts-linear-regressions/

    # Check input
    if _method_type_2 != "reduced major axis" and _method_type_1 != "ordinary least square":
        raise ValueError("'" + _method_type_2 + "' only supports '" + _method_type_1 + "' method as type 1.")

    # Set x, y depending on intercept requirement
    if _need_intercept:
        x_intercept = sm.add_constant(_x)
        y_intercept = sm.add_constant(_y)

    # Compute Regression Type I (if type II requires it)
    if (_method_type_2 == "reduced major axis" or
        _method_type_2 == "geometric mean"):
        if _method_type_1 == "OLS" or _method_type_1 == "ordinary least square":
            if _need_intercept:
                [intercept_a, slope_a] = sm.OLS(_y, x_intercept).fit().params
                [intercept_b, slope_b] = sm.OLS(_x, y_intercept).fit().params
            else:
                slope_a = sm.OLS(_y, _x).fit().params
                slope_b = sm.OLS(_x, _y).fit().params
        elif _method_type_1 == "WLS" or _method_type_1 == "weighted least square":
            if _need_intercept:
                [intercept_a, slope_a] = sm.WLS(
                    _y, x_intercept, weights=1. / _weight_y).fit().params
                [intercept_b, slope_b] = sm.WLS(
                    _x, y_intercept, weights=1. / _weight_x).fit().params
            else:
                slope_a = sm.WLS(_y, _x, weights=1. / _weight_y).fit().params
                slope_b = sm.WLS(_x, _y, weights=1. / _weight_x).fit().params
        elif _method_type_1 == "RLM" or _method_type_1 == "robust linear model":
            if _need_intercept:
                [intercept_a, slope_a] = sm.RLM(_y, x_intercept).fit().params
                [intercept_b, slope_b] = sm.RLM(_x, y_intercept).fit().params
            else:
                slope_a = sm.RLM(_y, _x).fit().params
                slope_b = sm.RLM(_x, _y).fit().params
        else:
            raise ValueError("Invalid literal for _method_type_1: " + _method_type_1)

    # Compute Regression Type II
    if (_method_type_2 == "reduced major axis" or
        _method_type_2 == "geometric mean"):
        # Transpose coefficients
        if _need_intercept:
            intercept_b = -intercept_b / slope_b
        slope_b = 1 / slope_b
        # Check if correlated in same direction
        if np.sign(slope_a) != np.sign(slope_b):
            raise RuntimeError('Type I regressions of opposite sign.')
        # Compute Reduced Major Axis Slope
        slope = np.sign(slope_a) * np.sqrt(slope_a * slope_b)
        if _need_intercept:
            # Compute Intercept (use mean for least square)
            if _method_type_1 == "OLS" or _method_type_1 == "ordinary least square":
                intercept = np.mean(_y) - slope * np.mean(_x)
            else:
                intercept = np.median(_y) - slope * np.median(_x)
        else:
            intercept = 0
        # Compute r
        r = np.sign(slope_a) * np.sqrt(slope_a / slope_b)
        # Compute predicted values
        predict = slope * _x + intercept
        # Compute standard deviation of the slope and the intercept
        n = len(_x)
        diff = _y - predict
        Sx2 = np.sum(np.multiply(_x, _x))
        den = n * Sx2 - np.sum(_x) ** 2
        s2 = np.sum(np.multiply(diff, diff)) / (n - 2)
        std_slope = np.sqrt(n * s2 / den)
        if _need_intercept:
            std_intercept = np.sqrt(Sx2 * s2 / den)
        else:
            std_intercept = 0
    elif (_method_type_2 == "Pearson's major axis" or
          _method_type_2 == "major axis"):
        if not _need_intercept:
            raise ValueError("Invalid value for _need_intercept: " + str(_need_intercept))
        xm = np.mean(_x)
        ym = np.mean(_y)
        xp = _x - xm
        yp = _y - ym
        sumx2 = np.sum(np.multiply(xp, xp))
        sumy2 = np.sum(np.multiply(yp, yp))
        sumxy = np.sum(np.multiply(xp, yp))
        slope = ((sumy2 - sumx2 + np.sqrt((sumy2 - sumx2)**2 + 4 * sumxy**2)) /
                 (2 * sumxy))
        intercept = ym - slope * xm
        # Compute r
        r = sumxy / np.sqrt(sumx2 * sumy2)
        # Compute standard deviation of the slope and the intercept
        n = len(_x)
        std_slope = (slope / r) * np.sqrt((1 - r ** 2) / n)
        sigx = np.sqrt(sumx2 / (n - 1))
        sigy = np.sqrt(sumy2 / (n - 1))
        std_i1 = (sigy - sigx * slope) ** 2
        std_i2 = (2 * sigx * sigy) + ((xm ** 2 * slope * (1 + r)) / r ** 2)
        std_intercept = np.sqrt((std_i1 + ((1 - r) * slope * std_i2)) / n)
        # Compute predicted values
        predict = slope * _x + intercept
    elif _method_type_2 == "arithmetic mean":
        if not _need_intercept:
            raise ValueError("Invalid value for _need_intercept: " + str(_need_intercept))
        n = len(_x)
        sg = np.floor(n / 2)
        # Sort x and y in order of x
        sorted_index = sorted(range(len(_x)), key=lambda i: _x[i])
        x_w = np.array([_x[i] for i in sorted_index])
        y_w = np.array([_y[i] for i in sorted_index])
        x1 = x_w[1:sg + 1]
        x2 = x_w[sg:n]
        y1 = y_w[1:sg + 1]
        y2 = y_w[sg:n]
        x1m = np.mean(x1)
        x2m = np.mean(x2)
        y1m = np.mean(y1)
        y2m = np.mean(y2)
        xm = (x1m + x2m) / 2
        ym = (y1m + y2m) / 2
        slope = (x2m - x1m) / (y2m - y1m)
        intercept = ym - xm * slope
        # r (to verify)
        r = []
        # Compute predicted values
        predict = slope * _x + intercept
        # Compute standard deviation of the slope and the intercept
        std_slope = []
        std_intercept = []

    # Return all that
    return {"slope": float(slope), "intercept": intercept, "r": r,
            "std_slope": std_slope, "std_intercept": std_intercept,
            "predict": predict}


# if __name__ == '__main__':
#     x = np.linspace(0, 10, 100)
#     # Add random error on y
#     e = np.random.normal(size=len(x))
#     y = x + e
#     results = regress2(x, y, _method_type_2="reduced major axis",
#                        _need_intercept=False)
#     # print(results)

from scipy.optimize import curve_fit

def plot_curve_bespoke(func_name, ax, x, y, precision = 2, with_text = False, lineshape = '-'):
    if func_name not in ['lin', 'exp', 'poly2']: raise Exception('func_name must be `lin`, `exp`, or `poly2`!')
    def func_lin(x, a, b):
        return a * x + b

    def func_poly2(x, a, b, c):
        # return a * np.e**(b * x) + c
        return a * x**2 + b * x + c

    def func_exp(x, a, b, c):
        return a * np.exp(-b * x) + c

    func_dict = {
        'lin': func_lin,
        'poly2': func_poly2,
        'exp': func_exp
    }
    x = x.copy(); y = y.copy()
    idx = (~x.isna()) & (~y.isna())
    x = x[idx]; y = y[idx]

    func = func_dict[func_name]
    popt, pcov = curve_fit(func,  x,  y)
    # +++++++++++++++++++++++++++++++++++++++++++++++
    # Print fitting p-values and r2
    fit_p = get_curve_fit_p_value(func, popt, x, y)
    fit_r2 = get_curve_fit_r2(func, popt, x, y)
    print(fit_p)
    print(fit_r2)
    # +++++++++++++++++++++++++++++++++++++++++++++++
    # np.polyfit(x, y, 3)
    ax.plot(x, func(x, *popt), lineshape, color = 'k', label = 'Fitted')
    if func_name == 'lin':
        a, b = popt
        a = roundit(a, precision); b = roundit(b, precision)
        sign = '+' if b >= 0 else '-'
        text = fr'y={a}$x$ {sign} {b}'
    elif func_name == 'exp':
        a, b, c = popt
        a = roundit(a, precision); b = roundit(b, precision); c = roundit(c, precision)
        sign = '+' if c >= 0 else '-'
        # text = r'$y = {:.2f} e^{:.3f}x + {:.2f}$'.format(a, b, c)
        text = fr'$y = {a} \times e^{{{b}x}} {sign} {np.abs(c)}$'
    elif func_name == 'poly2':
        a, b, c = popt
        a = roundit(a, precision); b = roundit(b, precision); c = roundit(c, precision)
        sign1 = '+' if b >= 0 else '-'
        sign2 = '+' if c >= 0 else '-'
        text = f'y={a}$x^2$ {sign1} {b}x {sign2} {c}'
    else:
        raise Exception('func_name must be `lin`, `exp`, or `poly2`!')

    if with_text: add_text(ax, 0.05, 0.05, text, horizontalalignment = 'left')

"""# Global dataset"""

name_dict = {
    'MODIS-NIRv': 'MODIS-NIRv-ERA5',
    'NO-TIMESTAMP': 'MODIS-NIRv-ERA5-NT',
    'YEARLY-NO-TIMESTAMP': 'MODIS-NIRv-ERA5-WY',
    'MODIS-EVI2': 'MODIS-EVI2-ERA5',
    'MODIS-NDVI': 'MODIS-NDVI-ERA5',
    'AVHRR-EVI2': 'AVHRR-EVI2-ERA5',
    'AVHRR-NDVI': 'AVHRR-NDVI-ERA5',
    'AVHRR-NIRv': 'AVHRR-NIRv-ERA5',
    'CFSV2': 'MODIS-NIRv-CFSV2',
    'GOME-2': 'GOME-2-SIF-ERA5',
    'GOSAT-755': 'GOSAT-755-SIF-ERA5',
    'GOSAT-772': 'GOSAT-772-SIF-ERA5',
    'OCO-2-CSIF_v2': 'OCO-2-CSIF-ERA5'
}

paths = list(root_proj1.joinpath('models').rglob('*.csv'))

df_path = []
for p in paths:
    save_suffix = p.stem.split('validation-')[1]
    flux = save_suffix.split('-')[0]
    ml = save_suffix.split('-')[1]
    label = save_suffix.split(f'{flux}-{ml}-')[-1]
    if label == f'{flux}-{ml}': label = 'MODIS-NIRv'
    label = name_dict[label]
    df_path.append([flux, ml, label, p])
df_path = pd.DataFrame(df_path, columns = ['FLUX', 'ML', 'LABEL', 'PATH'])

dfm = []
for idx in df_path.index:
    p = df_path.loc[idx, 'PATH']
    flux = df_path.loc[idx, 'FLUX']
    label = df_path.loc[idx, 'LABEL']
    dft = pd.read_csv(p, index_col = 0)
    metrics = pipelines.get_metrics(dft, truth = 'truth', pred = 'pred', return_dict = True)
    metrics['FLUX'] = flux
    metrics['LABEL'] = label
    metrics['MEAN'] = dft['truth'].abs().mean()
    metrics['MRAE'] = metrics['MAE'] / metrics['MEAN'] # mean relative absolute error
    dfm.append(metrics)

order_index = [
    'GOME-2-SIF-ERA5', 'GOSAT-755-SIF-ERA5', 'GOSAT-772-SIF-ERA5', 'OCO-2-CSIF-ERA5',
    'AVHRR-EVI2-ERA5', 'AVHRR-NDVI-ERA5', 'AVHRR-NIRv-ERA5',
    'MODIS-NIRv-CFSV2', 'MODIS-NIRv-ERA5-NT',
    'MODIS-EVI2-ERA5', 'MODIS-NDVI-ERA5', 'MODIS-NIRv-ERA5',
]
order_columns = ['GPP', 'Reco', 'NEE', 'H', 'LE']

dfm = pd.concat(dfm, axis = 0)
dfm_r2 = dfm.pivot(index = 'LABEL', columns = 'FLUX', values = 'R2').dropna().loc[order_index, order_columns]
dfm_mrae = dfm.pivot(index = 'LABEL', columns = 'FLUX', values = 'MRAE').dropna().loc[order_index, order_columns]

google.download_file(dfm, 'UFLUX-validation-all-metrics.csv')
google.download_file(dfm_r2, 'UFLUX-validation-R2.csv')
google.download_file(dfm_mrae, 'UFLUX-validation-MRAE.csv')
roundit(dfm)

roundit(dfm[dfm['LABEL'] == 'MODIS-NIRv-ERA5'])

from numpy.lib.function_base import quantile
xs = np.arange(dfm_r2.shape[1])
ys = np.arange(dfm_r2.shape[0])
xs_, ys_ = np.meshgrid(xs, ys)

dfp = []
for x, y in zip(xs_.ravel(), ys_.ravel()):
    dfp.append([x, y, dfm_r2.iloc[y, x], dfm_mrae.iloc[y, x]])
dfp = pd.DataFrame(dfp, columns = ['X', 'Y', 'R2', 'MRAE'])

fig, ax = setup_canvas(1, 1, figsize = (4, 10))
im = ax.scatter(
    dfp['X'], dfp['Y'], c = dfp['R2'],
    s = (dfp['MRAE'] - dfp['MRAE'].min()) / (dfp['MRAE'].max() - dfp['MRAE'].min()) * 1000,
    edgecolor = 'k',
    cmap = 'Spectral_r'
)
ax.set_xticks(xs)
ax.set_xticklabels(dfm_r2.columns)

ax.set_yticks(ys)
ax.set_yticklabels(dfm_r2.index)

ax.set_xlim(xs[0] - 1, xs[-1] + 1)
ax.set_ylim(ys[0] - 1, ys[-1] + 1)

pos = ax.get_position()
cbar_ax = fig.add_axes([pos.x1, pos.y0, 0.05, pos.y1 - pos.y0])
cbar = fig.colorbar(im, cax=cbar_ax, orientation = 'vertical')

for x, y in zip(xs_.ravel(), ys_.ravel()):
    r2 = roundit(dfm_r2.iloc[y, x], 1)
    mrae = roundit(dfm_mrae.iloc[y, x], 1)
    text = f'{r2}|{mrae}'
    ax.text(x - 0.47, y + 0.45, text, fontsize = 10)

for qt_ in [0.1, 0.25, 0.5, 0.75, 0.99]:
    ax.scatter([], [],
        color = 'k',
        edgecolor = 'k',
        alpha = 0.6,
        s = (dfp['MRAE'].quantile(qt_) - dfp['MRAE'].min()) / (dfp['MRAE'].max() - dfp['MRAE'].min()) * 1000,
        label = f"{roundit(dfp['MRAE'].quantile(qt_), 1)}"
    )
upper_legend(ax, 0.5, 1.08)

print('')

google.download_file(fig, 'UFLUX-validation.pdf', dpi = 600)

"""# Global maps"""

# name_dict = {
#     'MODIS-NIRv': 'MODIS-NIRv-ERA5',
#     'NO-TIMESTAMP': 'MODIS-NIRv-ERA5-NT',
#     'YEARLY-NO-TIMESTAMP': 'MODIS-NIRv-ERA5-WY',
#     'MODIS-EVI2': 'MODIS-EVI2-ERA5',
#     'MODIS-NDVI': 'MODIS-NDVI-ERA5',
#     'AVHRR-EVI2': 'AVHRR-EVI2-ERA5',
#     'AVHRR-NDVI': 'AVHRR-NDVI-ERA5',
#     'AVHRR-NIRv': 'AVHRR-NIRv-ERA5',
#     'CFSV2': 'MODIS-NIRv-CFSV2',
#     'GOME-2': 'GOME-2-SIF-ERA5',
#     'GOSAT-755': 'GOSAT-755-SIF-ERA5',
#     'GOSAT-772': 'GOSAT-772-SIF-ERA5',
#     'OCO-2-CSIF_v2': 'OCO-2-CSIF-ERA5'
# }

# paths = list(root_proj1.joinpath('products').rglob('*.nc'))

# df_path = []
# for p in paths:
#     save_suffix = p.stem.split('validation-')[1]
#     flux = save_suffix.split('-')[0]
#     ml = save_suffix.split('-')[1]
#     label = save_suffix.split(f'{flux}-{ml}-')[-1]
#     if label == f'{flux}-{ml}': label = 'MODIS-NIRv'
#     label = name_dict[label]
#     df_path.append([flux, ml, label, p])
# df_path = pd.DataFrame(df_path, columns = ['FLUX', 'ML', 'LABEL', 'PATH'])

prod_name_dict = {
    'DF21': 'MODIS-NIRv-ERA5',
    'DF21-MODIS-EVI2': 'MODIS-EVI2-ERA5',
    'DF21-MODIS-NDVI': 'MODIS-NDVI-ERA5',
    'DF21-AVHRR-EVI2': 'AVHRR-EVI2-ERA5',
    'DF21-AVHRR-NDVI': 'AVHRR-NDVI-ERA5',
    'DF21-AVHRR-NIRv': 'AVHRR-NIRv-ERA5',
    'DF21-CFSV2': 'MODIS-NIRv-CFSV2',
    'DF21-GOME-2': 'GOME-2-SIF-ERA5',
    'DF21-GOSAT-755': 'GOSAT-755-SIF-ERA5',
    'DF21-GOSAT-772': 'GOSAT-772-SIF-ERA5',
    'DF21-OCO-2-CSIF_v2': 'OCO-2-CSIF-ERA5',
    'DF21-NO-TIMESTAMP': 'MODIS-NIRv-ERA5-NT',
    'DF21-YEARLY-NO-TIMESTAMP': 'MODIS-NIRv-ERA5-WY',
}

prod_path = {}
folders = list(root_proj1.joinpath('products').glob('*'))
for fld in folders:
    if fld.stem == 'DF21-YEARLY-NO-TIMESTAMP': continue
    dfpt = []
    for p in list(fld.glob('*.nc')):
        _, flux, year = p.stem.split('-')
        dfpt.append([flux, int(year), p])
    dfpt = pd.DataFrame(dfpt, columns = ['FLUX', 'YEAR', 'PATH']).pivot(index = 'YEAR', columns = 'FLUX')
    if dfpt.columns.nlevels != 1: dfpt.columns = dfpt.columns.droplevel(0)
    dfpt = dfpt[['GPP', 'Reco', 'NEE', 'H', 'LE']]
    prod_path[prod_name_dict[fld.stem]] = dfpt

# nca = []
# for name in prod_path.keys():
#     if name == 'MODIS-NIRv-ERA5-WY': continue
#     df_path = prod_path[name]
#     nc_prod = []
#     for flux in df_path.columns:
#         if flux != 'GPP': continue
#         print(name, flux)
#         nc_flux = []
#         for p in df_path[flux]:
#             nct = xr.load_dataset(p)
#             nct = nct.resample(time = '1YS').mean() #[flux].rename('ooo').to_dataset()
#             nc_flux.append(nct)
#             nct.close(); del(nct)
#         nc_flux = xr.merge(nc_flux)
#         nc_prod.append(nc_flux)
#         nc_flux.close(); del(nc_flux)
#     nc_prod = xr.merge(nc_prod).mean(dim = 'time').expand_dims(prod = [name]) #.to_array()
#     nca.append(nc_prod)
# nca = xr.merge(nca)

# nca['GPP'].plot(
#     x="longitude",
#     y="latitude",
#     # col="prod",
#     row = 'prod',
#     col_wrap=3,
# )

savefile = root_proj.joinpath('products-yearly-five-fluxes.nc')

if savefile.exists():
    nca = xr.load_dataset(savefile)
else:
    nca = []
    for name in prod_path.keys():
        df_path = prod_path[name]
        nc_prod = []
        for flux in df_path.columns:
            print(name, flux)
            nc_flux = []
            for p in df_path[flux]:
                nct = xr.load_dataset(p)
                nct = nct.resample(time = '1YS').mean()
                nc_flux.append(nct)
                nct.close(); del(nct)
            nc_flux = xr.merge(nc_flux)
            nc_prod.append(nc_flux)
            nc_flux.close(); del(nc_flux)
        nc_prod = xr.merge(nc_prod).mean(dim = 'time').expand_dims(prod = [name])
        nca.append(nc_prod)
    nca = xr.merge(nca)

    nca.to_netcdf(savefile)

p = root_proj0.joinpath(f"products_01_21/upscaled_GPP_2011-2020_025deg.nc")
sample = xr.load_dataset(p).mean(dim = 'time').rename(lat = 'latitude', lon = 'longitude')
sample = sample.assign_coords(latitude = sample.latitude[::-1])
sample = sample.interp(latitude = nca.latitude, longitude = nca.longitude)
sample['data'].plot(vmin = 0, vmax = 10, cmap = 'viridis_r')

# nca['NEE_ind'] = nca['Reco'] - nca['GPP']
nca['GPP'] = nca['GPP'].where(~sample['data'].isnull())
nca['Reco'] = nca['Reco'].where(~sample['data'].isnull())
nca['NEE'] = nca['NEE'].where(~sample['data'].isnull())

"""## All products average"""

prod_names = [
    'GOME-2-SIF-ERA5', 'GOSAT-755-SIF-ERA5', 'GOSAT-772-SIF-ERA5', 'OCO-2-CSIF-ERA5',
    'AVHRR-EVI2-ERA5', 'AVHRR-NDVI-ERA5', 'AVHRR-NIRv-ERA5',
    'MODIS-NIRv-CFSV2', 'MODIS-NIRv-ERA5-NT',
    'MODIS-EVI2-ERA5', 'MODIS-NDVI-ERA5', 'MODIS-NIRv-ERA5',
]
prod_names = prod_names[::-1]

flux_names = ['GPP', 'Reco', 'NEE', 'H', 'LE']

flux_range = {
    'GPP': [0, 20],
    'Reco': [0, 20],
    'NEE': [-5, 5],
    'H': [0, 100],
    'LE': [0, 100]

}

cmap_dict = {
    'GPP': 'viridis_r',
    'Reco': 'plasma_r',
    'NEE': 'RdYlGn_r',
    'H': 'coolwarm',
    'LE': 'coolwarm',
}

fig, axes = setup_canvas(13, 5, figsize = (30, 60), wspace = 0.2, hspace = 0.2)
for j, name in enumerate(nca['prod'].values):
    for i, flux in enumerate(flux_names):
        ax = axes[j * len(flux_names) + i]
        vmin, vmax = flux_range[flux]
        nca.sel(prod = name)[flux].plot(ax = ax, vmin = vmin, vmax = vmax, cmap = cmap_dict[flux])
        ax.set_title(name, fontsize = 12)

google.download_file(fig, 'UFLUX-maps.jpg', dpi = 600)

google.download_file(fig, 'UFLUX-maps.pdf')

"""## Flat and spheric maps"""

# for cnt, flux in enumerate(['GPP', 'Reco', 'NEE', 'H', 'LE']):

flux_range = {
    'GPP': [0, 10],
    'Reco': [0, 10],
    'NEE': [-5, 5],
    'H': [0, 100],
    'LE': [0, 100]

}

cmap_dict = {
    'GPP': 'viridis_r',
    'Reco': 'plasma_r',
    'NEE': 'RdYlGn_r',
    'H': 'coolwarm',
    'LE': 'coolwarm',
}

# ncp = nca.sel(prod = 'MODIS-NIRv-ERA5')[flux].drop_vars('prod')

flux = 'LE'
ncp = nca.mean(dim = 'prod')[flux]

fig, ax = setup_canvas(1, 1)
vmin, vmax = flux_range[flux]
ncp.plot(ax = ax, vmin = vmin, vmax = vmax, cmap = cmap_dict[flux])
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')

# google.download_file(fig, f'UFLUX-global-{flux}-maps.jpg', dpi = 600)
# google.download_file(fig, f'UFLUX-global-{flux}-maps.pdf')

import cartopy.crs as ccrs
import matplotlib.pyplot as plt

# https://xarray.dev/blog/introducing-pint-xarray

# p = squared_wind.isel(time="2014-01").plot(
#     subplot_kws=dict(projection=ccrs.Orthographic(-80, 35), facecolor="gray"),
#     transform=ccrs.PlateCarree(),
# )

focal_dict = {
    'GPP': [0, 35],
    'Reco': [100, 10],
    'NEE': [-100, 80],
    'H': [20, 20],
    'LE': [-20, -20]

}

flux = 'H'
ncp = nca.mean(dim = 'prod')[flux]
vmin, vmax = flux_range[flux]
p = ncp.plot(
    vmin = vmin, vmax = vmax, cmap = cmap_dict[flux],
    subplot_kws=dict(projection=ccrs.Orthographic(*focal_dict[flux]), facecolor = None),
    transform=ccrs.PlateCarree(),
    cbar_kwargs={'orientation':'horizontal', 'shrink':0.6, 'aspect':20, 'pad': 0.02},
)

p.axes.set_global()
p.axes.coastlines()
fig = p.get_figure()
plt.show()

# google.download_file(fig, f'UFLUX-global-{flux}-sphere.jpg', dpi = 600)
# google.download_file(fig, f'UFLUX-global-{flux}-sphere.pdf')
google.download_file(fig, f'UFLUX-global-{flux}-sphere.png', dpi = 600, transparent = True)

"""# Global IAV"""

def deg2m(longitude, latitude, scale_lon, scale_lat):
    # deg x deg => m2
    # Length in km of 1° of latitude = always 111.32 km
    # Length in km of 1° of longitude = 40075 km * cos( latitude ) / 360
    _, lats = np.meshgrid(longitude, latitude)
    coef_mat = 40075 * np.cos(np.deg2rad(np.abs(lats))) / 360 * 111.32 * 1e3 * 1e3
    coef_mat = coef_mat * scale_lon * scale_lat
    return coef_mat

def gC2Pg_v2(nc, varname, xscale, yscale):
    # gC m-2 d-1 => Pg C yr-1
    coef_mat = deg2m(nc.longitude.values, nc.latitude.values, xscale, yscale)
    if 'time' in list(nc.coords):
        arr2D = nc[varname].mean(dim = 'time').values
    else:
        arr2D = nc[varname].values
    # if assure_positive: arr2D[arr2D < 0] = np.nan
    return np.nansum(arr2D * coef_mat * 365 * 1e-15)

# --------------------------------------------------------------------------------------

def deg2m(longitude, latitude, scale_lon, scale_lat):
    # deg x deg => m2
    # Length in km of 1° of latitude = always 111.32 km
    # Length in km of 1° of longitude = 40075 km * cos( latitude ) / 360
    _, lats = np.meshgrid(longitude, latitude)
    coef_mat = 40075 * np.cos(np.deg2rad(np.abs(lats))) / 360 * 111.32 * 1e3 * 1e3
    coef_mat = coef_mat * scale_lon * scale_lat
    return coef_mat

def gC2Pg(nc, varname, scale):
    # gC m-2 d-1 => Pg C yr-1
    coef_mat = deg2m(nc.longitude.values, nc.latitude.values, scale, scale)
    if 'time' in list(nc.coords):
        arr2D = nc[varname].mean(dim = 'time').values
    else:
        arr2D = nc[varname].values
    # if assure_positive: arr2D[arr2D < 0] = np.nan
    return np.nansum(arr2D * coef_mat * 365 * 1e-15)

def W2MJ(nc, varname, annual = True):
    # W m-2 aka J m-2 s-1 => MJ m-2 d-1
    # arr2D = nc[varname].mean(dim = 'time').values
    if annual:
        return float(nc[varname].mean() * 1e-6 * 24 * 3600)
    else:
        return nc[flux].mean(dim = ['latitude', 'longitude']).to_dataframe() * 1e-6 * 24 * 3600

flux = 'GPP'

unit_dict = {
    'NEE': 'NEE $(Pg \ C \ yr^{-1})$',
    'GPP': 'GPP $(Pg \ C \ yr^{-1})$',
    'Reco': 'Reco $(Pg \ C \ yr^{-1})$',
    'H': 'H $(MJ \ m^{-2} \ d^{-1})$',
    'LE': 'LE $(MJ \ m^{-2} \ d^{-1})$'
}

savefile = root_proj.joinpath(f'UFLUX-{flux}.csv')

if savefile.exists():
    dfo = pd.read_csv(savefile, index_col = 0)
    dfo = dfo.pivot(index = 'YEAR', columns = 'PROD')
    dfo.columns = dfo.columns.droplevel(0)
    dfo.index = pd.to_datetime(dfo.index, format = '%Y')
else:
    df_path = pd.DataFrame(
        [[p.parent.stem, int(p.stem.split('-')[-1]), p] for p in list(root_proj1.joinpath('products').rglob(f'*{flux}*.nc'))], columns = ['SUFFIX', 'YEAR', 'PATH']
    )
    df_path = pd.pivot(df_path, index = 'YEAR', columns = 'SUFFIX', values = 'PATH')#.drop(['DF21-NO-TIMESTAMP', 'DF21-YEARLY-NO-TIMESTAMP'], axis = 1)

    dfo = []
    for year in df_path.index:
        print(year)
        dft = df_path.loc[year, :].dropna()
        for suffix in dft.index:
            # print(suffix)
            prod_name = prod_name_dict[suffix]
            p = dft[suffix]
            suffix = suffix.split('-')
            if 'GOSAT' in suffix:
                scale = 1
            else:
                scale = 0.25

            nc = xr.load_dataset(p)
            if flux in ['NEE', 'GPP', 'Reco']:
                val = gC2Pg(nc, flux, scale)
                res = [year, prod_name, val]
            elif flux in ['H', 'LE']:
                # val = W2MJ(nc, flux, annual = False)
                # res = val.rename(columns = {flux: prod_name})
                val = W2MJ(nc, flux, annual = True)
                res = [year, prod_name, val]
            else:
                raise Exception('Wrong flux')
            dfo.append(res)

    dfo = pd.DataFrame(dfo, columns = ['YEAR', 'PROD', 'VAL'])
    # if flux in ['NEE', 'GPP', 'Reco']:
    #     dfo = pd.DataFrame(dfo, columns = ['YEAR', 'PROD', 'VAL'])
    # else:
    #     dfo = pd.concat(dfo, axis = 1)
    dfo.to_csv(savefile)

fig, ax = setup_canvas(1, 1)
dfo.plot(ax = ax, style = '-o', markersize = 10)
upper_legend(ax, 0.5, 1.3, ncols = 4)
ax.set_xlabel('')
ax.set_ylabel('Pg C')

df_trendy_oml = pd.read_csv(root_proj1.joinpath('Trendy/GPP-products-v5.csv'), index_col = 0)
df_trendy_oml.index = pd.to_datetime(df_trendy_oml.index, format = '%Y-%m-%d')

df_oml = df_trendy_oml[['FLUXSAT', 'FLUXCOM', 'ZENG']]
df_trendy = df_trendy_oml[['ORCHIDEEv3', 'CLM5.0', 'JSBACH', 'IBIS', 'LPJ-GUESS', 'CABLE-POP']]
dft = df_trendy_oml[['UFLUX(WM)', 'UFLUX(NY)', 'UFLUX(WY)']]

df_oml_mn = df_oml.mean(axis = 1)
df_oml_std = df_oml.std(axis = 1)

df_trendy_mn = df_trendy.mean(axis = 1)
df_trendy_std = df_trendy.std(axis = 1)

# fig, ax = setup_canvas(1, 1)
# df_oml.plot(ax = ax)
# # df_trendy.plot(ax = ax)
# dft.plot(ax = ax)

df_train = load_pickle(root_proj1.joinpath('dataset/train_space.pkl'))
# dft_mean = df_train['GPP_NT_VUT_REF'].resample('1YS').mean(); dft_mean.index = dft_mean.index.year
# dft_std = df_train['GPP_NT_VUT_REF'].resample('1YS').std(); dft_std.index = dft_std.index.year
# # ----------------------------------------------------------------------------
site_names, site_list = [], []
for g, gp in df_train.groupby('ID'):
    # if (gp.index[0].year < 2001) & (gp.index[-1].year > 2013):
    if (gp.index[-1] - gp.index[0]).days / 365 > 3:
        site_names.append(g)
        site_list.append(gp['GPP_NT_VUT_REF'])
df_ect = pd.concat(site_list, axis = 0)
dfec_mean = df_ect.resample('1YS').mean(); dfec_mean.index = dfec_mean.index.year
dfec_std = df_ect.resample('1YS').std(); dfec_std.index = dfec_std.index.year

dfec_mean.index = pd.to_datetime(dfec_mean.index, format = '%Y')
dfec_std.index = pd.to_datetime(dfec_std.index, format = '%Y')

df_nt_mn = dfo[['MODIS-NIRv-ERA5-NT', 'MODIS-NIRv-ERA5-WY']].mean(axis = 1)
df_nt_std = dfo[['MODIS-NIRv-ERA5-NT', 'MODIS-NIRv-ERA5-WY']].std(axis = 1)

df_wt_mn = dfo[[
    'AVHRR-EVI2-ERA5', 'AVHRR-NDVI-ERA5', 'AVHRR-NIRv-ERA5',
    'GOME-2-SIF-ERA5', 'GOSAT-755-SIF-ERA5', 'GOSAT-772-SIF-ERA5',
    'MODIS-EVI2-ERA5', 'MODIS-NDVI-ERA5', 'MODIS-NIRv-CFSV2',
    'MODIS-NIRv-ERA5', 'OCO-2-CSIF-ERA5'
]].mean(axis = 1)
df_wt_std = dfo[[
    'AVHRR-EVI2-ERA5', 'AVHRR-NDVI-ERA5', 'AVHRR-NIRv-ERA5',
    'GOME-2-SIF-ERA5', 'GOSAT-755-SIF-ERA5', 'GOSAT-772-SIF-ERA5',
    'MODIS-EVI2-ERA5', 'MODIS-NDVI-ERA5', 'MODIS-NIRv-CFSV2',
    'MODIS-NIRv-ERA5', 'OCO-2-CSIF-ERA5'
]].std(axis = 1)

# ------------------------------------------------------------------------------
fig, ax = setup_canvas(1, 1)
df_nt_mn.plot(ax = ax, style = '-o', markersize = 10, label = 'UFLUX(NT)')
ax.fill_between(df_nt_mn.index, df_nt_mn - df_nt_std, df_nt_mn + df_nt_std, color = colors[0], alpha = 0.3, zorder = 1)

df_wt_mn.plot(ax = ax, style = '-o', markersize = 10, label = 'UFLUX(WT)')
ax.fill_between(df_wt_mn.index, df_wt_mn - df_wt_std, df_wt_mn + df_wt_std, color = colors[1], alpha = 0.3, zorder = 1)

df_oml_mn.plot(ax = ax, style = '-o', markersize = 10, label = 'Upscale')
ax.fill_between(df_oml_mn.index, df_oml_mn - df_oml_std, df_oml_mn + df_oml_std, color = colors[2], alpha = 0.3, zorder = 1)

df_trendy_mn.plot(ax = ax, style = '-o', markersize = 10, label = 'Trendy')
ax.fill_between(df_trendy_mn.index, df_trendy_mn - df_trendy_std, df_trendy_mn + df_trendy_std, color = colors[3], alpha = 0.1, zorder = 2)

ax2 = ax.twinx()
dfec_mean.plot(ax = ax2, style = '--^', markersize = 10, label = 'EC', color = 'k')
# ax2.fill_between(dfec_mean.index, dfec_mean - dfec_std, dfec_mean + dfec_std, color = 'k', alpha = 0.1, zorder = 1)
ax2.set_ylabel('gC $m^{-2}$ $d^{-1}$')

ax.set_xlim(df_nt_mn.index[0] - pd.DateOffset(years = 2), df_nt_mn.index[-1] + pd.DateOffset(years = 1))

upper_legend(ax, 0.5, 1.2, ncols = 4)
upper_legend(ax2, 0.5, 1.1, ncols = 4)
ax.set_xlabel('')
ax.set_ylabel('Pg C')

google.download_file(fig, f'UFLUX-IAV-lineplot.jpg', dpi = 600)
google.download_file(fig, f'UFLUX-IAV-lineplot.pdf')



# def plot_pie(ax, series, nsites):
#     from matplotlib import cm
#     def my_autopct(pct):
#         return ('%1.1f%%' % pct) if pct >= 10 else ''
#     # Pie chart, where the slices will be ordered and plotted counter-clockwise:
#     num = len(series)
#     cs=cm.Spectral(np.arange(num)/num)[2::]
#     labels = series.index
#     sizes = series.values
#     sizes = sizes / np.sum(sizes)

#     explode = deepcopy(sizes)
#     explode[explode < 0.1] = 0
#     explode = (explode - explode.min()) / (explode.max() - explode.min()) * 0.2

#     wedges, labels, pct_texts = ax.pie(
#         sizes, explode=explode, labels=labels,
#         autopct = my_autopct,
#         # autopct='%1.1f%%',
#         shadow=True, startangle=90, colors = cs)#, textprops={'fontsize': 12}, rotatelabels = 30)
#     ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

#     for ea, eb in zip(wedges, labels):
#         # mang =(ea.theta1 + ea.theta2)/2.  # get mean_angle of the wedge
#         # print(mang, eb.get_rotation())
#         eb.set_rotation(30)

#     centre_circle = plt.Circle((0,0), 0.3, color='black', fc='white',linewidth=0)
#     ax.add_patch(centre_circle)
#     add_text(ax, 0.5, 0.5, f'No.: {nsites}')
#     ax.xaxis.set_tick_params(labelsize = 3)
#     ax.yaxis.set_tick_params(labelsize = 3)
#     # ax.set_facecolor('white')

# df_train = load_pickle(root_proj1.joinpath('dataset/train_space.pkl'))
# # dft_mean = df_train['GPP_NT_VUT_REF'].resample('1YS').mean(); dft_mean.index = dft_mean.index.year
# # dft_std = df_train['GPP_NT_VUT_REF'].resample('1YS').std(); dft_std.index = dft_std.index.year
# # # ----------------------------------------------------------------------------
# site_names, site_list = [], []
# for g, gp in df_train.groupby('ID'):
#     # if (gp.index[0].year < 2001) & (gp.index[-1].year > 2013):
#     if (gp.index[-1] - gp.index[0]).days / 365 > 3:
#         site_names.append(g)
#         site_list.append(gp['GPP_NT_VUT_REF'])
# dft = pd.concat(site_list, axis = 0)
# dft_mean = dft.resample('1YS').mean(); dft_mean.index = dft_mean.index.year
# print(len(site_names))
# print(meta.loc[site_names, 'IGBP'].value_counts())
# # # ----------------------------------------------------------------------------
# # site_list = []
# # for g, gp in df_train.groupby('ID'):
# #     gp = gp['GPP_NT_VUT_REF'].resample('1YS').mean()
# #     site_list.append(gp)
# # dft = pd.concat(site_list, axis = 0)#['2001'::]

# # dft_mean = dft.resample('1YS').mean(); dft_mean.index = dft_mean.index.year
# # dft_std = dft.resample('1YS').std() / 10; dft_std.index = dft_std.index.year

# # ==============================================================================
# fig, ax = setup_canvas(1, 1, figsize = (12, 6))
# ax.plot(dft_mean, '-o', color = colors[6], lw = 2, ms = 10, markeredgecolor = 'k', label = 'EC')
# # ax.fill_between(dft_std.index, dft_mean - dft_std, dft_mean + dft_std, color = colors[6], alpha = 0.5)
# # ax.errorbar(dft_mean.index, dft_mean, yerr = dft_std, linestyle = '-', marker = 'o', color = colors[6], lw = 2, ms = 10, markeredgecolor = 'k', capsize = 5, label = 'EC')

# # ax.set_xlim(x_range)
# ax.set_xticks(np.arange(2001, 2021, 3))
# ax.set_xticklabels(np.arange(2001, 2021, 3))
# ax.set_ylabel('EC GPP $(gC \ m^{-2} \ d^{-1})$')

# x0 = ax.get_position().x0; x1 = ax.get_position().x1
# y0 = ax.get_position().y0; y1 = ax.get_position().y1
# width = x1 - x0; height = y1 - y0
# ax_pie = fig.add_axes([x0 + width * 0.6, y0 + height * 0.2, width * 0.7, height])

# plot_pie(ax_pie, meta.loc[site_names, 'IGBP'].value_counts(), len(site_names))

# # google.download_file(fig, 'EC GPP trends.png')

# ax.fill_between(df_nees_year.index, df_nees_year['truth_NEE'] - df_nees_std['truth_NEE'], df_nees_year['truth_NEE'] + df_nees_std['truth_NEE'], color = colors[0], alpha = 0.3)

"""# Latitudinal (MODIS_NIRv.ipynb)"""

savefile = root_proj.joinpath('UFLUX-latitude.nc')
if savefile.exists():
    nca = xr.load_dataset(savefile)
else:
    nca = []
    for name in prod_path.keys():
        # if name == 'MODIS-NIRv-ERA5-WY': continue
        df_path = prod_path[name]
        nc_prod = []
        for flux in df_path.columns:
            # if flux != 'GPP': continue
            print(name, flux)
            nc_flux = []
            for p in df_path[flux]:
                # if p.stem.split('-')[-1] != '2018': continue
                nct = xr.load_dataset(p)
                nc_flux.append(nct.mean(dim = 'longitude').resample(time = '1YS').mean())
                nct.close(); del(nct)
            nc_flux = xr.merge(nc_flux, compat='override')
            nc_prod.append(nc_flux)
            nc_flux.close(); del(nc_flux)
        nc_prod = xr.merge(nc_prod).mean(dim = 'time').expand_dims(prod = [name]) #.to_array()
        nca.append(nc_prod)
    nca = xr.merge(nca)
    nca.to_netcdf(savefile)

# nca['GPP'].plot(
#     x="longitude",
#     y="latitude",
#     # col="prod",
#     row = 'prod',
#     col_wrap=3,
# )

# fig, axes = setup_canvas(5, 1, figsize = (6, 9))
import matplotlib.gridspec as gridspec
from scitbx.sciplt import *

fig = plt.figure(figsize = (12, 8))
gs = gridspec.GridSpec(8, 12, figure = fig)
gs.update(wspace = 36, hspace = 2)
ax1 = plt.subplot(gs[0:4, :4])
ax2 = plt.subplot(gs[0:4, 4:8])
ax3 = plt.subplot(gs[0:4, 8:])
ax4 = plt.subplot(gs[4:, 2:6])
ax5 = plt.subplot(gs[4:, 6:10])

axes = [ax1, ax2, ax3, ax4, ax5]

flux_units = {
    'NEE': 'NEE $(gC \ m^{-2} \ d^{-1})$',
    'GPP': 'GPP $(gC \ m^{-2} \ d^{-1})$',
    'Reco': 'Reco $(gC \ m^{-2} \ d^{-1})$',
    'H': 'H $(MJ \ m^{-2} \ d^{-1})$',
    'LE': 'LE $(MJ \ m^{-2} \ d^{-1})$'
}
for cnt, flux in enumerate(['GPP', 'Reco', 'NEE', 'H', 'LE']):
    ax = axes[cnt]
    dfp = nca[flux].to_dataframe().reset_index().pivot(index = 'latitude', columns = 'prod')
    dfp.columns = dfp.columns.droplevel(0)
    # dfp.plot()
    for c in dfp.columns:
        if c in ['MODIS-NIRv-ERA5-NT', 'OCO-2-CSIF-ERA5']:
            ax.plot(dfp[c], dfp.index, label = c, ls = '--')
        else:
            ax.plot(dfp[c], dfp.index, label = c)
    # ax.text(0.1, 0.85, f'({chr(97 + cnt)}) {flux}', transform = ax.transAxes)
    ax.text(0.1, 0.85, f'({chr(97 + cnt)})', transform = ax.transAxes)
    ax.set_ylabel('Latitude')
    ax.set_xlabel(flux_units[flux])

upper_legend(axes[1], 0.5, 1.3, ncols = 4)


# fig, ax = setup_canvas(1, 1, figsize = (6, 9))
# for flux in ['GPP', 'Reco', 'NEE', 'H', 'LE']:
#     dft_mn = nca[flux].to_dataframe().reset_index().pivot(index = 'latitude', columns = 'prod').mean(axis = 1)
#     dft_std = nca[flux].to_dataframe().reset_index().pivot(index = 'latitude', columns = 'prod').std(axis = 1)
#     break

# dft_mn.plot(ax = ax)
# ax.fill_between(dft_mn.index, dft_mn - dft_std, dft_mn + dft_std, color = colors[0], alpha = 0.1, zorder = 2)
# # ax.plot(dft_mn, dft_mn.index, label = flux)
# # ax.fill_between(dft_mn - dft_std, dft_mn + dft_std, dft_mn.index, color = colors[0], alpha = 0.1, zorder = 2)

# google.download_file(fig, f'UFLUX-latitude.jpg', dpi = 600)
# google.download_file(fig, f'UFLUX-latitude.pdf')

"""# ICOS (ICOS_dryness_v3-paper.ipynb)

## Scatter
"""

from scipy.stats import gaussian_kde

def kde_scatter(ax, dfp, x_name, y_name):
    dfp = dfp[[x_name, y_name]].dropna().sample(frac = 0.1)
    x = dfp[x_name]
    y = dfp[y_name]

    # Calculate the point density
    xy = np.vstack([x,y])
    z = gaussian_kde(xy)(xy)

    # Sort the points by density, so that the densest points are plotted last
    idx = z.argsort()
    x, y, z = x[idx], y[idx], z[idx]

    # fig, ax = plt.subplots(1, 1, figsize = (9, 9))
    ax.scatter(x, y, c=z, s=50, cmap = 'RdYlBu_r')

    xl = np.arange(np.floor(x.min()), np.ceil(x.max()))
    ax.plot(xl, xl, ls = '-.', color = 'k')

def unify_xylim(ax):
    xylim = np.vstack([ax.get_xlim(), ax.get_ylim()])
    vmin = xylim[:, 0].min()
    vmax = xylim[:, 1].max()
    ax.set_xlim(vmin, vmax)
    ax.set_ylim(vmin, vmax)
    return vmin, vmax

import matplotlib.gridspec as gridspec

fig = plt.figure(figsize = (12, 8))
gs = gridspec.GridSpec(8, 12, figure = fig)
gs.update(wspace = 36, hspace = 2)
ax1 = plt.subplot(gs[0:4, :4])
ax2 = plt.subplot(gs[0:4, 4:8])
ax3 = plt.subplot(gs[0:4, 8:])
ax4 = plt.subplot(gs[4:, 2:6])
ax5 = plt.subplot(gs[4:, 6:10])

axes = [ax1, ax2, ax3, ax4, ax5]

# flux = 'GPP'
for cnt, flux in enumerate(['GPP', 'Reco', 'NEE', 'H', 'LE']):
    m = 'DF21'
    m_name = 'UFLUX'
    p = root_proj2.joinpath(f'models/evaluation-{flux}-{m}.csv') # ICOS
    p = root_proj1.joinpath(f'models/DF21/validation-{flux}-{m}.csv') # FLUXNET
    dft = pd.read_csv(p, index_col = 0)
    dft.index = pd.to_datetime(dft.index, format = '%Y-%m-%d')
    # dft[dft < 0] = np.nan
    # dft = dft.dropna()
    # fig, ax = setup_canvas(1, 1, figsize = (4, 4))
    ax = axes[cnt]
    x = dft['truth']; y = dft['pred']
    slope, intercept, rvalue, pvalue, stderr = stats.linregress(x, y)

    kde_scatter(ax, dft, 'truth', 'pred') # KDE
    # ax.scatter(x, y, s = 50, edgecolor = 'k', alpha = 0.7) # Normal scatter
    vmin, vmax = unify_xylim(ax)
    if flux in ['GPP', 'Reco', 'H', 'LE']: vmin = 0
    ax.plot(np.linspace(vmin, vmax, 100), np.linspace(vmin, vmax, 100), ls = '-.', color = 'k')
    ax.text(0.1, 0.7, f'({chr(97 + cnt)}) {flux} ({m_name})\ny={roundit(slope, 2)}x+{roundit(intercept, 2)}\n$R^2:$ {roundit(rvalue**2)}', transform = ax.transAxes)
    ax.set_xlim(vmin, vmax)
    ax.set_ylim(vmin, vmax)
    ax.set_xlabel('EC')
    ax.set_ylabel('UFLUX')

# google.download_file(fig, f'Europe-{flux}-{m_name}-scatter.png')

# google.download_file(fig, f'UFLUX-EU-ICOS-{flux}-scatter.jpg', dpi = 600)
# google.download_file(fig, f'UFLUX-EU-ICOS-{flux}-scatter.pdf')

google.download_file(fig, f'UFLUX-FLUXNET-{flux}-scatter.jpg', dpi = 600)
google.download_file(fig, f'UFLUX-FLUXNET-{flux}-scatter.pdf')

"""## Maps"""

# hillshade

!pip install earthpy

import earthpy as et
import earthpy.spatial as es
import earthpy.plot as ep

# ------------------------------------------------------------------------------
# # xarray spatial, does not work
# !pip install xarray-spatial

# from xrspatial import generate_terrain
# from xrspatial import hillshade
# import datashader as ds
# from datashader.transfer_functions import shade
# from datashader.transfer_functions import stack
# from datashader.transfer_functions import dynspread
# from datashader.transfer_functions import set_background
# from datashader.colors import Elevation

# terrain = generate_terrain(elev['elevation'])
# illuminated = hillshade(terrain)

# hillshade_gray_white = shade(
#     illuminated, cmap=["gray", "white"], alpha=255, how="linear"
# )
# hillshade_gray_white
# # terrain_elevation = shade(terrain, cmap=Elevation, alpha=128, how="linear")
# # stack(hillshade_gray_white, terrain_elevation)
# ------------------------------------------------------------------------------

from shapely.ops import cascaded_union

warnings.simplefilter('ignore')

world = gpd.read_file(root.joinpath('workspace/project_data/o3_model/world_json/world-continents.json'))
boundary = gpd.GeoSeries(cascaded_union(world.geometry))[0]

europe = gpd.read_file(root_proj3.joinpath('europe_roi.geojson'))

# p = root_proj1.joinpath('dataset/MODIS-IGBP/MODIS-IGBP-2010-01deg.nc')
# lucc_modis = xr.load_dataset(p)
# lucc_modis = lucc_modis.interp(longitude = nct.longitude, latitude = nct.latitude)
# lucc_modis['IGBP'].plot()

# names_lc = ('water', 'trees', 'grass', 'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built', 'bare', 'snow_and_ice', 'label')

# df_path_dw = []
# for p in root.joinpath('shared_files/Dynamic-World-annual').glob('*.tif'):
#     yr, _, roi = p.stem.split('Dynamic-World-')[1].split('-')
#     df_path_dw.append([int(yr), roi, p])
# df_path_dw = pd.DataFrame(df_path_dw, columns = ['YEAR', 'ROI', 'PATH']).pivot(index = 'YEAR', columns = 'ROI')
# df_path_dw.columns = df_path_dw.columns.droplevel(0)
# nc_lc = []
# for p in df_path_dw.loc[2020, :]:
#     print(p)
#     lc = rxr.open_rasterio(p, band_as_variable = True)
#     d_lc = dict(zip(list(lc.keys()), names_lc))
#     d_lc.update({'x': 'longitude', 'y': 'latitude'})
#     lc = lc.rename(d_lc)
#     lc = lc[['water']]
#     lc = lc.coarsen(longitude = 100, latitude = 100, boundary='pad').mean()
#     nc_lc.append(lc)

# nc_lc = xr.merge(nc_lc)
# nc_lc.to_netcdf(root_proj.joinpath('DW-2020-water-coarsen100.nc'))

nc_lcr = xr.load_dataset(root_proj.joinpath('DW-2020-water-coarsen100.nc'))

flux_range = {
    'GPP': [0, 10],
    'Reco': [0, 10],
    'NEE': [-5, 5],
    'H': [0, 100],
    'LE': [0, 100]

}

cmap_dict = {
    'GPP': 'viridis_r',
    'Reco': 'plasma_r',
    'NEE': 'RdYlGn_r',
    'H': 'coolwarm',
    'LE': 'coolwarm',
}

ml = 'DF21'
paths = list(root_proj2.joinpath(f'dataset/products/{ml}').glob(f'*.nc'))
df_path = []
for p in paths:
    _, flux, year = p.stem.split('-')
    df_path.append([flux, year, p])
df_path = pd.DataFrame(df_path, columns = ['FLUX', 'YEAR', 'PATH'])
df_path = df_path.pivot(index = 'YEAR', columns = 'FLUX')
df_path.columns = df_path.columns.droplevel(0)

# ------------------------------------------------------------------------------
nc_sample = xr.load_dataset(df_path.iloc[0, 0]).resample(time = '1YS').mean()

elev = rxr.open_rasterio(root_proj.joinpath('SRTM90.tif'), band_as_variable = True)
d_elev = dict(zip(list(elev.keys()), ['elevation']))
d_elev.update({'x': 'longitude', 'y': 'latitude'})
elev = elev.rename(d_elev)
elev = elev.rio.write_crs("epsg:4326", inplace = False).rio.clip(europe.geometry.buffer(-0.).values, europe.crs)
elev = elev.interp(latitude = nc_sample.latitude, longitude = nc_sample.longitude)
# elev['elevation'].plot.surface()

hillshade = es.hillshade(elev.where(elev['elevation'] >= 0)['elevation'])[np.newaxis, :, :]
hillshade = xr.DataArray(
    data = hillshade,
    dims = ['time', 'latitude', 'longitude'],
    coords = dict(
        time = (['time'], nc_sample.time.data),
        latitude = (["latitude"], nc_sample.latitude.data),
        longitude =(["longitude"], nc_sample.longitude.data),
    ),
)
# ------------------------------------------------------------------------------

for flux in ['GPP', 'Reco', 'NEE', 'H', 'LE']:
    nc_flux = []
    for p in df_path[flux]:
        year = p.stem.split('-')[-1]
        nct = xr.load_dataset(p).resample(time = '1YS').mean()
        nc_lc = nc_lcr.interp(latitude = nct.latitude, longitude = nct.longitude)
        nct = nct.where(nc_lc['water'] < 0.2)
        nct = nct.drop_vars('spatial_ref')
        nct['hillshade'] = hillshade
        vmin, vmax = flux_range[flux]
        fig, ax = setup_canvas(1, 1, figsize = (7, 5.24))
        plot_ = nct[flux].plot(
            ax = ax,
            vmin = vmin, vmax = vmax, cmap = cmap_dict[flux],
            cbar_kwargs={'orientation':'vertical', 'shrink':0.91, 'aspect':20, 'pad': 0.0},
        )
        # ax = plot_.axes
        world.plot(ax = ax, alpha = 0.3, facecolor = 'None')
        nct['hillshade'].plot(ax = ax, alpha = 0.3, cmap = 'gray', add_colorbar=False)
        ax.set_xlabel('Longitude')
        ax.set_ylabel('Latitude')
        ax.set_title('')
        ax.text(0.05, 0.9, year, transform = ax.transAxes)
        break
    break

flux = 'GPP'
select_day = pd.to_datetime('2010-06-15')
file_idx = 10

flux_range = {
    'GPP': [0, 10],
    'Reco': [0, 10],
    'NEE': [-5, 5],
    'H': [0, 100],
    'LE': [0, 100]

}

cmap_dict = {
    'GPP': 'viridis_r',
    'Reco': 'plasma_r',
    'NEE': 'RdYlGn_r',
    'H': 'coolwarm',
    'LE': 'coolwarm',
}

# ------------------------------------------------------------------------------
nc_sample = xr.load_dataset(df_path.iloc[0, 0]).resample(time = '1YS').mean()

elev = rxr.open_rasterio(root_proj.joinpath('SRTM90.tif'), band_as_variable = True)
d_elev = dict(zip(list(elev.keys()), ['elevation']))
d_elev.update({'x': 'longitude', 'y': 'latitude'})
elev = elev.rename(d_elev)
elev = elev.rio.write_crs("epsg:4326", inplace = False).rio.clip(europe.geometry.buffer(-0.).values, europe.crs)
elev = elev.interp(latitude = nc_sample.latitude, longitude = nc_sample.longitude)
# elev['elevation'].plot.surface()

hillshade = es.hillshade(elev.where(elev['elevation'] >= 0)['elevation'])[np.newaxis, :, :]
hillshade = xr.DataArray(
    data = hillshade,
    dims = ['time', 'latitude', 'longitude'],
    coords = dict(
        time = (['time'], nc_sample.time.data),
        latitude = (["latitude"], nc_sample.latitude.data),
        longitude =(["longitude"], nc_sample.longitude.data),
    ),
)
# ------------------------------------------------------------------------------

p = df_path[flux][file_idx]

print(p.stem)
nct = xr.load_dataset(p)
nc_lc = nc_lcr.interp(latitude = nct.latitude, longitude = nct.longitude)
nct = nct.where(nc_lc['water'] < 0.2)
# nct = nct.where(lucc_modis['IGBP'] != 15.)
# nct = nct.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.geometry.buffer(-0.).values, world.crs)
nct = nct.drop_vars('spatial_ref')
nct['hillshade'] = hillshade
fig, ax = setup_canvas(1, 1, figsize = (7, 5.24))
vmin, vmax = flux_range[flux]
plot_ = nct.sel(time = select_day)[flux].plot(
    ax = ax,
    vmin = vmin, vmax = vmax, cmap = cmap_dict[flux],
    # add_colorbar = False,
    cbar_kwargs={'orientation':'vertical', 'shrink':0.91, 'aspect':20, 'pad': 0.0},
)
# ax = plot_.axes
world.plot(ax = ax, alpha = 0.3, facecolor = 'None')
hillshade.plot(ax = ax, alpha = 0.3, cmap = 'gray', add_colorbar=False)
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
ax.set_title('')
ax.text(0.05, 0.9, select_day.strftime('%d/%m/%Y'), transform = ax.transAxes)
# ax.set_axis_off()

# google.download_file(fig, f"UFLUX-EU-ICOS-{flux}-{select_day.strftime('%Y-%m-%d')}-maps.jpg", dpi = 600)

"""# LUCC (Upscaling-LUCC.ipynb)"""

df_path = []
for p in root_proj3.joinpath('downscaled-fluxes').glob('*.nc'):
    label = p.stem.split('UFLUX-')[1]
    if '-' in label:
        yr, sn = label.split('-')
    else:
        sn = 'annual'
        yr = label
    yr = int(yr)
    df_path.append([yr, sn, p])
df_path = pd.DataFrame(df_path, columns = ['YEAR', 'SEASON', 'PATH'])
df_path = df_path.pivot(index = 'YEAR', columns = 'SEASON')
df_path.columns = df_path.columns.droplevel(0)
df_path

europe_roi = gpd.read_file(root_proj3.joinpath('europe_roi.geojson'))

p1 = df_path['winter'][2019]
p2 = df_path['summer'][2019]
print(p1)
print(p2)
nc1 = xr.load_dataset(p1)#, drop_variables = ['Reco', 'NEE', 'H', 'LE'])
nc2 = xr.load_dataset(p2)#, drop_variables = ['Reco', 'NEE', 'H', 'LE'])
nc1 = nc1.rio.write_crs("epsg:4326", inplace = False).rio.clip(europe_roi.geometry.values, europe_roi.crs)
nc2 = nc2.rio.write_crs("epsg:4326", inplace = False).rio.clip(europe_roi.geometry.values, europe_roi.crs)

nc = (nc1 + nc2) / 2

nc = nc.drop_vars(['spatial_ref'])

flux = 'NEE'

flux_range = {
    'GPP': [0, 10],
    'Reco': [0, 10],
    'NEE': [-5, 5],
    'H': [0, 100],
    'LE': [0, 100]

}

cmap_dict = {
    'GPP': 'viridis_r',
    'Reco': 'plasma_r',
    'NEE': 'RdYlGn_r',
    'H': 'coolwarm',
    'LE': 'coolwarm',
}

nct = nc[flux]
nc_lc = nc_lcr.interp(latitude = nct.latitude, longitude = nct.longitude)
nct = nct.where(nc_lc['water'] < 0.2)

# ------------------------------------------------------------------------------
elev = rxr.open_rasterio(root_proj.joinpath('SRTM90.tif'), band_as_variable = True)
d_elev = dict(zip(list(elev.keys()), ['elevation']))
d_elev.update({'x': 'longitude', 'y': 'latitude'})
elev = elev.rename(d_elev)
elev = elev.rio.write_crs("epsg:4326", inplace = False).rio.clip(europe.geometry.buffer(-0.).values, europe.crs)
elev = elev.interp(latitude = nct.latitude, longitude = nct.longitude)
# elev['elevation'].plot.surface()

hillshade = es.hillshade(elev.where(elev['elevation'] >= 0)['elevation'])
hillshade = xr.DataArray(
    data = hillshade,
    dims = ['latitude', 'longitude'],
    coords = dict(
        latitude = (["latitude"], nct.latitude.data),
        longitude =(["longitude"], nct.longitude.data),
    ),
)
# ------------------------------------------------------------------------------

fig, ax = setup_canvas(1, 1, figsize = (7, 5.24))
vmin, vmax = flux_range[flux]
nct.plot(
    ax = ax, vmin = vmin, vmax = vmax, cmap = cmap_dict[flux],
    cbar_kwargs={'orientation':'vertical', 'shrink':0.91, 'aspect':20, 'pad': 0.0},
)
world.plot(ax = ax, alpha = 0.3, facecolor = 'None')
hillshade.plot(ax = ax, alpha = 0.3, cmap = 'gray', add_colorbar=False)
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')

# google.download_file(fig, f'UFLUX-EU-LUCC-{flux}-maps.jpg', dpi = 600)
# google.download_file(fig, f'UFLUX-EU-LUCC-{flux}-maps.pdf')

"""# UK UFLUX"""

# hillshade

!pip install earthpy

import earthpy as et
import earthpy.spatial as es
import earthpy.plot as ep

from shapely.ops import cascaded_union

warnings.simplefilter('ignore')

world = gpd.read_file(root.joinpath('workspace/project_data/o3_model/world_json/world-continents.json'))
boundary = gpd.GeoSeries(cascaded_union(world.geometry))[0]

europe = gpd.read_file(root_proj3.joinpath('europe_roi.geojson'))

flux = 'GPP'

flux_range = {
    'GPP': [0, 10],
    'Reco': [0, 10],
    'NEE': [-5, 5],
    'H': [0, 100],
    'LE': [0, 100]

}

cmap_dict = {
    'GPP': 'viridis_r',
    'Reco': 'plasma_r',
    'NEE': 'RdYlGn_r',
    'H': 'coolwarm',
    'LE': 'coolwarm',
}


nc = xr.load_dataset(root_proj.joinpath('UK-UFLUX_20221221-20221231.nc')).interpolate_na(dim = 'longitude', limit = 3, max_gap = None)#.interpolate_na(dim = 'latitude', limit = 10, max_gap = None)

# ------------------------------------------------------------------------------
elev = rxr.open_rasterio(root_proj.joinpath('SRTM90.tif'), band_as_variable = True)
d_elev = dict(zip(list(elev.keys()), ['elevation']))
d_elev.update({'x': 'longitude', 'y': 'latitude'})
elev = elev.rename(d_elev)
elev = elev.rio.write_crs("epsg:4326", inplace = False).rio.clip(europe.geometry.buffer(-0.).values, europe.crs)
elev = elev.interp(latitude = nc.latitude, longitude = nc.longitude)
# elev['elevation'].plot.surface()

hillshade = es.hillshade(elev.where(elev['elevation'] >= 0)['elevation'])
hillshade = xr.DataArray(
    data = hillshade,
    dims = ['latitude', 'longitude'],
    coords = dict(
        latitude = (["latitude"], nc.latitude.data),
        longitude =(["longitude"], nc.longitude.data),
    ),
)
# ------------------------------------------------------------------------------

fig, ax = setup_canvas(1, 1, figsize = (7, 5.24))
vmin, vmax = flux_range[flux]
plot_ = nc[flux].plot(
    ax = ax,
    vmin = vmin, vmax = vmax, cmap = cmap_dict[flux],
    # add_colorbar = False,
    cbar_kwargs={'orientation':'vertical', 'shrink':0.91, 'aspect':20, 'pad': 0.0},
)
# ax = plot_.axes
world.plot(ax = ax, alpha = 0.3, facecolor = 'None')
hillshade.plot(ax = ax, alpha = 0.3, cmap = 'gray', add_colorbar=False)
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
ax.set_title('')
ax.text(0.05, 0.9, '21/12/2022-31/12/2022', transform = ax.transAxes)
# ax.set_axis_off()

# google.download_file(fig, f"UFLUX-EU-ICOS-{flux}-{select_day.strftime('%Y-%m-%d')}-maps.jpg", dpi = 600)





"""# Flux to ecosystem response"""

# NEE to PPFD
# Reco to air temperature

ec_path = []
for p in root.joinpath('workspace/datasets/FLUXNET2015').rglob('*_FULLSET_DD_*.csv'):
    ec_path.append([p, p.stem.split('_')[1]])
ec_path = pd.DataFrame(ec_path, columns = ['PATH', 'ID']).set_index('ID')

df_train = load_pickle(root_proj1.joinpath('dataset/train_space.pkl'))

x_name = 'TA_F'  # PPFD_IN, TA_F
y_name = 'Reco' # NEE, Reco

from tqdm import tqdm
from numpy import record
from scitbx.stutils import pprint, stats_measures_df

paths_GPP = root_proj0.joinpath(f'landsat7-res').glob('*.csv'); name_GPP = 'GPP'
paths_Reco = root_proj0.joinpath(f'results_20220301_reco/validation').glob('*.csv'); name_Reco = 'Reco'
paths_NEE = root_proj0.joinpath(f'results_20220301_reco/validation_nee').glob('*.csv'); name_NEE = 'NEE'
paths_H = root_proj0.joinpath(f'results_20220301_h/validation').glob('*.csv'); name_H = 'H'
paths_LE = root_proj0.joinpath(f'results_20220301_le/validation').glob('*.csv'); name_LE = 'LE'

GPP_path = pd.DataFrame([[p.stem.split('_')[0], p] for p in paths_GPP], columns = ['ID', 'GPP']).set_index('ID')
Reco_path = pd.DataFrame([[p.stem, p] for p in paths_Reco], columns = ['ID', 'Reco']).set_index('ID')
NEE_path = pd.DataFrame([[p.stem, p] for p in paths_NEE], columns = ['ID', 'NEE']).set_index('ID')
H_path = pd.DataFrame([[p.stem, p] for p in paths_H], columns = ['ID', 'H']).set_index('ID')
LE_path = pd.DataFrame([[p.stem, p] for p in paths_LE], columns = ['ID', 'LE']).set_index('ID')

df_path = pd.concat([GPP_path, Reco_path, NEE_path, H_path, LE_path], axis = 1).dropna()

dfa = []
for site in tqdm(df_path.index):
    try:
        p = df_path.loc[site, y_name] # CHANGE HERE!! NEE or Reco

        dft = pd.read_csv(p, index_col = 0)
        dft.index = pd.to_datetime(dft.index, format = '%Y-%m-%d')
        # dft = pd.concat([dft, df_train[df_train['ID'] == site]['SWIN']], axis = 1).dropna()

        p = ec_path.loc[site, 'PATH']
        df_ec = pd.read_csv(p, index_col = 0, usecols = ['TIMESTAMP', 'TA_F', 'TA_F_QC', 'VPD_F', 'VPD_F_QC', 'PPFD_IN', 'PPFD_IN_QC']).replace(-9999., np.nan)
        df_ec.index = pd.to_datetime(df_ec.index, format = '%Y%m%d')

        dft = pd.concat([dft, df_ec], axis = 1)
        dft['ID'] = site
        dft['IGBP'] = meta.loc[site, 'IGBP']
        dfa.append(dft)
    except Exception as e:
        print(e)

dfa = pd.concat(dfa)

def get_n_sites_years(dfa, igbp_code):
    dft = dfa.copy().loc[dfa['IGBP'] == igbp_code, 'ID']
    # print(len(dft.drop_duplicates()))

    df_site_year = []
    for g, gp in dft.reset_index().groupby('ID'):
        gp = gp.set_index('index')
        nyear = len(gp.index.year.drop_duplicates())
        df_site_year.append(nyear)
    df_site_year = np.array(df_site_year)
    n_sites = len(df_site_year)
    n_years = np.sum(df_site_year)
    return n_sites, n_years

# ------------------------------------------------------------------------------
import matplotlib.gridspec as gridspec

fig = plt.figure(figsize = (16, 12))
gs = gridspec.GridSpec(12, 16, figure = fig)
gs.update(wspace = 10, hspace = 2)
ax1 = plt.subplot(gs[0:4, :4])
ax2 = plt.subplot(gs[0:4, 4:8])
ax3 = plt.subplot(gs[0:4, 8:12])
ax4 = plt.subplot(gs[0:4, 12::])

ax5 = plt.subplot(gs[4:8, :4])
ax6 = plt.subplot(gs[4:8, 4:8])
ax7 = plt.subplot(gs[4:8, 8:12])
ax8 = plt.subplot(gs[4:8, 12::])

ax9 = plt.subplot(gs[8:, 2:6])
ax10 = plt.subplot(gs[8:, 6:10])
ax11 = plt.subplot(gs[8:, 10:14])

axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11]
# ------------------------------------------------------------------------------
for cnt, igbp_code in enumerate(['CRO', 'CSH', 'DBF', 'EBF', 'ENF', 'GRA', 'MF', 'OSH', 'WET', 'WSA', 'SAV']):
    ax = axes[cnt]
    n_sites, n_years = get_n_sites_years(dfa, igbp_code)
    dft = dfa[['IGBP', 'truth', 'pred', x_name]].reset_index().dropna()
    dft = dft.groupby(['index', 'IGBP']).mean().reset_index().set_index('index')
    # print(dft['IGBP'].drop_duplicates().values)
    dft = dft[dft['IGBP'] == igbp_code].dropna()
    dfp = dft.copy()
    # fig, ax = setup_canvas(1, 1)
    ax.scatter(dfp[x_name], dfp['truth'], color = colors[0], s = 15, edgecolor = 'gray', alpha = 0.3)
    ax.scatter(dfp[x_name], dfp['pred'], color = colors[1], s = 15, edgecolor = 'gray', alpha = 0.3)

    dfp = dfp.sort_values(by = x_name)
    plot_curve_bespoke('poly2', ax, dfp[x_name], dfp['truth'], precision = 2, with_text = False, lineshape = '-')
    plot_curve_bespoke('poly2', ax, dfp[x_name], dfp['pred'], precision = 2, with_text = False, lineshape = '-.')

    ax.text(0.1, 0.1, f"{igbp_code}\n{n_sites} towers\n{n_years} site-years", transform = ax.transAxes)

# ax5.set_ylabel('NEE ($gC \ m^{-2} \ d^{-1}$)')
# ax10.set_xlabel('PPFD ($\mu mol \ Photon \ m^{-2} \ s^{-1}$)')

ax5.set_ylabel('Reco ($gC \ m^{-2} \ d^{-1}$)')
ax10.set_xlabel('TA ($^\circ C$)')

# google.download_file(fig, f"NEE-PPFD-response.jpg", dpi = 600)
# google.download_file(fig, f"Reco-TA-response.jpg", dpi = 600)

"""# WUE and EBR"""

from scitbx.sciplt import *
fig, axes = setup_canvas(1, 2, figsize = (12, 6), sharex = False, sharey = False, wspace = 0.2)

"""## WUE"""

ec_path = []
for p in root.joinpath('workspace/datasets/FLUXNET2015').rglob('*_FULLSET_DD_*.csv'):
    ec_path.append([p, p.stem.split('_')[1]])
ec_path = pd.DataFrame(ec_path, columns = ['PATH', 'ID']).set_index('ID')

from tqdm import tqdm
from numpy import record
from scitbx.stutils import pprint, stats_measures_df

paths_GPP = root_proj0.joinpath(f'landsat7-res').glob('*.csv'); name_GPP = 'GPP'
paths_Reco = root_proj0.joinpath(f'results_20220301_reco/validation').glob('*.csv'); name_Reco = 'Reco'
paths_NEE = root_proj0.joinpath(f'results_20220301_reco/validation_nee').glob('*.csv'); name_NEE = 'NEE'
paths_H = root_proj0.joinpath(f'results_20220301_h/validation').glob('*.csv'); name_H = 'H'
paths_LE = root_proj0.joinpath(f'results_20220301_le/validation').glob('*.csv'); name_LE = 'LE'

GPP_path = pd.DataFrame([[p.stem.split('_')[0], p] for p in paths_GPP], columns = ['ID', 'GPP']).set_index('ID')
Reco_path = pd.DataFrame([[p.stem, p] for p in paths_Reco], columns = ['ID', 'Reco']).set_index('ID')
NEE_path = pd.DataFrame([[p.stem, p] for p in paths_NEE], columns = ['ID', 'NEE']).set_index('ID')
H_path = pd.DataFrame([[p.stem, p] for p in paths_H], columns = ['ID', 'H']).set_index('ID')
LE_path = pd.DataFrame([[p.stem, p] for p in paths_LE], columns = ['ID', 'LE']).set_index('ID')

df_path = pd.concat([GPP_path, Reco_path, NEE_path, H_path, LE_path], axis = 1).dropna()

dfa = []
for site in tqdm(df_path.index):
    try:
        p = df_path.loc[site, 'GPP'] # CHANGE HERE!! NEE or Reco
        dft_GPP = pd.read_csv(p, index_col = 0)
        dft_GPP.index = pd.to_datetime(dft_GPP.index, format = '%Y-%m-%d')
        dft_GPP.columns = ['truth_GPP', 'pred_GPP']

        p = df_path.loc[site, 'LE'] # CHANGE HERE!! NEE or Reco
        dft_LE = pd.read_csv(p, index_col = 0)
        dft_LE.index = pd.to_datetime(dft_LE.index, format = '%Y-%m-%d')
        dft_LE.columns = ['truth_LE', 'pred_LE']

        p = ec_path.loc[site, 'PATH']
        df_ec = pd.read_csv(p, index_col = 0, usecols = ['TIMESTAMP', 'TA_F', 'TA_F_QC', 'VPD_F', 'VPD_F_QC', 'PPFD_IN', 'PPFD_IN_QC']).replace(-9999., np.nan)
        df_ec.index = pd.to_datetime(df_ec.index, format = '%Y%m%d')

        dft = pd.concat([dft_GPP, dft_LE, df_ec], axis = 1).resample('1MS').mean()
        dft['ID'] = site
        dft['IGBP'] = meta.loc[site, 'IGBP']
        dfa.append(dft)
    except Exception as e:
        print(e)

dfa = pd.concat(dfa)

from scipy.stats import gaussian_kde

def kde_scatter(ax, dfp, x_name, y_name, frac = 0.1):
    dfp = dfp[[x_name, y_name]].dropna().sample(frac = frac)
    x = dfp[x_name]
    y = dfp[y_name]

    # Calculate the point density
    xy = np.vstack([x,y])
    z = gaussian_kde(xy)(xy)

    # Sort the points by density, so that the densest points are plotted last
    idx = z.argsort()
    x, y, z = x[idx], y[idx], z[idx]

    # fig, ax = plt.subplots(1, 1, figsize = (9, 9))
    ax.scatter(x, y, c=z, s=50, cmap = 'RdYlBu_r')

    xl = np.arange(np.floor(x.min()), np.ceil(x.max()))
    ax.plot(xl, xl, ls = '-.', color = 'k')

import warnings
from scitbx import bigleaf

warnings.simplefilter('ignore')
df_wue = dfa.dropna()

'''
Unit conversion:
# https://bitbucket.org/juergenknauer/bigleaf/src/master/R/WUE_metrics.r
# https://cran.r-project.org/web/packages/bigleaf/bigleaf.pdf

ET  <- LE.to.ET(LE,Tair)                 # kg H2O m-2 s-1
GPP <- (GPP * constants$umol2mol * constants$Cmol) * constants$kg2g  # gC m-2 s-1
NEE <- (NEE * constants$umol2mol * constants$Cmol) * constants$kg2g  # gC m-2 s-1

'''
coef = 1.03772448 # covert um CO2 m-2 s-1 to gC m-2 d-1
umol2mol = 1e-06  # conversion micromole (umol) to mole (mol)
Cmol = 0.012011   # molar mass of carbon (kg mol-1)
kg2g = 1000       # conversion kilogram (kg) to gram (g)

df_wue['truth_GPP'] = (df_wue['truth_GPP'] / coef * umol2mol * Cmol) * kg2g
df_wue['pred_GPP'] = (df_wue['pred_GPP'] / coef * umol2mol * Cmol) * kg2g

ET_truth = bigleaf.LE_to_ET(df_wue['truth_LE'], df_wue['TA_F'])  # kg H2O m-2 s-1
ET_pred = bigleaf.LE_to_ET(df_wue['pred_LE'], df_wue['TA_F'])  # kg H2O m-2 s-1

WUE = pd.concat([
    (df_wue['truth_GPP'] / ET_truth).rename('WUE_EC'),
    (df_wue['pred_GPP'] / ET_pred).rename('WUE_UFLUX'),
], axis = 1)

WUE[WUE < 0] = np.nan
q1 = WUE.mean(axis = 1).quantile(0.25)
q3 = WUE.mean(axis = 1).quantile(0.75)
iqr = q3 - q1
WUE.loc[WUE['WUE_EC'] > q3 + 3 * iqr, 'WUE_EC'] = np.nan
WUE.loc[WUE['WUE_UFLUX'] > q3 + 3 * iqr, 'WUE_UFLUX'] = np.nan
df_wue['WUE_EC'] = WUE['WUE_EC']
df_wue['WUE_UFLUX'] = WUE['WUE_UFLUX']
df_wue = df_wue[['ID', 'IGBP', 'WUE_EC', 'WUE_UFLUX']].dropna()

WUE = WUE.dropna()
x = WUE['WUE_EC']
y = WUE['WUE_UFLUX']

# fig, ax = setup_canvas(1, 1, figsize = (6, 6))
ax = axes[0]
# ax.scatter(x, y, s = 20, c = colors[0], edgecolor = 'k', alpha = 0.3)
kde_scatter(ax, WUE, 'WUE_EC', 'WUE_UFLUX', frac = 1)

# plot_curve_bespoke('lin', ax, WUE['WUE_EC'], WUE['WUE_UFLUX'], precision = 2, with_text = False, lineshape = '-')
res = regress2(x.values, y.values, _method_type_2="reduced major axis", _need_intercept=False)

slope = np.round(res['slope'], 2)
intercept = np.round(res['intercept'], 2)
r2 = np.round(res['r'][0] ** 2, 2)
# ax.plot(x, slope * x + intercept, '-', color = 'k', label = 'Fitted')


# ax.set_xlim(0, q3 + 3 * iqr)
# ax.set_ylim(0, q3 + 3 * iqr)
ax.set_xlim(0, 100)
ax.set_ylim(0, 100)

ax.text(0.05, 0.8, f'(a)\ny = {slope}x + {intercept}' + '\n$R^2$: ' + f'{r2}', transform = ax.transAxes)

ax.set_xlabel('WUE EC $(gC \ (kg \ H20)^{-1})$')
ax.set_ylabel('WUE UFLUX $(gC \ (kg \ H20)^{-1})$')

# google.download_file(fig, f"WUE.jpg", dpi = 600)
# google.download_file(fig, f"WUE-kde.jpg", dpi = 600)

# def get_n_sites_years(dfa, igbp_code):
#     dft = dfa.copy().loc[dfa['IGBP'] == igbp_code, 'ID']
#     # print(len(dft.drop_duplicates()))

#     df_site_year = []
#     for g, gp in dft.reset_index().groupby('ID'):
#         gp = gp.set_index('index')
#         nyear = len(gp.index.year.drop_duplicates())
#         df_site_year.append(nyear)
#     df_site_year = np.array(df_site_year)
#     n_sites = len(df_site_year)
#     n_years = np.sum(df_site_year)
#     return n_sites, n_years

# # ------------------------------------------------------------------------------
# import matplotlib.gridspec as gridspec

# fig = plt.figure(figsize = (16, 12))
# gs = gridspec.GridSpec(12, 16, figure = fig)
# gs.update(wspace = 10, hspace = 2)
# ax1 = plt.subplot(gs[0:4, :4])
# ax2 = plt.subplot(gs[0:4, 4:8])
# ax3 = plt.subplot(gs[0:4, 8:12])
# ax4 = plt.subplot(gs[0:4, 12::])

# ax5 = plt.subplot(gs[4:8, :4])
# ax6 = plt.subplot(gs[4:8, 4:8])
# ax7 = plt.subplot(gs[4:8, 8:12])
# ax8 = plt.subplot(gs[4:8, 12::])

# ax9 = plt.subplot(gs[8:, 2:6])
# ax10 = plt.subplot(gs[8:, 6:10])
# ax11 = plt.subplot(gs[8:, 10:14])

# axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11]
# # ------------------------------------------------------------------------------
# for cnt, igbp_code in enumerate(['CRO', 'CSH', 'DBF', 'EBF', 'ENF', 'GRA', 'MF', 'OSH', 'WET', 'WSA', 'SAV']):
#     ax = axes[cnt]
#     n_sites, n_years = get_n_sites_years(df_wue, igbp_code)
#     dft = df_wue.reset_index().dropna()
#     dft = dft.groupby(['index', 'IGBP']).mean().reset_index().set_index('index')
#     # print(dft['IGBP'].drop_duplicates().values)
#     dft = dft[dft['IGBP'] == igbp_code].dropna()
#     dfp = dft.copy()[['WUE_EC', 'WUE_UFLUX']]


#     dfp[dfp < 0] = np.nan
#     q1 = dfp.mean(axis = 1).quantile(0.25)
#     q3 = dfp.mean(axis = 1).quantile(0.75)
#     iqr = q3 - q1
#     dfp.loc[dfp['WUE_EC'] > q3 + 3 * iqr, 'WUE_EC'] = np.nan
#     dfp.loc[dfp['WUE_UFLUX'] > q3 + 3 * iqr, 'WUE_UFLUX'] = np.nan
#     dfp = dfp.dropna()

#     # fig, ax = setup_canvas(1, 1)
#     ax.scatter(dfp['WUE_EC'], dfp['WUE_UFLUX'], color = colors[0], s = 15, edgecolor = 'gray', alpha = 0.3)

#     dfp = dfp.sort_values(by = 'WUE_EC')
#     plot_curve_bespoke('lin', ax, dfp['WUE_EC'], dfp['WUE_UFLUX'], precision = 2, with_text = False, lineshape = '-')

#     ax.text(0.1, 0.1, f"{igbp_code}\n{n_sites} towers\n{n_years} site-years", transform = ax.transAxes)
#     # ax.set_xlim(0, 100)
#     # ax.set_ylim(0, 100)

# # ax5.set_ylabel('NEE ($gC \ m^{-2} \ d^{-1}$)')
# # ax10.set_xlabel('PPFD ($\mu mol \ Photon \ m^{-2} \ s^{-1}$)')
# # ax5.set_ylabel('Reco ($gC \ m^{-2} \ d^{-1}$)')
# # ax10.set_xlabel('TA ($^\circ C$)')

"""## EBR (Upscaling-universe.ipynb)"""

ec_path = []
for p in root.joinpath('workspace/datasets/FLUXNET2015').rglob('*_FULLSET_DD_*.csv'):
    ec_path.append([p, p.stem.split('_')[1]])
ec_path = pd.DataFrame(ec_path, columns = ['PATH', 'ID']).set_index('ID')

from tqdm import tqdm
from numpy import record
from scitbx.stutils import pprint, stats_measures_df

paths_GPP = root_proj0.joinpath(f'landsat7-res').glob('*.csv'); name_GPP = 'GPP'
paths_Reco = root_proj0.joinpath(f'results_20220301_reco/validation').glob('*.csv'); name_Reco = 'Reco'
paths_NEE = root_proj0.joinpath(f'results_20220301_reco/validation_nee').glob('*.csv'); name_NEE = 'NEE'
paths_H = root_proj0.joinpath(f'results_20220301_h/validation').glob('*.csv'); name_H = 'H'
paths_LE = root_proj0.joinpath(f'results_20220301_le/validation').glob('*.csv'); name_LE = 'LE'

GPP_path = pd.DataFrame([[p.stem.split('_')[0], p] for p in paths_GPP], columns = ['ID', 'GPP']).set_index('ID')
Reco_path = pd.DataFrame([[p.stem, p] for p in paths_Reco], columns = ['ID', 'Reco']).set_index('ID')
NEE_path = pd.DataFrame([[p.stem, p] for p in paths_NEE], columns = ['ID', 'NEE']).set_index('ID')
H_path = pd.DataFrame([[p.stem, p] for p in paths_H], columns = ['ID', 'H']).set_index('ID')
LE_path = pd.DataFrame([[p.stem, p] for p in paths_LE], columns = ['ID', 'LE']).set_index('ID')

df_path = pd.concat([GPP_path, Reco_path, NEE_path, H_path, LE_path], axis = 1).dropna()

dfa = []
for site in tqdm(df_path.index):
    try:
        p = df_path.loc[site, 'H'] # CHANGE HERE!! NEE or Reco
        dft_H = pd.read_csv(p, index_col = 0)
        dft_H.index = pd.to_datetime(dft_H.index, format = '%Y-%m-%d')
        dft_H.columns = ['truth_H', 'pred_H']

        p = df_path.loc[site, 'LE'] # CHANGE HERE!! NEE or Reco
        dft_LE = pd.read_csv(p, index_col = 0)
        dft_LE.index = pd.to_datetime(dft_LE.index, format = '%Y-%m-%d')
        dft_LE.columns = ['truth_LE', 'pred_LE']

        p = ec_path.loc[site, 'PATH']
        df_ec = pd.read_csv(p, index_col = 0, usecols = ['TIMESTAMP', 'NETRAD', 'G_F_MDS']).replace(-9999., np.nan)
        df_ec.index = pd.to_datetime(df_ec.index, format = '%Y%m%d')

        dft = pd.concat([dft_H, dft_LE, df_ec], axis = 1).resample('1MS').mean()
        dft['ID'] = site
        dft['IGBP'] = meta.loc[site, 'IGBP']
        dfa.append(dft)
    except Exception as e:
        print(e)

dfa = pd.concat(dfa)

df_ebr = dfa.copy().dropna()
df_ebr['NETRAD-G'] = df_ebr['NETRAD'] - df_ebr['G_F_MDS']
df_ebr['H+LE_EC'] = df_ebr['truth_H'] + df_ebr['truth_LE']
df_ebr['H+LE_UFLUX'] = df_ebr['pred_H'] + df_ebr['pred_LE']
dfp = pd.concat([
    (df_ebr['H+LE_EC'] / df_ebr['NETRAD-G']).rename('EBR_EC'),
    (df_ebr['H+LE_UFLUX'] / df_ebr['NETRAD-G']).rename('EBR_UFLUX')
], axis = 1)

dfp[dfp < 0] = np.nan
q1 = dfp.mean(axis = 1).quantile(0.25)
q3 = dfp.mean(axis = 1).quantile(0.75)
iqr = q3 - q1
dfp.loc[dfp['EBR_EC'] > q3 + 3 * iqr, 'EBR_EC'] = np.nan
dfp.loc[dfp['EBR_UFLUX'] > q3 + 3 * iqr, 'EBR_UFLUX'] = np.nan
# dfp[dfp > 10] = np.nan

dfp = dfp.dropna()

from scipy.stats import gaussian_kde

def kde_scatter(ax, dfp, x_name, y_name):
    dfp = dfp[[x_name, y_name]].dropna()#.sample(frac = 0.3)
    x = dfp[x_name]
    y = dfp[y_name]

    # Calculate the point density
    xy = np.vstack([x,y])
    z = gaussian_kde(xy)(xy)

    # Sort the points by density, so that the densest points are plotted last
    idx = z.argsort()
    x, y, z = x[idx], y[idx], z[idx]

    # fig, ax = plt.subplots(1, 1, figsize = (9, 9))
    ax.scatter(x, y, c=z, s=50, cmap = 'RdYlBu_r')

    xl = np.arange(np.floor(x.min()), np.ceil(x.max()))
    ax.plot(xl, xl, ls = '-.', color = 'k')

ax =axes[1]
# fig, ax = setup_canvas(1, 1, figsize = (6, 6))
x = dfp['EBR_EC']
y = dfp['EBR_UFLUX']
res = regress2(x.values, y.values, _method_type_2="reduced major axis", _need_intercept=False)

slope = np.round(res['slope'], 2)
intercept = np.round(res['intercept'], 2)
r2 = np.round(res['r'][0] ** 2, 2)

ax.set_xlim(0, 2)
ax.set_ylim(0, 2)
ax.text(0.05, 0.8, f'(b)\ny = {slope}x + {intercept}' + '\n$R^2$: ' + f'{r2}', transform = ax.transAxes)

kde_scatter(ax, dfp, 'EBR_EC', 'EBR_UFLUX')
ax.set_xlabel('EBR EC ([H + LE] / [NETRAD - G])')
ax.set_ylabel('EBR UFLUX ([H + LE] / [NETRAD - G])')

# google.download_file(fig, f"EBR.jpg", dpi = 600)

fig

# google.download_file(fig, f"WUE+EBR.jpg", dpi = 600)

"""# Obsolete"""

# nca = []
# for name in prod_path.keys():
#     if name == 'MODIS-NIRv-ERA5-WY': continue
#     df_path = prod_path[name]
#     nc_prod = []
#     for flux in df_path.columns:
#         print(name, flux)
#         nc_flux = []
#         for p in df_path[flux]:
#             nct = xr.load_dataset(p)
#             nct = nct.resample(time = '1YS').mean() #[flux].rename('ooo').to_dataset()
#             nc_flux.append(nct)
#             nct.close(); del(nct)
#         nc_flux = xr.merge(nc_flux)
#         nc_prod.append(nc_flux)
#         nc_flux.close(); del(nc_flux)
#     nc_prod = xr.merge(nc_prod).mean(dim = 'time').expand_dims(prod = [name]) #.to_array()
#     nca.append(nc_prod)
# nca = xr.merge(nca)

# nca['GPP'].plot(
#     x="longitude",
#     y="latitude",
#     # col="prod",
#     row = 'prod',
#     col_wrap=3,
# )

# # ------------------------------------------------------------------------------------------------

# nca = []
# for name in prod_path.keys():
#     if name == 'MODIS-NIRv-ERA5-WY': continue
#     df_path = prod_path[name]
#     nc_prod = []
#     for flux in df_path.columns:
#         print(name, flux)
#         nc_flux = []
#         for p in df_path[flux]:
#             nct = xr.load_dataset(p)
#             if flux in ['GPP', 'Reco', 'NEE']:
#                 val = gC2Pg_v2(nct, flux, 0.25, 0.25)
#             else:
#                 val = W2MJ(nct, flux, 0.25)
#             nc_flux.append([val])
#             nct.close(); del(nct)
#         nc_flux = xr.merge(nc_flux)
#         nc_prod.append(nc_flux)
#         nc_flux.close(); del(nc_flux)
#     nc_prod = xr.merge(nc_prod).mean(dim = 'time').expand_dims(prod = [name]) #.to_array()
#     nca.append(nc_prod)
# nca = xr.merge(nca)

# nca['GPP'].plot(
#     x="longitude",
#     y="latitude",
#     # col="prod",
#     row = 'prod',
#     col_wrap=3,
# )