# -*- coding: utf-8 -*-
"""Trendy-emulator-v1.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nB3wIadSakZLjA8EzFW66Gpvi7vdv6LC
"""

!pip install scitbx --quiet
!pip install netcdf4 --quiet

from scitbx import google
root = google.mount_drive()
root_proj = root.joinpath("workspace/project_data/emulator")
root_proj_platform = root.joinpath("workspace/project_data/platform")
# root_proj_card = root.joinpath("workspace/project_data/CARDAMOM/Global")
root_proj_trendy = root.joinpath("shared_files/Trendy")

from scitbx.easy_import import *
from sciml import regress2
from scigeo.remotesensing import *
from scigeo import climate
from sciml import pipelines

from scieco import photosynthesis, respiration, evapotranspiration
from scigeo.meteo import saturation_vapor_pressure, specific_humidity2vapor_pressure

warnings.simplefilter('ignore')

world_path = root.joinpath('workspace/datasets/WB_countries_Admin0_10m/WB_countries_Admin0_10m.shp')
world = gpd.read_file(world_path)

MODIS_IGBP_dict = get_MODIS_IGBPcode()
df_el = climate.get_ONI()

scale = 0.5
lons = np.arange(-179.75, 179.75 + scale, scale)
lats = np.arange(-89.75, 89.75 + scale, scale)
grids = xr.DataArray(
    data=np.zeros([len(lons), len(lats)]),
    dims=["longitude", "latitude"],
    coords=dict(
        longitude=(["longitude"], lons),
        latitude=(["latitude"], lats),
    ),
    attrs=dict(
        description = f"{scale} grids.",
    ),
)

scale = 0.1
lons = np.arange(-179.95, 179.95 + scale, scale)
lats = np.arange(-89.95, 89.95 + scale, scale)
grids_01deg = xr.DataArray(
    data=np.zeros([len(lons), len(lats)]),
    dims=["longitude", "latitude"],
    coords=dict(
        longitude=(["longitude"], lons),
        latitude=(["latitude"], lats),
    ),
    attrs=dict(
        description = f"{scale} grids.",
    ),
)

"""# Functions"""

def load_era5(p_era5, engine = None, drop_variables = None):
    era5 = xr.open_dataset(p_era5, engine = engine, drop_variables = drop_variables)

    data_vars = era5.data_vars
    for v in ['dewpoint_temperature_2m', 'temperature_2m', 'soil_temperature_level_1', 'temperature_2m_min', 'temperature_2m_max']:
        if v in data_vars:
            era5[v] -= 273.15
    for v in [
        'surface_latent_heat_flux_sum', 'surface_sensible_heat_flux_sum', 'surface_solar_radiation_downwards_sum',
        'surface_thermal_radiation_downwards_sum', 'surface_net_solar_radiation_sum', 'surface_net_thermal_radiation_sum'
        ]:
        if v in data_vars:
            era5[v] /= 86400

    if 'surface_pressure' in data_vars:
        era5['surface_pressure'] /= 100

    era5['VPD'] = saturation_vapor_pressure(era5['temperature_2m']) - saturation_vapor_pressure(era5['dewpoint_temperature_2m'])
    if ('surface_net_solar_radiation_sum' in data_vars) and ('surface_net_thermal_radiation_sum' in data_vars):
        era5['surface_net_radiation_sum'] = era5['surface_net_solar_radiation_sum'] + era5['surface_net_thermal_radiation_sum']
        era5 = era5.drop_vars(['surface_net_solar_radiation_sum', 'surface_net_thermal_radiation_sum'])
    era5 = era5.drop_vars('dewpoint_temperature_2m')
    era5 = era5.drop_vars('spatial_ref')
    return era5

drop_variables = [
    'forecast_albedo', 'surface_latent_heat_flux_sum', 'surface_sensible_heat_flux_sum',
    'evaporation_from_the_top_of_canopy_sum', 'temperature_2m_min', 'temperature_2m_max',
    'u_component_of_wind_10m', 'v_component_of_wind_10m', 'snow_cover'
]

from sciml.metrics import get_rmse, calculate_R2

def get_dfm(dfo, label, name1, name2):
    eval_res = regress2(dfo[name1].values, dfo[name2].values)
    rvalue = eval_res['r']
    slope = eval_res['slope']
    intercept = eval_res['intercept']
    mae = (dfo[name2] - dfo[name1]).abs().mean()
    # print(eval_res)

    r2 = eval_res['r']**2
    rmse = get_rmse(dfo[name1].values, dfo[name2].values)
    dfm = pd.DataFrame([[label, calculate_R2(dfo[name1], dfo[name2]), r2, rmse, slope, intercept, dfo[name1].mean(), mae]], columns = ['name', 'R2', 'r2', 'RMSE', 'slope', 'intercept', 'Mean', 'MAE']).set_index('name')
    return dfm

def get_scaling_factor(obs, est):
    dft = pd.concat([obs, est], axis = 1).dropna()
    dft.columns = ['obs', 'est']
    slope, intercept, rvalue, pvalue, stderr = stats.linregress(dft['obs'], dft['est'])
    return slope, intercept

"""# Prepare Trendy data

## Unify spatial resolutions into 0.5deg
"""

def load_trendy(p, name, cname, year_after = 2000, grids = None, engine = 'netcdf4'):
    if name == 'CARDAMOM':
        start_time = '2003-01-01'
    elif name == 'VISIT':
        start_time = '1860-01-01'
    elif name in ['CLM5.0', 'CLASSIC']:
        start_time = '1701-01-01'
    else:
        start_time = '1700-01-01'
    if name == 'ISBA-CTRIP':
        rename_dict = {'time_counter': 'time', 'lon_FULL': 'lon', 'lat_FULL': 'lat'}
    else:
        rename_dict = {}

    # --------------------------------------------------------------------------
    nct = xr.open_dataset(p, decode_times = False, engine = engine)
    try:
        nct = nct[[cname]]
    except Exception:
        print(list(nct.data_vars))
        if len(list(nct.data_vars)) != 1:
            raise ValueError('no such variable')
        else:
            nct = nct.rename({list(nct.data_vars)[0]: cname})

    if rename_dict:
        nct = nct.rename(rename_dict)

    # Convert start_time to a pandas Timestamp
    start_time = pd.to_datetime(start_time)
    if len(nct.time) > 1000:
        new_time = pd.to_datetime([start_time + pd.DateOffset(months = i) for i in range(len(nct.time))])
    else:
        time_months = pd.to_datetime([start_time + pd.DateOffset(months = i) for i in range(len(nct.time))])
        time_years = pd.to_datetime([start_time + pd.DateOffset(years = i) for i in range(len(nct.time))])
        if time_months[-1].year == 2022:
            new_time = time_months
        elif time_years[-1].year == 2022:
            new_time = time_years
        else:
            raise ValueError('no time intepretation')
    nct = nct.assign_coords({
        "time": new_time
    })

    if 'lat' in nct.dims:
        nct = nct.rename({'lon': 'longitude', 'lat': 'latitude'})
    if year_after: nct = nct.where(nct['time.year'] >= 2000, drop = True)

    if float(nct['longitude'].max()) > 300:
        nct = nct.assign_coords({
            "longitude": ((nct['longitude'] + 180) % 360) - 180
        })
    if float(nct['latitude'].max()) > 300:
        nct = nct.assign_coords({
            "latitude": nct['latitude'] - 90
        })

    if not grids is None: nct = nct.interp(latitude = grids.latitude, longitude = grids.longitude)
    nct = nct.rename({cname: name})
    return nct

df_path_trendy_all = []
for p in list(root_proj_trendy.joinpath('1_data/2_trendy_c').glob('*.nc')) + list(root_proj_trendy.joinpath('1_data/v12').glob('*.nc')) + list(root_proj_trendy.joinpath('1_data/3_AGB_grazing').glob('*.nc')):
    fmi = p.stem.split('_')
    model_ = fmi[0]
    scenario_ = fmi[1]
    fluxpool_ = fmi[-1]
    df_path_trendy_all.append([model_, scenario_, fluxpool_, p])
df_path_trendy_all = pd.DataFrame(df_path_trendy_all, columns = ['MODEL', 'SCENARIO', 'FLUXPOOL', 'PATH']).sort_values(['MODEL', 'FLUXPOOL'])
df_path_trendy_all = df_path_trendy_all.query("SCENARIO == 'S3'").drop('SCENARIO', axis = 1).pivot(index = 'MODEL', columns = 'FLUXPOOL', values = 'PATH')

# print(df_path_trendy_all['FLUXPOOL'].unique())

df_path_trendy_all

cname = 'gpp'
df_gpp_trendy_all = []
for name in tqdm(df_path_trendy_all[cname].index):
    p = df_path_trendy_all[cname][name]
    nct = load_trendy(p, name, cname, grids = grids).resample(time = '1YS').mean()

    coef_mat = xr.DataArray(
        deg2m(nct.longitude, nct.latitude, 0.5, 0.5),
        dims = ['latitude', 'longitude'],
        coords = {'longitude': nct.longitude, 'latitude': nct.latitude}
    ).expand_dims(time = nct.time)

    dft = (nct * 1000 * 86400 * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365
    df_gpp_trendy_all.append(dft)

df_gpp_trendy_all = pd.concat(df_gpp_trendy_all, axis = 1)

# df_gpp_trendy_all.plot()

# df_gpp_trendy_all['CARDAMOM'].plot()


# # ============================================================================

cname = 'ra'
df_ra_trendy_all = []
for name in tqdm(df_path_trendy_all[cname].index):
    p = df_path_trendy_all[cname][name]
    nct = load_trendy(p, name, cname, grids = grids).resample(time = '1YS').mean()

    coef_mat = xr.DataArray(
        deg2m(nct.longitude, nct.latitude, 0.5, 0.5),
        dims = ['latitude', 'longitude'],
        coords = {'longitude': nct.longitude, 'latitude': nct.latitude}
    ).expand_dims(time = nct.time)

    dft = (nct * 1000 * 86400 * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365
    df_ra_trendy_all.append(dft)

df_ra_trendy_all = pd.concat(df_ra_trendy_all, axis = 1)

# ------------------------------------------------------------------------------

cname = 'rh'
df_rh_trendy_all = []
for name in tqdm(df_path_trendy_all[cname].dropna().index):
    p = df_path_trendy_all[cname][name]
    nct = load_trendy(p, name, cname, grids = grids).resample(time = '1YS').mean()

    coef_mat = xr.DataArray(
        deg2m(nct.longitude, nct.latitude, 0.5, 0.5),
        dims = ['latitude', 'longitude'],
        coords = {'longitude': nct.longitude, 'latitude': nct.latitude}
    ).expand_dims(time = nct.time)

    dft = (nct * 1000 * 86400 * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365
    df_rh_trendy_all.append(dft)

df_rh_trendy_all = pd.concat(df_rh_trendy_all, axis = 1)

# ------------------------------------------------------------------------------

df_reco_trendy_all = df_ra_trendy_all + df_rh_trendy_all
df_reco_trendy_all = df_reco_trendy_all.dropna(axis = 1, how = 'all')

import itertools

np.random.seed(42)

cname = 'GPP'
dfp = df_gpp_trendy_all.copy()
# cname = 'RECO'
# dfp = df_reco_trendy_all.copy()

markers = ['o', 's', 'D', 'x', '^', 'v', '>', '<', 'p', '*']  # 10 unique markers
line_styles = ['-', '--', '-.', ':']  # 4 different line styles

style_combinations = list(itertools.product(colors, markers, line_styles))
np.random.seed(42)
np.random.shuffle(style_combinations)  # Shuffle the styles randomly

fig, ax = setup_canvas(1, 1, figsize = (10, 6))
for i in range(len(dfp.columns)):
    model_name = dfp.columns[i]

    color, marker, line_style = style_combinations[i % len(style_combinations)]  # Get unique styling

    ax.plot(
        dfp.index, dfp[model_name],
        linestyle = line_style, marker = marker, color = color,
        label = model_name, markersize=6
    )

    upper_legend(ax, yloc = 1.5, ncols = 3)
ax.set_ylabel(cname + ' ($PgC \ yr^{-1}$)')

# google.download_file(fig, f'Trendy-23year-{cname}.png')

# Carbon pools are annual
# for cname in ['cLitter', 'cSoil', 'cVeg']:
for cname in ['cLeaf', 'cWood', 'cWoodTotal']:
    ctype = 'pool'
    savefile = root_proj_trendy.joinpath('2_preproc').joinpath(f'Trendy/TrendyV12_21models_{cname}_05deg_yearly_2000-2022.nc')
    if savefile.exists(): continue
    df_path_trendy = df_path_trendy_all[cname].dropna()
    ncc = []
    print(cname)
    for name in tqdm(df_path_trendy.index):
        p = df_path_trendy[name]
        nct = load_trendy(p, name, cname, grids = grids).resample(time = '1YS').mean()
        ncc.append(nct); del(nct)
    ncc = xr.merge(ncc)
    ncc.to_netcdf(savefile)
    del(ncc); del(savefile)

# Carbon fluxes are monthly
# for cname in ['nbp', 'ra', 'rh', 'gpp', 'npp']:
for cname in ['fGrazing']:
    ctype = 'flux'
    savefile = root_proj_trendy.joinpath('2_preproc').joinpath(f'Trendy/TrendyV12_21models_{cname}_05deg_monthly_2000-2022.nc')
    if savefile.exists(): continue
    df_path_trendy = df_path_trendy_all[cname].dropna()
    ncc = []
    print(cname)
    for name in tqdm(df_path_trendy.index):
        p = df_path_trendy[name]
        nct = load_trendy(p, name, cname, ctype, grids = grids)
        ncc.append(nct); del(nct)
    ncc = xr.merge(ncc)
    ncc.to_netcdf(savefile)
    del(ncc); del(savefile)

# Put cWoodtotal into cWood
p_woodtotal = root_proj_trendy.joinpath('2_preproc').joinpath(f'Trendy/TrendyV12_21models_cWoodTotal_05deg_yearly_2000-2022.nc')
if p_woodtotal.exists():
    nc_woodtotal = xr.open_dataset(p_woodtotal, engine = 'netcdf4')

    p_wood = root_proj_trendy.joinpath('2_preproc').joinpath(f'Trendy/TrendyV12_21models_cWood_05deg_yearly_2000-2022.nc')
    nc_wood = xr.open_dataset(p_wood, engine = 'netcdf4')
    nct = xr.merge([nc_woodtotal, nc_wood])
    p_wood.unlink()
    nct.to_netcdf(p_wood)
    del(nct); del(nc_woodtotal); del(nc_wood); p_woodtotal.unlink()

"""
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!! WRONG CODE nep = gpp - ra - rh !!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
"""

# Check consistency between npp and gpp - ra - rh
# for cname in ['nbp', 'ra', 'rh', 'gpp', 'npp']:
for cname in ['fGrazing']:
    ctype = 'flux'
    df_path_trendy = df_path_trendy_all[cname].dropna()
    ncc = []
    print(cname)
    for name in tqdm(df_path_trendy.index):
        p = df_path_trendy[name]
        nct = load_trendy(p, name, cname, ctype, grids = grids)
        ncc.append(nct); del(nct)
    ncc = xr.merge(ncc)
    del(ncc); del(savefile)

"""## Checking NPP_dir and NPP_ind = GPP - Ra"""

dfm_npp = []
for model_ in tqdm(df_path_trendy_all.index):
    cnames = df_path_trendy_all.loc[model_, :].dropna().index
    if ('gpp' in cnames) & ('npp' in cnames) & ('ra' in cnames) & ('rh' in cnames):
        ctype = 'flux'
        cname = 'npp'
        npp = load_trendy(df_path_trendy_all.loc[model_, cname], model_, cname, ctype, grids = grids).resample(time = '1YS').mean().mean(dim = ['longitude', 'latitude']).to_dataframe() * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1

        cname = 'gpp'
        gpp = load_trendy(df_path_trendy_all.loc[model_, cname], model_, cname, ctype, grids = grids).resample(time = '1YS').mean().mean(dim = ['longitude', 'latitude']).to_dataframe() * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1

        cname = 'ra'
        ra = load_trendy(df_path_trendy_all.loc[model_, cname], model_, cname, ctype, grids = grids).resample(time = '1YS').mean().mean(dim = ['longitude', 'latitude']).to_dataframe() * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1

        cname = 'rh'
        rh = load_trendy(df_path_trendy_all.loc[model_, cname], model_, cname, ctype, grids = grids).resample(time = '1YS').mean().mean(dim = ['longitude', 'latitude']).to_dataframe() * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1

        gpp.columns = ['gpp']
        npp.columns = ['npp']
        ra.columns = ['ra']
        rh.columns = ['rh']

        npp_ind = (gpp['gpp'] - ra['ra']).rename('npp_ind').to_frame()
        # NPP = GPP - Ra
        # NEP = GPP - Ra - Rh

        dfm_npp.append([model_, npp['npp'].mean(), npp_ind['npp_ind'].mean()])
    else:
        print(model_)
        dfm_npp.append([model_, np.nan, np.nan])
dfm_npp = pd.DataFrame(dfm_npp, columns = ['model', 'npp_dir', 'npp_ind']).set_index('model')
dfm_npp
# google.download_file(dfm_npp, 'Trendy-23year-npp-2methods.csv')

model_ = 'CARDAMOM'
cname = 'npp'
npp = load_trendy(df_path_trendy_all.loc[model_, cname], model_, cname, ctype, grids = grids)#.resample(time = '1YS').mean().mean(dim = ['longitude', 'latitude']).to_dataframe() * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1

cname = 'gpp'
gpp = load_trendy(df_path_trendy_all.loc[model_, cname], model_, cname, ctype, grids = grids)#.resample(time = '1YS').mean().mean(dim = ['longitude', 'latitude']).to_dataframe() * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1

cname = 'ra'
ra = load_trendy(df_path_trendy_all.loc[model_, cname], model_, cname, ctype, grids = grids)#.resample(time = '1YS').mean().mean(dim = ['longitude', 'latitude']).to_dataframe() * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1

cname = 'rh'
rh = load_trendy(df_path_trendy_all.loc[model_, cname], model_, cname, ctype, grids = grids)#.resample(time = '1YS').mean().mean(dim = ['longitude', 'latitude']).to_dataframe() * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1

root_proj_card = root.joinpath("workspace/project_data/CARDAMOM/Global")

def load_card_ed(df_path_card, loi):
    card = xr.open_dataset(df_path_card.loc[f'S3_{loi}', 'PATH'])
    df_card = card[loi].mean(dim = 'time').to_dataframe().dropna()
    card = card.assign_coords({
        "time": [pd.to_datetime('2003-01-01', format = '%Y-%m-%d') + pd.DateOffset(months = i) for i in card.time.data - 1]
    })
    return card

df_path_card = []
for p in root_proj_card.joinpath('CARDAMOM_Trendy12/').glob('*.nc'):
    _, sv = p.stem.split('CARDAMOM_')
    df_path_card.append([sv, p])

df_path_card = pd.DataFrame(df_path_card, columns = ['PROD', 'PATH']).set_index('PROD')
# df_path_card.index
npp = load_card_ed(df_path_card, 'nbp')['nbp'] * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
gpp = load_card_ed(df_path_card, 'gpp')['gpp'] * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
ra = load_card_ed(df_path_card, 'ra')['ra'] * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
rh = load_card_ed(df_path_card, 'rh')['rh'] * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1

npp_dir = npp.rename('npp_dir').resample(time = '1YS').mean().mean(dim = ['longitude', 'latitude']).to_dataframe()
npp_ind = (gpp - ra - rh).rename('npp_ind').resample(time = '1YS').mean().mean(dim = ['longitude', 'latitude']).to_dataframe()

pd.concat([npp_dir, npp_ind], axis = 1).plot()

"""## Calculate NEP"""

# actually 17 model NEP
savefile = root_proj_trendy.joinpath('2_preproc').joinpath(f'Trendy/TrendyV12_21models_nep_05deg_monthly_2000-2022.nc')

if not savefile.exists():
    p = root_proj_trendy.joinpath('2_preproc').joinpath(f'Trendy/TrendyV12_21models_npp_05deg_monthly_2000-2022.nc')
    npp = xr.open_dataset(p, engine = 'netcdf4')

    p = root_proj_trendy.joinpath('2_preproc').joinpath(f'Trendy/TrendyV12_21models_rh_05deg_monthly_2000-2022.nc')
    rh = xr.open_dataset(p, engine = 'netcdf4')

    nep = []
    for name in tqdm(list(set(list(npp.data_vars)) & set(list(rh.data_vars)))):
        nep.append(npp[name] - rh[name])
    nep = xr.merge(nep)
    nep.to_netcdf(savefile)
    del(nep)
del(savefile)

"""## Create mean, 25th, and 75th across models"""

for cname in tqdm(['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']):
    if cname in ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']:
        cfreq = 'yearly'
    elif cname in ['gpp', 'npp', 'nbp', 'ra', 'rh', 'fGrazing', 'nep']:
        cfreq = 'monthly'
    else:
        raise ValueError('Unknown cname')
        cfreq = None
    savefile = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_{cfreq}_2000-2022.nc')
    if savefile.exists(): continue
    ncc_in = xr.open_dataset(root_proj_trendy.joinpath('2_preproc/Trendy').joinpath(f'TrendyV12_21models_{cname}_05deg_{cfreq}_2000-2022.nc'),  engine="netcdf4")

    ncc = xr.merge([ncc_in[name].rename(cname).expand_dims(model = [name]) for name in list(ncc_in.keys())])
    ncc = xr.merge([
        ncc.mean(dim = 'model'),
        ncc.quantile(0.25, dim = 'model').drop_vars('quantile').rename({cname: cname + '_25th'}),
        ncc.quantile(0.75, dim = 'model').drop_vars('quantile').rename({cname: cname + '_75th'}),
    ])
    ncc.to_netcdf(savefile)
    del(ncc)

for cname in tqdm(['gpp', 'npp', 'nbp', 'ra', 'rh', 'fGrazing', 'nep']):
    if cname in ['cVeg', 'cLitter', 'cSoil', 'cLeaf']:
        cfreq = 'yearly'
    elif cname in ['gpp', 'npp', 'nbp', 'ra', 'rh', 'fGrazing', 'nep']:
        cfreq = 'monthly'
    else:
        raise ValueError('Unknown cname')
        cfreq = None
    savefile = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_{cfreq}_2000-2022.nc')
    if savefile.exists(): continue
    ncc_in = xr.open_dataset(root_proj_trendy.joinpath('2_preproc/Trendy').joinpath(f'TrendyV12_21models_{cname}_05deg_{cfreq}_2000-2022.nc'),  engine="netcdf4")

    ncc = []
    for yr in tqdm(np.unique(ncc_in['time.year'])):
        nct = ncc_in.where(ncc_in['time.year'] == yr, drop = True)

        nct = xr.merge([nct[name].rename(cname).expand_dims(model = [name]) for name in list(nct.keys())])
        nct = xr.merge([
            nct.mean(dim = 'model'),
            nct.quantile(0.25, dim = 'model').drop_vars('quantile').rename({cname: cname + '_25th'}),
            nct.quantile(0.75, dim = 'model').drop_vars('quantile').rename({cname: cname + '_75th'}),
        ])
        ncc.append(nct)
    ncc = xr.merge(ncc)

    ncc.to_netcdf(savefile)
    del(ncc)

# Save year by year and combine together
cname = 'nep'
cfreq = 'monthly'

savefile_years = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_{cfreq}_2000-2022.nc')
if not savefile_years.exists():

    ncc_in = xr.open_dataset(root_proj_trendy.joinpath('2_preproc/Trendy').joinpath(f'TrendyV12_21models_{cname}_05deg_{cfreq}_2000-2022.nc'),  engine="netcdf4")

    root_proj_trendy.joinpath('2_preproc/Trendy_model_averages_temp').mkdir(exist_ok = True)
    for yr in tqdm(np.unique(ncc_in['time.year'])):
        savefile = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages_temp').joinpath(f'TrendyV12_{cname}_05deg_{cfreq}_{yr}.nc')
        if savefile.exists(): continue
        nct = ncc_in.where(ncc_in['time.year'] == yr, drop = True)

        nct = xr.merge([nct[name].rename(cname).expand_dims(model = [name]) for name in list(nct.keys())])
        nct = xr.merge([
            nct.mean(dim = 'model'),
            nct.quantile(0.25, dim = 'model').drop_vars('quantile').rename({cname: cname + '_25th'}),
            nct.quantile(0.75, dim = 'model').drop_vars('quantile').rename({cname: cname + '_75th'}),
        ])

        nct.to_netcdf(savefile)
        del(nct); del(savefile)

    ncc = []
    for p in root_proj_trendy.joinpath('2_preproc/Trendy_model_averages_temp').glob(f'TrendyV12_{cname}_05deg_{cfreq}_*.nc'):
        ncc.append(xr.open_dataset(p, engine="netcdf4"))

    ncc = xr.merge(ncc)
    ncc.to_netcdf(savefile_years)
    del(ncc)

"""# Prepare training geospatial data

## 0.5deg
"""

df_path_ERA5 = []
for p in root_proj_trendy.joinpath('1_data/1_geospatial/ERA5').glob('*.tif'):
    dt = pd.to_datetime(p.stem.split('_')[-1])
    df_path_ERA5.append([dt, p])
df_path_ERA5 = pd.DataFrame(df_path_ERA5, columns = ['time', 'path'])
df_path_ERA5 = df_path_ERA5.set_index('time')
df_path_ERA5 = df_path_ERA5.sort_index()

df_path_ERA52 = []
for p in root_proj_trendy.joinpath('1_data/1_geospatial/ERA5_2').glob('*.tif'):
    dt = pd.to_datetime(p.stem.split('_')[-1])
    df_path_ERA52.append([dt, p])
df_path_ERA52 = pd.DataFrame(df_path_ERA52, columns = ['time', 'path'])
df_path_ERA52 = df_path_ERA52.set_index('time')
df_path_ERA52 = df_path_ERA52.sort_index()

df_path_ERA5 = pd.concat([df_path_ERA5, df_path_ERA52], axis = 1)
df_path_ERA5.columns = ['path1', 'path2']

root_proj_trendy.joinpath('2_preproc/ERA5').mkdir(exist_ok = True)

for yr, df_path_ERA5t in df_path_ERA5.groupby(df_path_ERA5.index.year):
    savefile = root_proj_trendy.joinpath('2_preproc/ERA5').joinpath(f'ERA5_05deg_monthly_{yr}.nc')
    if savefile.exists(): continue
    print(yr)
    era5 = []
    for p in tqdm(df_path_ERA5t['path1']):
        dt = pd.to_datetime(p.stem.split('_')[-1])
        era5t = rxr.open_rasterio(p, band_as_variable = True)
        name_dict = {key: era5t[key].attrs['long_name'].split('_mean')[0] for key in era5t.data_vars}
        name_dict.update({'x': 'longitude', 'y': 'latitude'})
        era5t = era5t.rename(name_dict)
        era5t = era5t.interp(latitude = grids.latitude, longitude = grids.longitude)
        # ----------------------------------------------------------------------
        p2 = df_path_ERA5t.loc[dt, 'path2']
        era5t2 = rxr.open_rasterio(p2, band_as_variable = True)
        name_dict = {key: era5t2[key].attrs['long_name'].split('_mean')[0] for key in era5t2.data_vars}
        name_dict.update({'x': 'longitude', 'y': 'latitude'})
        era5t2 = era5t2.rename(name_dict)
        era5t2 = era5t2.interp(latitude = grids.latitude, longitude = grids.longitude)
        era5t = xr.merge([era5t, era5t2])
        # ----------------------------------------------------------------------
        era5.append(era5t.expand_dims({'time': [dt]}))
        era5t.close(); del(era5t)
    era5 = xr.merge(era5)
    era5.to_netcdf(savefile)

df_path_MODIS = []
for p in root_proj_trendy.joinpath('1_data/1_geospatial/MODIS_SR').glob('*.tif'):
    dt = pd.to_datetime(p.stem.split('_')[-1])
    df_path_MODIS.append([dt, p])
df_path_MODIS = pd.DataFrame(df_path_MODIS, columns = ['time', 'path'])
df_path_MODIS = df_path_MODIS.set_index('time')
df_path_MODIS = df_path_MODIS.sort_index()

for yr, df_path_MODISt in df_path_MODIS.groupby(df_path_MODIS.index.year):
    savefile = root_proj_trendy.joinpath('2_preproc').joinpath(f'MODIS_SR/MODIS_05deg_monthly_{yr}.nc')
    if savefile.exists(): continue
    print(yr)
    modis = []
    for p in tqdm(df_path_MODISt['path']):
        dt = pd.to_datetime(p.stem.split('_')[-1])
        modist = rxr.open_rasterio(p, band_as_variable = True)
        name_dict = {key: modist[key].attrs['long_name'].split('_mean')[0] for key in modist.data_vars}
        name_dict.update({'x': 'longitude', 'y': 'latitude'})
        modist = modist.rename(name_dict)

        modist = modist.interp(latitude = grids.latitude, longitude = grids.longitude)
        modis.append(modist.expand_dims({'time': [dt]}))
        modist.close(); del(modist)
    modis = xr.merge(modis)
    modis.to_netcdf(savefile)

df_path_LUCC = []
for p in root_proj_trendy.joinpath('1_data/1_geospatial/MODIS_LC').glob('*.tif'):
    dt = pd.to_datetime(p.stem.split('_')[-1])
    df_path_LUCC.append([dt, p])
df_path_LUCC = pd.DataFrame(df_path_LUCC, columns = ['time', 'path'])
df_path_LUCC = df_path_LUCC.set_index('time')
df_path_LUCC = df_path_LUCC.sort_index()


savefile = root_proj_trendy.joinpath('2_preproc').joinpath(f'LUCC_05deg_2001-2023.nc')
if not savefile.exists():
    lucc = []
    for p in tqdm(df_path_LUCC['path']):
        dt = pd.to_datetime(p.stem.split('_')[-1])
        lucct = rxr.open_rasterio(p, band_as_variable = True)
        name_dict = {key: lucct[key].attrs['long_name'].split('_mean')[0] for key in lucct.data_vars}
        name_dict.update({'x': 'longitude', 'y': 'latitude'})
        lucct = lucct.rename(name_dict)

        lucct = lucct.interp(latitude = grids.latitude, longitude = grids.longitude)
        lucc.append(lucct.expand_dims({'time': [dt]}))
        lucct.close(); del(lucct)
    lucc = xr.merge(lucc)
    lucc.to_netcdf(savefile)

"""## 0.1 deg"""

scale = 0.1
lons = np.arange(-179.95, 179.95 + scale, scale)
lats = np.arange(-89.95, 89.95 + scale, scale)

lon_splits = np.array_split(lons, 2)
lat_splits = np.array_split(lats, 2)

subregions = {}
for i, lon_chunk in enumerate(lon_splits):
    for j, lat_chunk in enumerate(lat_splits):
        subregion_key = f"{i+1}_{j+1}"
        subregions[subregion_key] = xr.DataArray(
            data=np.zeros([len(lon_chunk), len(lat_chunk)]),
            dims=["longitude", "latitude"],
            coords=dict(
                longitude=(["longitude"], lon_chunk),
                latitude=(["latitude"], lat_chunk),
            ),
            attrs=dict(description=f"Subregion {subregion_key} of {scale} grids."),
        )
print(list(subregions.keys()))

# df_path_ERA5 = []
# for p in root_proj_trendy.joinpath('1_data/1_geospatial/ERA5').glob('*.tif'):
#     dt = pd.to_datetime(p.stem.split('_')[-1])
#     df_path_ERA5.append([dt, p])
# df_path_ERA5 = pd.DataFrame(df_path_ERA5, columns = ['time', 'path'])
# df_path_ERA5 = df_path_ERA5.set_index('time')
# df_path_ERA5 = df_path_ERA5.sort_index()

# df_path_ERA52 = []
# for p in root_proj_trendy.joinpath('1_data/1_geospatial/ERA5_2').glob('*.tif'):
#     dt = pd.to_datetime(p.stem.split('_')[-1])
#     df_path_ERA52.append([dt, p])
# df_path_ERA52 = pd.DataFrame(df_path_ERA52, columns = ['time', 'path'])
# df_path_ERA52 = df_path_ERA52.set_index('time')
# df_path_ERA52 = df_path_ERA52.sort_index()

# df_path_ERA5 = pd.concat([df_path_ERA5, df_path_ERA52], axis = 1)
# df_path_ERA5.columns = ['path1', 'path2']

# root_proj_trendy.joinpath('2_preproc/ERA5_01deg').mkdir(exist_ok = True)

# for yr, df_path_ERA5t in df_path_ERA5.groupby(df_path_ERA5.index.year):
#     savefile = root_proj_trendy.joinpath('2_preproc/ERA5_01deg').joinpath(f'ERA5_01deg_monthly_{yr}.nc')
#     if savefile.exists(): continue
#     print(yr)
#     era5 = []
#     for p in tqdm(df_path_ERA5t['path1']):
#         dt = pd.to_datetime(p.stem.split('_')[-1])
#         era5t = rxr.open_rasterio(p, band_as_variable = True)
#         name_dict = {key: era5t[key].attrs['long_name'].split('_mean')[0] for key in era5t.data_vars}
#         name_dict.update({'x': 'longitude', 'y': 'latitude'})
#         era5t = era5t.rename(name_dict)
#         era5t = era5t.drop_vars([
#             'snow_cover', 'forecast_albedo', 'surface_latent_heat_flux_sum', 'surface_pressure',
#             'surface_sensible_heat_flux_sum', 'evaporation_from_the_top_of_canopy_sum',
#             'temperature_2m_min', 'temperature_2m_max', 'u_component_of_wind_10m', 'v_component_of_wind_10m'
#         ])
#         era5t['dewpoint_temperature_2m'] -= 273.15
#         era5t['temperature_2m'] -= 273.15
#         era5t['soil_temperature_level_1'] -= 273.15

#         era5t['surface_solar_radiation_downwards_sum'] /= 86400
#         era5t['surface_thermal_radiation_downwards_sum'] /= 86400
#         # era5t['surface_pressure'] /= 100
#         era5t['VPD'] = saturation_vapor_pressure(era5t['temperature_2m']) - saturation_vapor_pressure(era5t['dewpoint_temperature_2m'])
#         era5t = era5t.drop_vars('dewpoint_temperature_2m')
#         # ----------------------------------------------------------------------
#         p2 = df_path_ERA5t.loc[dt, 'path2']
#         era5t2 = rxr.open_rasterio(p2, band_as_variable = True)
#         name_dict = {key: era5t2[key].attrs['long_name'].split('_mean')[0] for key in era5t2.data_vars}
#         name_dict.update({'x': 'longitude', 'y': 'latitude'})
#         era5t2 = era5t2.rename(name_dict)
#         era5t2['surface_net_solar_radiation_sum'] /= 86400
#         era5t2['surface_net_thermal_radiation_sum'] /= 86400
#         era5t2['surface_net_radiation_sum'] = era5t2['surface_net_solar_radiation_sum'] + era5t2['surface_net_thermal_radiation_sum']
#         del(era5t2['surface_net_solar_radiation_sum'])
#         del(era5t2['surface_net_thermal_radiation_sum'])
#         era5t = xr.merge([era5t, era5t2]); del(era5t2)
#         era5t = era5t.interp(latitude = grids_01deg.latitude, longitude = grids_01deg.longitude)
#         era5t = era5t.where((era5t.latitude > -55) & (era5t.latitude < 85), drop = True)
#         # ----------------------------------------------------------------------
#         era5.append(era5t.expand_dims({'time': [dt]}))
#         era5t.close(); del(era5t)
#         gc.collect()
#     era5 = xr.merge(era5)
#     era5.to_netcdf(savefile)
#     del(era5); gc.collect(); time.sleep(0.1)



root_proj_trendy.joinpath('2_preproc/ERA5_01deg').mkdir(exist_ok = True)

for sub_k in subregions.keys():
    if sub_k not in ['2_2']: continue
    sub_grids = subregions[sub_k]
    df_path_ERA5 = []
    for p in root_proj_trendy.joinpath('1_data/1_geospatial/ERA5').glob('*.tif'):
        dt = pd.to_datetime(p.stem.split('_')[-1])
        df_path_ERA5.append([dt, p])
    df_path_ERA5 = pd.DataFrame(df_path_ERA5, columns = ['time', 'path'])
    df_path_ERA5 = df_path_ERA5.set_index('time')
    df_path_ERA5 = df_path_ERA5.sort_index()

    df_path_ERA52 = []
    for p in root_proj_trendy.joinpath('1_data/1_geospatial/ERA5_2').glob('*.tif'):
        dt = pd.to_datetime(p.stem.split('_')[-1])
        df_path_ERA52.append([dt, p])
    df_path_ERA52 = pd.DataFrame(df_path_ERA52, columns = ['time', 'path'])
    df_path_ERA52 = df_path_ERA52.set_index('time')
    df_path_ERA52 = df_path_ERA52.sort_index()

    df_path_ERA5 = pd.concat([df_path_ERA5, df_path_ERA52], axis = 1)
    df_path_ERA5.columns = ['path1', 'path2']

    for yr, df_path_ERA5t in df_path_ERA5.groupby(df_path_ERA5.index.year):
        savefile = root_proj_trendy.joinpath('2_preproc/ERA5_01deg').joinpath(f'ERA5_01deg_monthly_{yr}_{sub_k}.nc')
        if savefile.exists(): continue
        print(yr)
        era5 = []
        for p in tqdm(df_path_ERA5t['path1']):
            dt = pd.to_datetime(p.stem.split('_')[-1])
            era5t = rxr.open_rasterio(p, band_as_variable = True)
            name_dict = {key: era5t[key].attrs['long_name'].split('_mean')[0] for key in era5t.data_vars}
            name_dict.update({'x': 'longitude', 'y': 'latitude'})
            era5t = era5t.rename(name_dict)
            era5t = era5t.interp(latitude = sub_grids.latitude, longitude = sub_grids.longitude)
            era5t = era5t.drop_vars([
                'snow_cover', 'forecast_albedo', 'surface_latent_heat_flux_sum',
                'surface_sensible_heat_flux_sum', 'evaporation_from_the_top_of_canopy_sum',
                'temperature_2m_min', 'temperature_2m_max', 'u_component_of_wind_10m', 'v_component_of_wind_10m'
            ])
            era5t['dewpoint_temperature_2m'] -= 273.15
            era5t['temperature_2m'] -= 273.15
            era5t['soil_temperature_level_1'] -= 273.15

            era5t['surface_solar_radiation_downwards_sum'] /= 86400
            era5t['surface_thermal_radiation_downwards_sum'] /= 86400
            # era5t['surface_pressure'] /= 100
            era5t['VPD'] = saturation_vapor_pressure(era5t['temperature_2m']) - saturation_vapor_pressure(era5t['dewpoint_temperature_2m'])
            era5t = era5t.drop_vars('dewpoint_temperature_2m')
            # ----------------------------------------------------------------------
            p2 = df_path_ERA5t.loc[dt, 'path2']
            era5t2 = rxr.open_rasterio(p2, band_as_variable = True)
            name_dict = {key: era5t2[key].attrs['long_name'].split('_mean')[0] for key in era5t2.data_vars}
            name_dict.update({'x': 'longitude', 'y': 'latitude'})
            era5t2 = era5t2.rename(name_dict)
            era5t2 = era5t2.interp(latitude = sub_grids.latitude, longitude = sub_grids.longitude)
            era5t2['surface_net_solar_radiation_sum'] /= 86400
            era5t2['surface_net_thermal_radiation_sum'] /= 86400
            era5t2['surface_net_radiation_sum'] = era5t2['surface_net_solar_radiation_sum'] + era5t2['surface_net_thermal_radiation_sum']
            del(era5t2['surface_net_solar_radiation_sum'])
            del(era5t2['surface_net_thermal_radiation_sum'])
            era5t = xr.merge([era5t, era5t2]); del(era5t2)
            # era5t = era5t.interp(latitude = grids_01deg.latitude, longitude = grids_01deg.longitude)
            era5t = era5t.where((era5t.latitude > -55) & (era5t.latitude < 85), drop = True)
            # ----------------------------------------------------------------------
            era5.append(era5t.expand_dims({'time': [dt]}))
            era5t.close(); del(era5t)
        era5 = xr.merge(era5)
        era5.to_netcdf(savefile)
        del(era5); gc.collect(); time.sleep(0.1)

root_proj_trendy.joinpath('2_preproc/MODIS_SR_01deg').mkdir(exist_ok = True)

df_path_MODIS = []
for p in root_proj_trendy.joinpath('1_data/1_geospatial/MODIS_SR').glob('*.tif'):
    dt = pd.to_datetime(p.stem.split('_')[-1])
    df_path_MODIS.append([dt, p])
df_path_MODIS = pd.DataFrame(df_path_MODIS, columns = ['time', 'path'])
df_path_MODIS = df_path_MODIS.set_index('time')
df_path_MODIS = df_path_MODIS.sort_index()

for yr, df_path_MODISt in df_path_MODIS.groupby(df_path_MODIS.index.year):
    savefile = root_proj_trendy.joinpath('2_preproc').joinpath(f'MODIS_SR_01deg/MODIS_05deg_monthly_{yr}.nc')
    if savefile.exists(): continue
    print(yr)
    modis = []
    for p in tqdm(df_path_MODISt['path']):
        dt = pd.to_datetime(p.stem.split('_')[-1])
        modist = rxr.open_rasterio(p, band_as_variable = True)
        name_dict = {key: modist[key].attrs['long_name'].split('_mean')[0] for key in modist.data_vars}
        name_dict.update({'x': 'longitude', 'y': 'latitude'})
        modist = modist.rename(name_dict)

        modist = modist.interp(latitude = grids_01deg.latitude, longitude = grids_01deg.longitude)
        modis.append(modist.expand_dims({'time': [dt]}))
        modist.close(); del(modist)
    modis = xr.merge(modis)
    modis.to_netcdf(savefile)

df_path_LUCC = []
for p in root_proj_trendy.joinpath('1_data/1_geospatial/MODIS_LC').glob('*.tif'):
    dt = pd.to_datetime(p.stem.split('_')[-1])
    df_path_LUCC.append([dt, p])
df_path_LUCC = pd.DataFrame(df_path_LUCC, columns = ['time', 'path'])
df_path_LUCC = df_path_LUCC.set_index('time')
df_path_LUCC = df_path_LUCC.sort_index()


savefile = root_proj_trendy.joinpath('2_preproc').joinpath(f'LUCC_01deg_2001-2023.nc')
if not savefile.exists():
    lucc = []
    for p in tqdm(df_path_LUCC['path']):
        dt = pd.to_datetime(p.stem.split('_')[-1])
        lucct = rxr.open_rasterio(p, band_as_variable = True)
        name_dict = {key: lucct[key].attrs['long_name'].split('_mean')[0] for key in lucct.data_vars}
        name_dict.update({'x': 'longitude', 'y': 'latitude'})
        lucct = lucct.rename(name_dict)

        lucct = lucct.interp(latitude = grids_01deg.latitude, longitude = grids_01deg.longitude)
        lucc.append(lucct.expand_dims({'time': [dt]}))
        lucct.close(); del(lucct)
    lucc = xr.merge(lucc)
    lucc.to_netcdf(savefile)

"""# Common things

## 0.5 deg
"""

savefile_lucc = root_proj_trendy.joinpath('2_preproc').joinpath(f'LUCC_05deg_2001-2023.nc')
if savefile_lucc.exists():
    luccr = xr.open_dataset(savefile_lucc,  engine="netcdf4")

savefile_geo = root.joinpath("workspace/project_data/platform").joinpath('1grid_inputs/geo_info.nc')
if savefile_geo.exists():
    nc_geor = xr.open_dataset(savefile_geo,  engine="netcdf4")

savefile_co2 = root_proj_trendy.joinpath('2_preproc/co2_info.nc')
if savefile_co2.exists():
    co2r = xr.open_dataset(savefile_co2,  engine="netcdf4")
else:
    root_ct = root.joinpath("workspace/project_data/LUCC").joinpath("carbontracker_daily")
    if savefile_co2.exists():
        co2r = xr.open_dataset(savefile_co2)
    else:
        root_ct = root.joinpath("workspace/project_data/LUCC").joinpath("carbontracker")
        co2r = []
        for p in root_ct.glob('*.nc'):
            co2t = xr.open_dataset(p)
            co2r.append(co2t)
        co2r = xr.merge(co2r)

        co2r = co2r.interp(latitude = grids.latitude, longitude = grids.longitude)
        co2r.to_netcdf(savefile_co2)

df_path_era5 = []
for p in root_proj_trendy.joinpath('2_preproc/ERA5').glob(f'ERA5*.nc'):
    df_path_era5.append([pd.to_datetime(p.stem.split('_')[-1], format = '%Y'), p])
df_path_era5 = pd.DataFrame(df_path_era5, columns = ['time', 'path']).set_index('time')

df_path_MODIS = []
for p in root_proj_trendy.joinpath('2_preproc/MODIS_SR').glob(f'MODIS*.nc'):
    df_path_MODIS.append([pd.to_datetime(p.stem.split('_')[-1], format = '%Y'), p])
df_path_MODIS = pd.DataFrame(df_path_MODIS, columns = ['time', 'path']).set_index('time')

"""## 0.1 deg"""

savefile_lucc = root_proj_trendy.joinpath('2_preproc').joinpath(f'LUCC_01deg_2001-2023.nc')
if savefile_lucc.exists():
    luccr = xr.open_dataset(savefile_lucc,  engine="netcdf4")

savefile_geo = root.joinpath("workspace/project_data/platform").joinpath('1grid_inputs/geo_info.nc')
if savefile_geo.exists():
    nc_geor = xr.open_dataset(savefile_geo,  engine="netcdf4")

savefile_co2 = root_proj_trendy.joinpath('2_preproc/co2_info.nc')
if savefile_co2.exists():
    co2r = xr.open_dataset(savefile_co2,  engine="netcdf4")
else:
    root_ct = root.joinpath("workspace/project_data/LUCC").joinpath("carbontracker_daily")
    if savefile_co2.exists():
        co2r = xr.open_dataset(savefile_co2)
    else:
        root_ct = root.joinpath("workspace/project_data/LUCC").joinpath("carbontracker")
        co2r = []
        for p in root_ct.glob('*.nc'):
            co2t = xr.open_dataset(p)
            co2r.append(co2t)
        co2r = xr.merge(co2r)

        co2r = co2r.interp(latitude = grids.latitude, longitude = grids.longitude)
        co2r.to_netcdf(savefile_co2)

df_path_era5 = []
for p in root_proj_trendy.joinpath('2_preproc/ERA5_01deg').glob(f'ERA5*.nc'):
    sub_k = f"{p.stem.split('_')[-2]}_{p.stem.split('_')[-1]}"
    df_path_era5.append([pd.to_datetime(p.stem.split('_')[-3], format = '%Y'), sub_k, p])
df_path_era5 = pd.DataFrame(df_path_era5, columns = ['time', 'sub_k', 'path']).set_index('time')

df_path_MODIS = []
for p in root_proj_trendy.joinpath('2_preproc/MODIS_SR_01deg').glob(f'MODIS*.nc'):
    df_path_MODIS.append([pd.to_datetime(p.stem.split('_')[-1], format = '%Y'), p])
df_path_MODIS = pd.DataFrame(df_path_MODIS, columns = ['time', 'path']).set_index('time')

"""# UFLUXv1

## Functions
"""

def get_4VIs(data, r_name, nir_name, as_one = True):
    r = data[r_name]
    nir = data[nir_name]
    ndvi = get_NDVI(r, nir)
    evi = get_EVI2band(r, nir)
    nirv = get_NIRv(ndvi, nir)
    kndvi = get_kNDVI(ndvi)

    if as_one:
        ndvi.name = 'NDVI'
        evi.name = 'EVI'
        nirv.name = 'NIRv'
        kndvi.name = 'kNDVI'
        ndvi = ndvi.to_dataset()
        evi = evi.to_dataset()
        nirv = nirv.to_dataset()
        kndvi = kndvi.to_dataset()
        return xr.merge([ndvi, evi, nirv, kndvi])
    else:
        return (ndvi, evi, nirv, kndvi)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

def run_ml(X_train, X_test, y_train, y_test):
    xgb_params = {
        "objective": "reg:squarederror",
        "random_state": 0,
        'seed': 0,
    }

    regr = XGBRegressor(**xgb_params)
    regr.fit(X_train, y_train)
    y_pred = regr.predict(X_test)
    # print(regr.score(X_train, y_train))
    # print(regr.score(X_test, y_test))

    dfo = y_test.copy()#.to_frame()
    dfo.columns = ['truth']
    dfo['pred'] = y_pred
    # dfo = dfo / np.power(10, vexp)
    # dfo.index = dfo.index.droplevel([0, 1])

    # eval_res = regress2(dfo['truth'].values, dfo['pred'].values)
    # rvalue = eval_res['r']
    # slope = eval_res['slope']
    # intercept = eval_res['intercept']
    eval_res = pipelines.get_metrics(dfo, truth = 'truth', pred = 'pred', return_dict = True)
    # print(eval_res)
    return regr, dfo, eval_res

def agg_global_GPP_025deg(nct, world, name = 'GPP'):
    coef = 365 * 0.25 * 0.25 * 1e5 * 1e5 / 1e15
    nct = nct.where(nct[name] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
    dft = nct[name].sum(dim = ['latitude', 'longitude']).drop_vars('spatial_ref').to_dataframe() * coef
    return dft

def load_GPP(name, foldername, world):
    dfo = []
    for p in foldername.glob(f'{name}*.nc'):
        print(p.stem)
        nct = xr.open_dataset(p)
        dft = agg_global_GPP_025deg(nct, world)
        dfo.append(dft)
        nct.close(); del(nct)
    dfo = pd.concat(dfo, axis = 0)
    return dfo

def get_grid_NIRv(p, FAPAR_ok = False, reproj_ok = False):
    rnc = rxr.open_rasterio(p, band_as_variable = True)
    if reproj_ok:
        rnc = rnc.rio.reproject("EPSG:4326")
    assert (rnc.rio.crs == 'EPSG:4326'), rnc.rio.crs
    name_dict = dict(zip(rnc.keys(), ['R', 'NIR']))
    name_dict.update({'x': 'longitude', 'y': 'latitude'})
    rnc = rnc.rename(name_dict)
    R = rnc['R']
    NIR = rnc['NIR']
    if np.nanmean(NIR.data) > 100:
        NIR = NIR / 10000
        R = R / 10000
    NIRv = (NIR - R) / (NIR + R) * NIR
    NIRv.name = 'NIRv'
    if FAPAR_ok:
        EVI = 2.5 * (NIR - R) / (NIR + 2.4 * R + 1)
        FAPAR = (EVI - 0.1 + 0.1) * 1.25
        FAPAR.name = 'FAPAR'
        return NIRv, FAPAR
    else:
        return NIRv

def get_Amazon_patch(nct, minx = -35.2, miny = -9, maxx = -35, maxy = -6):
    cond = (nct.longitude > minx) & (nct.longitude < maxx) & (nct.latitude > miny) & (nct.latitude < maxy)
    return ~cond

"""## Train"""

meta = pd.read_csv((r'https://github.com/soonyenju/scieco/blob/main/scieco/data/fluxnet_meta_212.csv?raw=true'),index_col = 0)

nc_ec = xr.open_dataset(root_proj_platform.joinpath('FLUXNET2015_DD.nc'))

nc_era5 = xr.open_dataset(root_proj_platform.joinpath('0tower_level/ERA5-Land-daily-var19.nc'), engine = 'netcdf4')
df_era5 = nc_era5.to_dataframe()
df_era5['dewpoint_temperature_2m'] -= 273.15
df_era5['temperature_2m'] -= 273.15
df_era5['soil_temperature_level_1'] -= 273.15

df_era5['surface_latent_heat_flux_sum'] /= 86400
df_era5['surface_sensible_heat_flux_sum'] /= 86400
df_era5['surface_solar_radiation_downwards_sum'] /= 86400
df_era5['surface_thermal_radiation_downwards_sum'] /= 86400
df_era5['surface_net_solar_radiation_sum'] /= 86400
df_era5['surface_net_thermal_radiation_sum'] /= 86400

df_era5['surface_pressure'] /= 100

df_era5['temperature_2m_min'] -= 273.15
df_era5['temperature_2m_max'] -= 273.15

df_era5['VPD'] = saturation_vapor_pressure(df_era5['temperature_2m']) - saturation_vapor_pressure(df_era5['dewpoint_temperature_2m'])
df_era5['surface_net_radiation_sum'] = df_era5['surface_net_solar_radiation_sum'] + df_era5['surface_net_thermal_radiation_sum']

# era5r = df_era5[[
#     'temperature_2m', 'soil_temperature_level_1',
#     'snow_cover', 'volumetric_soil_water_layer_1', 'forecast_albedo',
#     'surface_latent_heat_flux_sum', 'surface_sensible_heat_flux_sum',
#     'surface_solar_radiation_downwards_sum',
#     'surface_thermal_radiation_downwards_sum',
#     'surface_net_solar_radiation_sum',
#     'surface_net_thermal_radiation_sum',
#     'evaporation_from_the_top_of_canopy_sum', 'surface_pressure',
#     'total_precipitation_sum', 'temperature_2m_min', 'temperature_2m_max',
#     'u_component_of_wind_10m', 'v_component_of_wind_10m', 'VPD'
# ]].to_xarray()

era5r = df_era5[[
    'temperature_2m', 'soil_temperature_level_1',
    'volumetric_soil_water_layer_1',
    'surface_solar_radiation_downwards_sum',
    'surface_thermal_radiation_downwards_sum',
    'surface_net_radiation_sum',
    'surface_pressure',
    'total_precipitation_sum', 'VPD'
]].to_xarray()

root_proj_lucc = root.joinpath("workspace/project_data/LUCC")
df_c4 = pd.read_csv(root_proj_lucc.joinpath(f'0Training/c4map_tower.csv'), index_col = 0)
df_co2 = load_pickle(root_proj_lucc.joinpath(f'0Training/CO2_tower.pkl'))
df_koppen = pd.read_csv(root_proj_lucc.joinpath(f'0Training/KOPPEN_tower.csv'), index_col = 0)
df_dem = pd.read_csv(root_proj_lucc.joinpath(f'0Training/DEM_tower.csv'), index_col = 0)
df_fpar = pd.read_csv(root_proj_lucc.joinpath('0Training/MODIS_Fpar_tower.csv'), index_col = 0)
df_fpar.index = pd.to_datetime(df_fpar.index, format = '%Y-%m-%d')
df_fpar = df_fpar.resample('1MS').mean()

# Load reflectance

nc_sat_high = xr.open_dataset(root_proj_platform.joinpath('0tower_level/Satellite_surface_reflectance_NIR_R_high-resolution.nc'))

# # ----------------------------------------------------------------------------
# Calibrate Landsat

# Harmonise Landsat satellites
# https://developers.google.com/earth-engine/tutorials/community/landsat-etm-to-oli-harmonization
# https://openprairie.sdstate.edu/cgi/viewcontent.cgi?referer=https://scholar.google.com/&httpsredir=1&article=1035&context=gsce_pubs

# Red: OLI = 0.0061 + 0.9047 ETM+
# NIR: OLI = 0.0412 + 0.8462 ETM+

dft = nc_sat_high.to_dataframe()
index = dft.index.get_level_values(2)
dft.loc[index == 'Landsat5', 'R'] = dft.loc[index == 'Landsat5', 'R'] * 0.9047 + 0.0061
dft.loc[index == 'Landsat5', 'NIR'] = dft.loc[index == 'Landsat5', 'NIR'] * 0.8462 + 0.0412
dft.loc[index == 'Landsat7', 'R'] = dft.loc[index == 'Landsat7', 'R'] * 0.9047 + 0.0061
dft.loc[index == 'Landsat7', 'NIR'] = dft.loc[index == 'Landsat7', 'NIR'] * 0.8462 + 0.0412
nc_sat_high = dft.to_xarray()

# # ----------------------------------------------------------------------------
# Calculate VI

# ndvi_high, evi_high, nirv_high, kndvi_high = get_4VIs(nc_sat_high, 'R', 'NIR', as_one = False)
vi_high = get_4VIs(nc_sat_high, 'R', 'NIR')

era5 = deepcopy(era5r)
nc_ec_mon = nc_ec.resample(time = '1MS').mean()
vi_mon_high = vi_high.resample(time = '1MS').mean()
era5_mon = era5.resample(time = '1MS').mean()

nc_ec_mon = nc_ec_mon.where((nc_ec_mon['time.year'] > 1999) & (nc_ec_mon['time.year'] < 2015), drop = True)
vi_mon_high = vi_mon_high.where((vi_mon_high['time.year'] > 1999) & (vi_mon_high['time.year'] < 2015), drop = True)
era5_mon = era5_mon.where((era5_mon['time.year'] > 1999) & (era5_mon['time.year'] < 2015), drop = True)

df_ml = []
for site in nc_ec.ID.data:
    df_ec = nc_ec_mon.sel(ID = site).drop_vars('ID').to_dataframe()

    df_ec = df_ec[df_ec['GPP_NT_VUT_REF'] >= 0]
    df_ec = df_ec[(df_ec['GPP_NT_VUT_REF'] - df_ec['GPP_DT_VUT_REF']).abs() <= 3]

    df_ec = df_ec[df_ec['RECO_NT_VUT_REF'] >= 0]
    df_ec = df_ec[(df_ec['RECO_NT_VUT_REF'] - df_ec['RECO_DT_VUT_REF']).abs() <= 3]

    df_vi_high = []
    for vi_name in ['NDVI', 'EVI', 'NIRv', 'kNDVI']:
        # dft = vi_mon_high.sel(ID = site).to_dataframe()[vi_name].rename(vi_name + '_high').reset_index().pivot(index = 'time', columns = 'satellite')
        dft = vi_mon_high.sel(ID = site).to_dataframe()[vi_name].reset_index().pivot(index = 'time', columns = 'satellite')
        dft.columns = dft.columns.map('_'.join).str.strip('_')
        df_vi_high.append(dft)
    df_vi_high = pd.concat(df_vi_high, axis = 1)

    df_era5 = era5_mon.sel(ID = site).drop_vars('ID').to_dataframe()

    dft = pd.concat([df_ec, df_vi_high, df_era5], axis = 1)

    dft = pd.concat([dft, df_co2[df_co2.index.get_level_values(1) == site].droplevel('ID')], axis = 1)
    dft = pd.concat([dft, df_fpar[site].rename('FAPAR_MODIS_PROD')], axis = 1)

    # dft_cold = dft[dft['temperature_2m'] <= -30]
    # dft = dft[dft['temperature_2m'] > -30]
    dft.loc[dft['temperature_2m'] < -30, 'temperature_2m'] = -30

    tc = dft['temperature_2m'].values
    co2 = dft['co2'].values
    patm = dft['surface_pressure'].values * 100
    vpd = dft['VPD'].values * 100
    do_ftemp_kphio = True

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = True, limitation_factors = 'none')
    # ----------------------------------------------------------------------------------------------------------------------------
    dft['C3_LUE'] = c3_lue
    dft['IWUE'] = iwue
    dft['C4_LUE'] = c4_lue
    # dft = pd.concat([dft, dft_cold], axis = 0)
    dft['C4_RATIO'] = df_c4.loc[site, 'C4_area'] # df_c4 has no NaN, min is 0
    dft['KOPPEN'] = df_koppen.loc[site, 'KOPPEN']
    dft['DEM'] = df_dem.loc[site, 'DEM']
    dft['ID'] = site

    dft['YEAR'] = dft.index.year
    dft['MONTH'] = dft.index.month
    dft['LAT'] = meta.loc[site, 'LAT']
    dft['LON'] = meta.loc[site, 'LON']

    df_ml.append(dft)

df_ml = pd.concat(df_ml, axis = 0)
for c in [c for c in df_ml.columns if c.startswith('EVI')]:
    df_ml[c.replace('EVI_', 'FAPAR_')] = (df_ml[c] - 0.1) * 1.25

df_ml['IGBP'] = [MODIS_IGBP_dict[igbp] for igbp in meta.loc[df_ml['ID'], 'IGBP'].values]
df_ml['El-Nino-La-Nina'] = df_el.loc[df_ml.index.year, 'El-Nino-La-Nina'].values
# df_ml = df_ml.reset_index()
df_mlr = df_ml.copy()

df_ml = df_mlr.copy()
# df_ml = df_ml[df_ml['NEE_VUT_REF_QC'] < 0.8]
# df_ml = df_ml[(df_ml['GPP_NT_VUT_REF'] - df_ml['GPP_DT_VUT_REF']) < 3]

root_proj_trendy.joinpath(f'3_output/UFLUXv1_model').mkdir(exist_ok = True, parents = True)

savefile = root_proj_trendy.joinpath(f'3_output/UFLUXv1_model/UFLUXv1.pkl')
dfi = df_ml.copy()
dfi.index.name = 'time'
dfi = dfi.reset_index().set_index(['ID', 'time'])

X_names = [
    'temperature_2m', 'surface_solar_radiation_downwards_sum','VPD',
    'soil_temperature_level_1', 'surface_thermal_radiation_downwards_sum', 'total_precipitation_sum',
    'surface_net_radiation_sum',
    # 'temperature_2m_min', 'temperature_2m_max',
    # 'u_component_of_wind_10m', 'v_component_of_wind_10m', 'volumetric_soil_water_layer_1',
    # 'co2', 'C3_LUE', 'IWUE', 'C4_LUE', 'C4_RATIO',
    'KOPPEN', 'DEM', 'IGBP', 'El-Nino-La-Nina', 'IWUE',
    'MONTH',
    # 'MONTH', 'LAT', 'LON',
    # 'YEAR', 'MONTH', 'LAT', 'LON'
]

y_names = {
    'GPP': ['GPP_NT_VUT_REF'],
    'RECO': ['RECO_NT_VUT_REF'],
    'NEE': ['NEE_VUT_REF'],
    'LE': ['LE_F_MDS']
}

df_res = []; dfm = []; dd_out = {}

for flux in ['GPP', 'RECO', 'NEE', 'LE']:
    y_name = y_names[flux][0]
    vi_name = 'NIRv_MODIS'
    evi_name = vi_name.replace('NIRv', 'EVI')
    dft = dfi.loc[dfi[[vi_name, evi_name] + X_names + [y_name]].dropna().index, :]
    dft = dft[dft[vi_name] > 1e-9]

    dft = dft[X_names + [y_name] + [vi_name]].dropna()

    Xs = dft[X_names + [vi_name]].rename(columns = {vi_name: 'NIRv'})
    ys = dft[[y_name]]
    # # --------------------------------------------------------------------------
    # # method 1:
    # X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.33, random_state=42)
    # --------------------------------------------------------------------------
    # method 2:
    test_index = dfi[dfi['NIRv_MODIS'] > 1e-9]['NIRv_MODIS'].sample(frac = 0.33, random_state = 42).index.intersection(dft.index)
    train_index = dfi[dfi['NIRv_MODIS'] > 1e-9]['NIRv_MODIS'].index.difference(test_index).intersection(dft.index)
    X_train = Xs.loc[train_index]
    X_test = Xs.loc[test_index]
    y_train = ys.loc[train_index]
    y_test = ys.loc[test_index]
    # --------------------------------------------------------------------------

    regr, dfo, res = run_ml(X_train, X_test, y_train, y_test)
    res.index = [flux]
    df_res.append(res)
    dfo['IGBP'] = meta.loc[dfo.index.get_level_values('ID'), 'IGBP'].values
    dfo[y_name] = dfi.loc[dfo.index, y_name]

    df_feat_imp = pd.DataFrame(regr.feature_importances_, columns = [flux], index = X_names + ['NIRv']).sort_values(by = flux, ascending = False)
    dd_ot = {
        'regr': regr,
        'X_test': X_test,
        'X_names': X_names,
        'df_feature_importance': df_feat_imp,
        'dfo': dfo
    }
    dd_out[flux] = dd_ot
df_res = pd.concat(df_res, axis = 0)
dd_out['df_res'] = df_res
dump_pickle(dd_out, savefile)
print(df_res)

from sciml.metrics import get_r2
from sciml.models import SmartForest4D

dfi = df_ml.copy()
dfi.index.name = 'time'
dfi = dfi.reset_index().set_index(['ID', 'time'])

X_names = [
    'temperature_2m', 'surface_solar_radiation_downwards_sum','VPD',
    'soil_temperature_level_1', 'surface_thermal_radiation_downwards_sum', 'total_precipitation_sum',
    'surface_net_radiation_sum',
    # 'temperature_2m_min', 'temperature_2m_max',
    # 'u_component_of_wind_10m', 'v_component_of_wind_10m', 'volumetric_soil_water_layer_1',
    # 'co2', 'C3_LUE', 'IWUE', 'C4_LUE', 'C4_RATIO',
    'KOPPEN', 'DEM', 'IGBP', 'El-Nino-La-Nina', 'IWUE',
    'MONTH',
    # 'MONTH', 'LAT', 'LON',
    # 'YEAR', 'MONTH', 'LAT', 'LON'
]

y_names = {
    'GPP': ['GPP_NT_VUT_REF'],
    'RECO': ['RECO_NT_VUT_REF'],
    'NEE': ['NEE_VUT_REF'],
    'LE': ['LE_F_MDS']
}

flux  = 'GPP'
y_name = y_names[flux][0]
vi_name = 'NIRv_MODIS'
evi_name = vi_name.replace('NIRv', 'EVI')
dft = dfi.loc[dfi[[vi_name, evi_name] + X_names + [y_name]].dropna().index, :]
dft = dft[dft[vi_name] > 1e-9]

dft = dft[X_names + [y_name] + [vi_name]].dropna()

Xs = dft[X_names + [vi_name]].rename(columns = {vi_name: 'NIRv'})
ys = dft[[y_name]]
# # --------------------------------------------------------------------------
# # method 1:
# X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.33, random_state=42)
# --------------------------------------------------------------------------
# method 2:
test_index = dfi[dfi['NIRv_MODIS'] > 1e-9]['NIRv_MODIS'].sample(frac = 0.33, random_state = 42).index.intersection(dft.index)
train_index = dfi[dfi['NIRv_MODIS'] > 1e-9]['NIRv_MODIS'].index.difference(test_index).intersection(dft.index)
X_train = Xs.loc[train_index]
X_test = Xs.loc[test_index]
y_train = ys.loc[train_index]
y_test = ys.loc[test_index]
# --------------------------------------------------------------------------

param_grid = {
    "objective": ["reg:squarederror"],
    "random_state": [0],
    'seed': [0],
}
param_grid = None
# regr = SmartForest(n_estimators_per_layer=5, max_layers=10, window_sizes=[], forget_factor=0., param_grid = param_grid)
regr = SmartForest4D(n_estimators_per_layer=5, max_layers=10, early_stopping_rounds=3, forget_factor=0., param_grid = param_grid, spatial_h=1, spatial_w=1, eval_metric = 'r2')
regr.fit(X_train.values, y_train.values, X_test.values, y_test.values)

best_model, best_rmse = regr.get_best_model()
y_pred = best_model[0][0].predict(X_test.values)

df_compare = y_test.copy()
df_compare[flux + '_pred'] = y_pred
df_compare.columns  = ['truth', 'pred']
stats.linregress(df_compare['truth'], df_compare['pred']), get_r2(df_compare['truth'], df_compare['pred'])

# Ensemble runs for uncertainty

def run_ensemble(X_train, y_train, n_models = 100, frac_sample = 0.8):
    base_params_xgb = {
        "objective": "reg:squarederror",
        'seed': 0,
        "random_state": 0,
    }
    params_xgb = deepcopy(base_params_xgb)
    # dropout-like regularization
    params_xgb.update({
        "subsample": 0.8,  # Use 80% of the data for each tree
        "colsample_bytree": 0.8,  # Use 80% of the features for each tree
    })

    models = []
    for i in tqdm(range(n_models)):
        # Create a bootstrapped dataset
        y_resampled = y_train.copy().sample(frac = frac_sample, random_state = i)
        X_resampled = X_train.copy().loc[y_resampled.index]
        # print(y_resampled.sort_index().index[0], y_resampled.sort_index().index[-1])

        # Train the XGBoost model
        params_xgb.update({'random_state': i})
        model = XGBRegressor(**params_xgb)
        model.fit(X_resampled, y_resampled)
        models.append(model)
    return models

root_proj_trendy.joinpath(f'3_output/UFLUXv1_model').mkdir(exist_ok = True, parents = True)

savefile = root_proj_trendy.joinpath(f'3_output/UFLUXv1_model/UFLUXv1-ensemble.pkl')
dfi = df_ml.copy()
dfi.index.name = 'time'
dfi = dfi.reset_index().set_index(['ID', 'time'])

X_names = [
    'temperature_2m', 'surface_solar_radiation_downwards_sum','VPD',
    'soil_temperature_level_1', 'surface_thermal_radiation_downwards_sum', 'total_precipitation_sum',
    'surface_net_radiation_sum',
    # 'temperature_2m_min', 'temperature_2m_max',
    # 'u_component_of_wind_10m', 'v_component_of_wind_10m', 'volumetric_soil_water_layer_1',
    # 'co2', 'C3_LUE', 'IWUE', 'C4_LUE', 'C4_RATIO',
    'KOPPEN', 'DEM', 'IGBP', 'El-Nino-La-Nina', 'IWUE',
    'MONTH',
    # 'MONTH', 'LAT', 'LON',
    # 'YEAR', 'MONTH', 'LAT', 'LON'
]

y_names = {
    'GPP': ['GPP_NT_VUT_REF'],
    'RECO': ['RECO_NT_VUT_REF'],
    'NEE': ['NEE_VUT_REF'],
    'LE': ['LE_F_MDS']
}

dd_out = {}

for flux in ['GPP', 'RECO', 'NEE', 'LE']:
    y_name = y_names[flux][0]
    vi_name = 'NIRv_MODIS'
    evi_name = vi_name.replace('NIRv', 'EVI')
    dft = dfi.loc[dfi[[vi_name, evi_name] + X_names + [y_name]].dropna().index, :]
    dft = dft[dft[vi_name] > 1e-9]

    dft = dft[X_names + [y_name] + [vi_name]].dropna()

    Xs = dft[X_names + [vi_name]].rename(columns = {vi_name: 'NIRv'})
    ys = dft[[y_name]]
    # # --------------------------------------------------------------------------
    # # method 1:
    # X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.33, random_state=42)
    # --------------------------------------------------------------------------
    # method 2:
    test_index = dfi[dfi['NIRv_MODIS'] > 1e-9]['NIRv_MODIS'].sample(frac = 0.33, random_state = 42).index.intersection(dft.index)
    train_index = dfi[dfi['NIRv_MODIS'] > 1e-9]['NIRv_MODIS'].index.difference(test_index).intersection(dft.index)
    X_train = Xs.loc[train_index]
    X_test = Xs.loc[test_index]
    y_train = ys.loc[train_index]
    y_test = ys.loc[test_index]
    # --------------------------------------------------------------------------

    models = run_ensemble(X_train, y_train, n_models = 10, frac_sample = 0.8)

    y_pred = []
    for i in range(len(models)):
        model = models[i]
        y_pred_t = model.predict(X_test)
        y_pred_t = pd.DataFrame(y_pred_t, index = y_test.index, columns = ['pred_' + str(i).zfill(2)])
        y_pred.append(y_pred_t)
    y_pred = pd.concat(y_pred, axis = 1)
    dfo = pd.concat([y_test[y_name].rename('truth'), y_pred], axis = 1)
    # # --------------------------------------------------------------------------
    # # Plot
    # fig, ax = setup_canvas(1, 1)
    # dfp = dfo.reset_index(drop = True)
    # ax.plot(dfp.drop('truth', axis = 1).mean(axis = 1), label = 'mean')
    # ax.plot(dfp['truth'])
    # ax.fill_between(dfp.index, dfp.drop('truth', axis = 1).min(axis = 1), dfp.drop('truth', axis = 1).max(axis = 1), color = colors[0], alpha = 0.2)
    # # --------------------------------------------------------------------------
    dfo['IGBP'] = meta.loc[dfo.index.get_level_values('ID'), 'IGBP'].values
    dd_ot = {
        'regr': models,
        'X_test': X_test,
        'X_names': X_names,
        'dfo': dfo
    }
    dd_out[flux] = dd_ot
dump_pickle(dd_out, savefile)

"""## Application"""

dd_out = load_pickle(root_proj_trendy.joinpath(f'3_output/UFLUXv1_model/UFLUXv1.pkl'))
print(dd_out.keys())

def get_lue_iwue_grid(nct):
    nct = nct.copy()
    tc =  nct['temperature_2m']
    tc = tc.where(tc >= -30, -30)
    tc = tc.values
    co2 = nct['co2'].values
    patm = nct['surface_pressure'].values * 100
    vpd = nct['VPD'].values * 100

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, True, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, True, c4 = True, limitation_factors = 'none')

    c3_lue = xr.DataArray(c3_lue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('C3_LUE')
    c4_lue = xr.DataArray(c4_lue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('C4_LUE')
    iwue = xr.DataArray(iwue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('IWUE')
    lue = (c3_lue * (1 - nct['C4_area'].fillna(0) / 100) + c4_lue  * nct['C4_area'].fillna(0) / 100)
    return lue, iwue

def fill_negtive_da(nct, nct_masked = None):
    if nct_masked is None: nct_masked = nct.where(nct >= 1e-9)
    interpolated_nct = nct_masked.interpolate_na(dim = ['longitude', 'latitude'], method="linear")
    result = interpolated_nct.where(~np.isnan(nct), np.nan)
    return result

root_proj_trendy.joinpath(f'3_output/UFLUXv1').mkdir(exist_ok = True)
for dt in tqdm(df_path_MODIS.index):
    savefile = root_proj_trendy.joinpath('3_output/UFLUXv1').joinpath(f"{dt.year}-UFLUXv1-ECup.nc")
    if savefile.exists(): continue

    p_sat = df_path_MODIS.loc[dt, 'path']
    p_era5 = df_path_era5.loc[dt, 'path']

    if dt.year == 2000:
        lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
    else:
        lucc = luccr.sel(time = dt)
    monthly_time = pd.date_range(
        dt.strftime('%Y-%m-%d'),
        (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
        freq = "MS"
    )
    lucc = lucc.expand_dims(time = monthly_time).rename({'LC_Type1': 'IGBP'})
    nc_geo = nc_geor.interp(latitude = grids.latitude, longitude = grids.longitude).expand_dims(time = monthly_time)
    nc_geo['C4_area'] = nc_geo['C4_area'].fillna(0)
    co2 = co2r.sel(time = dt).interp(latitude = grids.latitude, longitude = grids.longitude).expand_dims(time = monthly_time)

    v = df_el.loc[dt.year, 'El-Nino-La-Nina']
    el_grid = xr.DataArray(np.ones((len(grids.latitude), len(grids.longitude))), dims=["latitude", "longitude"], coords={"latitude": grids.latitude, "longitude": grids.longitude}) * v
    el_grid = el_grid.expand_dims(time = monthly_time).rename('El-Nino-La-Nina')

    sat = xr.open_dataset(p_sat)
    sat['Nadir_Reflectance_Band1'] /= 10000
    sat['Nadir_Reflectance_Band2'] /= 10000
    sat['NDVI'] = get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('NDVI')
    sat['NIRv'] = get_NIRv(get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']), sat['Nadir_Reflectance_Band2']).rename('NIRv')
    sat['EVI'] = get_EVI2band(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('EVI')
    sat = sat.where(sat['NIRv'] > 1e-9, np.nan)

    era5 = load_era5(p_era5, engine = 'netcdf4', drop_variables = drop_variables)

    nct = xr.merge([sat, era5, lucc, nc_geo, co2, el_grid])
    del(sat); del(era5); del(lucc); del(nc_geo); del(co2)
    lue, iwue = get_lue_iwue_grid(nct)

    months = xr.DataArray(monthly_time.month, dims=["time"], coords={"time": monthly_time}).expand_dims(latitude = grids.latitude, longitude = grids.longitude).rename('MONTH')
    years = xr.DataArray(monthly_time.year, dims=["time"], coords={"time": monthly_time}).expand_dims(latitude = grids.latitude, longitude = grids.longitude).rename('YEAR')
    lons, lats = np.meshgrid(grids.longitude.data, grids.latitude.data)
    lats = xr.DataArray(lats, dims=["latitude", "longitude"], coords={"latitude": grids.latitude, "longitude": grids.longitude}).expand_dims(time = monthly_time).rename('LAT')
    lons = xr.DataArray(lons, dims=["latitude", "longitude"], coords={"latitude": grids.latitude, "longitude": grids.longitude}).expand_dims(time = monthly_time).rename('LON')
    nct = xr.merge([nct, iwue, months, years, lons, lats])
    # nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]))
    del(months); del(lats); del(lons)

    # ------------------------------------------------------------------------------
    regr = dd_out['GPP']['regr']
    GPPmML = regr.predict(nct.to_dataframe()[dd_out['GPP']['X_names'] + ['NIRv']])
    GPPmML = pd.DataFrame(GPPmML, columns = ['GPPmML'], index = nct.to_dataframe().index).reset_index().set_index(['latitude', 'longitude', 'time']).to_xarray()['GPPmML']
    GPPmML = GPPmML.where(~nct['IGBP'].isin([0, 13, 15, 16, 17]), np.nan)
    GPPmML = GPPmML.where(nct['NIRv'] > 1e-9, np.nan)

    regr = dd_out['RECO']['regr']
    RECOmML = regr.predict(nct.to_dataframe()[dd_out['RECO']['X_names'] + ['NIRv']])
    RECOmML = pd.DataFrame(RECOmML, columns = ['RECOmML'], index = nct.to_dataframe().index).reset_index().set_index(['latitude', 'longitude', 'time']).to_xarray()['RECOmML']
    RECOmML = RECOmML.where(~nct['IGBP'].isin([0, 13, 15, 16, 17]), np.nan)
    RECOmML = RECOmML.where(nct['NIRv'] > 1e-9, np.nan)

    regr = dd_out['LE']['regr']
    LEmML = regr.predict(nct.to_dataframe()[dd_out['LE']['X_names'] + ['NIRv']])
    LEmML = pd.DataFrame(LEmML, columns = ['LEmML'], index = nct.to_dataframe().index).reset_index().set_index(['latitude', 'longitude', 'time']).to_xarray()['LEmML']
    LEmML = LEmML.where(~nct['IGBP'].isin([0, 13, 15, 16, 17]), np.nan)
    LEmML = LEmML.where(nct['NIRv'] > 1e-9, np.nan)


    regr = dd_out['NEE']['regr']
    NEEmML = regr.predict(nct.to_dataframe()[dd_out['NEE']['X_names'] + ['NIRv']])
    NEEmML = pd.DataFrame(NEEmML, columns = ['NEEmML'], index = nct.to_dataframe().index).reset_index().set_index(['latitude', 'longitude', 'time']).to_xarray()['NEEmML']
    NEEmML = NEEmML.where(~nct['IGBP'].isin([0, 13, 15, 16, 17]), np.nan)
    NEEmML = NEEmML.where(nct['NIRv'] > 1e-9, np.nan)
    NEEmML_mask = NEEmML.where((GPPmML >= 1e-9) & (RECOmML >= 1e-9)).to_dataset()

    GPPmML = fill_negtive_da(GPPmML.to_dataset())['GPPmML']
    RECOmML = fill_negtive_da(RECOmML.to_dataset())['RECOmML']
    NEEmML = fill_negtive_da(NEEmML.to_dataset(), nct_masked = NEEmML_mask)['NEEmML']
    # NEEmML = (RECOmML - GPPmML).rename('NEEmML')

    nco = xr.merge([GPPmML, RECOmML, LEmML, NEEmML, nct['NIRv']])

    nco.to_netcdf(savefile)
    del(GPPmML); del(RECOmML); del(LEmML); del(NEEmML); del(nct); del(nco)

"""## Application ensemble"""

dd_out = load_pickle(root_proj_trendy.joinpath(f'3_output/UFLUXv1_model/UFLUXv1-ensemble.pkl'))
print(dd_out.keys())

def get_lue_iwue_grid(nct):
    nct = nct.copy()
    tc =  nct['temperature_2m']
    tc = tc.where(tc >= -30, -30)
    tc = tc.values
    co2 = nct['co2'].values
    patm = nct['surface_pressure'].values * 100
    vpd = nct['VPD'].values * 100

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, True, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, True, c4 = True, limitation_factors = 'none')

    c3_lue = xr.DataArray(c3_lue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('C3_LUE')
    c4_lue = xr.DataArray(c4_lue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('C4_LUE')
    iwue = xr.DataArray(iwue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('IWUE')
    lue = (c3_lue * (1 - nct['C4_area'].fillna(0) / 100) + c4_lue  * nct['C4_area'].fillna(0) / 100)
    return lue, iwue

def fill_negtive_da(nct, nct_masked = None):
    if nct_masked is None: nct_masked = nct.where(nct >= 1e-9)
    interpolated_nct = nct_masked.interpolate_na(dim = ['longitude', 'latitude'], method="linear")
    result = interpolated_nct.where(~np.isnan(nct), np.nan)
    return result

root_proj_trendy.joinpath(f'3_output/UFLUXv1-ensemble').mkdir(exist_ok = True)
for dt in tqdm(df_path_MODIS.index):
    savefile = root_proj_trendy.joinpath('3_output/UFLUXv1-ensemble').joinpath(f"{dt.year}-UFLUXv1-ECup.nc")
    if savefile.exists(): continue

    p_sat = df_path_MODIS.loc[dt, 'path']
    p_era5 = df_path_era5.loc[dt, 'path']

    if dt.year == 2000:
        lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
    else:
        lucc = luccr.sel(time = dt)
    monthly_time = pd.date_range(
        dt.strftime('%Y-%m-%d'),
        (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
        freq = "MS"
    )
    lucc = lucc.expand_dims(time = monthly_time).rename({'LC_Type1': 'IGBP'})
    nc_geo = nc_geor.interp(latitude = grids.latitude, longitude = grids.longitude).expand_dims(time = monthly_time)
    nc_geo['C4_area'] = nc_geo['C4_area'].fillna(0)
    co2 = co2r.sel(time = dt).interp(latitude = grids.latitude, longitude = grids.longitude).expand_dims(time = monthly_time)

    v = df_el.loc[dt.year, 'El-Nino-La-Nina']
    el_grid = xr.DataArray(np.ones((len(grids.latitude), len(grids.longitude))), dims=["latitude", "longitude"], coords={"latitude": grids.latitude, "longitude": grids.longitude}) * v
    el_grid = el_grid.expand_dims(time = monthly_time).rename('El-Nino-La-Nina')

    sat = xr.open_dataset(p_sat)
    sat['Nadir_Reflectance_Band1'] /= 10000
    sat['Nadir_Reflectance_Band2'] /= 10000
    sat['NDVI'] = get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('NDVI')
    sat['NIRv'] = get_NIRv(get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']), sat['Nadir_Reflectance_Band2']).rename('NIRv')
    sat['EVI'] = get_EVI2band(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('EVI')
    sat = sat.where(sat['NIRv'] > 1e-9, np.nan)

    era5 = load_era5(p_era5, engine = 'netcdf4', drop_variables = drop_variables)

    nct = xr.merge([sat, era5, lucc, nc_geo, co2, el_grid])
    del(sat); del(era5); del(lucc); del(nc_geo); del(co2)
    lue, iwue = get_lue_iwue_grid(nct)

    months = xr.DataArray(monthly_time.month, dims=["time"], coords={"time": monthly_time}).expand_dims(latitude = grids.latitude, longitude = grids.longitude).rename('MONTH')
    years = xr.DataArray(monthly_time.year, dims=["time"], coords={"time": monthly_time}).expand_dims(latitude = grids.latitude, longitude = grids.longitude).rename('YEAR')
    lons, lats = np.meshgrid(grids.longitude.data, grids.latitude.data)
    lats = xr.DataArray(lats, dims=["latitude", "longitude"], coords={"latitude": grids.latitude, "longitude": grids.longitude}).expand_dims(time = monthly_time).rename('LAT')
    lons = xr.DataArray(lons, dims=["latitude", "longitude"], coords={"latitude": grids.latitude, "longitude": grids.longitude}).expand_dims(time = monthly_time).rename('LON')
    nct = xr.merge([nct, iwue, months, years, lons, lats])
    # nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]))
    del(months); del(lats); del(lons)

    # ------------------------------------------------------------------------------
    models = dd_out['GPP']['regr']
    GPPmML = []
    for i, regr in enumerate(models):
        GPPmMLt = regr.predict(nct.to_dataframe()[dd_out['GPP']['X_names'] + ['NIRv']])
        GPPmMLt = pd.DataFrame(GPPmMLt, columns = ['GPPmML' + str(i).zfill(2)], index = nct.to_dataframe().index).reset_index().set_index(['latitude', 'longitude', 'time'])
        GPPmML.append(GPPmMLt)
        del(GPPmMLt)
    GPPmML = pd.concat(GPPmML, axis = 1)
    GPPmML = pd.concat([
        GPPmML.min(axis = 1).rename('GPPmML_min'),
        GPPmML.mean(axis = 1).rename('GPPmML_mean'),
        GPPmML.max(axis = 1).rename('GPPmML_max')
    ], axis = 1)
    GPPmML = GPPmML.to_xarray()
    GPPmML = GPPmML.where(~nct['IGBP'].isin([0, 13, 15, 16, 17]), np.nan)
    GPPmML = GPPmML.where(nct['NIRv'] > 1e-9, np.nan)
    # GPPmML = GPPmML.where(GPPmML > 1e-9, np.nan)

    models = dd_out['RECO']['regr']
    RECOmML = []
    for i, regr in enumerate(models):
        RECOmMLt = regr.predict(nct.to_dataframe()[dd_out['RECO']['X_names'] + ['NIRv']])
        RECOmMLt = pd.DataFrame(RECOmMLt, columns=['RECOmML' + str(i).zfill(2)], index=nct.to_dataframe().index).reset_index().set_index(['latitude', 'longitude', 'time'])
        RECOmML.append(RECOmMLt)
        del(RECOmMLt)
    RECOmML = pd.concat(RECOmML, axis=1)
    RECOmML = pd.concat([
        RECOmML.min(axis=1).rename('RECOmML_min'),
        RECOmML.mean(axis=1).rename('RECOmML_mean'),
        RECOmML.max(axis=1).rename('RECOmML_max')
    ], axis=1)
    RECOmML = RECOmML.to_xarray()
    RECOmML = RECOmML.where(~nct['IGBP'].isin([0, 13, 15, 16, 17]), np.nan)
    RECOmML = RECOmML.where(nct['NIRv'] > 1e-9, np.nan)
    # RECOmML = RECOmML.where(RECOmML > 1e-9, np.nan)

    models = dd_out['LE']['regr']
    LEmML = []
    for i, regr in enumerate(models):
        LEmMLt = regr.predict(nct.to_dataframe()[dd_out['LE']['X_names'] + ['NIRv']])
        LEmMLt = pd.DataFrame(LEmMLt, columns=['LEmML' + str(i).zfill(2)], index=nct.to_dataframe().index).reset_index().set_index(['latitude', 'longitude', 'time'])
        LEmML.append(LEmMLt)
        del(LEmMLt)
    LEmML = pd.concat(LEmML, axis=1)
    LEmML = pd.concat([
        LEmML.min(axis=1).rename('LEmML_min'),
        LEmML.mean(axis=1).rename('LEmML_mean'),
        LEmML.max(axis=1).rename('LEmML_max')
    ], axis=1)
    LEmML = LEmML.to_xarray()
    LEmML = LEmML.where(~nct['IGBP'].isin([0, 13, 15, 16, 17]), np.nan)
    LEmML = LEmML.where(nct['NIRv'] > 1e-9, np.nan)
    # LEmML = LEmML.where(LEmML > 1e-9, np.nan)

    models = dd_out['NEE']['regr']
    NEEmML = []
    for i, regr in enumerate(models):
        NEEmMLt = regr.predict(nct.to_dataframe()[dd_out['NEE']['X_names'] + ['NIRv']])
        NEEmMLt = pd.DataFrame(NEEmMLt, columns = ['NEEmML' + str(i).zfill(2)], index = nct.to_dataframe().index).reset_index().set_index(['latitude', 'longitude', 'time'])
        NEEmML.append(NEEmMLt)
        del(NEEmMLt)
    NEEmML = pd.concat(NEEmML, axis=1)
    NEEmML = pd.concat([
        NEEmML.min(axis=1).rename('NEEmML_min'),
        NEEmML.mean(axis=1).rename('NEEmML_mean'),
        NEEmML.max(axis=1).rename('NEEmML_max')
    ], axis=1)
    NEEmML = NEEmML.to_xarray()
    NEEmML = NEEmML.where(~nct['IGBP'].isin([0, 13, 15, 16, 17]), np.nan)
    NEEmML = NEEmML.where(nct['NIRv'] > 1e-9, np.nan)
    # NEEmML = NEEmML.where(NEEmML > 1e-9, np.nan)
    NEEmML_mask = NEEmML.where((GPPmML['GPPmML_mean'] >= 1e-9) & (RECOmML['RECOmML_mean'] >= 1e-9))

    GPPmML = fill_negtive_da(GPPmML)
    RECOmML = fill_negtive_da(RECOmML)
    NEEmML = fill_negtive_da(NEEmML, nct_masked = NEEmML_mask)
    # NEEmML = (RECOmML - GPPmML).rename('NEEmML')

    nco = xr.merge([GPPmML, RECOmML, LEmML, NEEmML, nct['NIRv']])

    nco.to_netcdf(savefile)
    del(GPPmML); del(RECOmML); del(LEmML); del(NEEmML); del(nct); del(nco)

"""# UFLUXv2

## Functions
"""

def get_4VIs(data, r_name, nir_name, as_one = True):
    r = data[r_name]
    nir = data[nir_name]
    ndvi = get_NDVI(r, nir)
    evi = get_EVI2band(r, nir)
    nirv = get_NIRv(ndvi, nir)
    kndvi = get_kNDVI(ndvi)

    if as_one:
        ndvi.name = 'NDVI'
        evi.name = 'EVI'
        nirv.name = 'NIRv'
        kndvi.name = 'kNDVI'
        ndvi = ndvi.to_dataset()
        evi = evi.to_dataset()
        nirv = nirv.to_dataset()
        kndvi = kndvi.to_dataset()
        return xr.merge([ndvi, evi, nirv, kndvi])
    else:
        return (ndvi, evi, nirv, kndvi)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

def run_ml(X_train, X_test, y_train, y_test):
    xgb_params = {
        "objective": "reg:squarederror",
        "random_state": 0,
        'seed': 0,
    }

    regr = XGBRegressor(**xgb_params)
    regr.fit(X_train, y_train)
    y_pred = regr.predict(X_test)
    # print(regr.score(X_train, y_train))
    # print(regr.score(X_test, y_test))

    dfo = y_test.copy()#.to_frame()
    dfo.columns = ['truth']
    dfo['pred'] = y_pred
    # dfo = dfo / np.power(10, vexp)
    # dfo.index = dfo.index.droplevel([0, 1])

    # eval_res = regress2(dfo['truth'].values, dfo['pred'].values)
    # rvalue = eval_res['r']
    # slope = eval_res['slope']
    # intercept = eval_res['intercept']
    eval_res = pipelines.get_metrics(dfo, truth = 'truth', pred = 'pred', return_dict = True)
    # print(eval_res)
    return regr, dfo, eval_res

def agg_global_GPP_025deg(nct, world, name = 'GPP'):
    coef = 365 * 0.25 * 0.25 * 1e5 * 1e5 / 1e15
    nct = nct.where(nct[name] > 1e-9, 0.0001).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
    dft = nct[name].sum(dim = ['latitude', 'longitude']).drop_vars('spatial_ref').to_dataframe() * coef
    return dft

def load_GPP(name, foldername, world):
    dfo = []
    for p in foldername.glob(f'{name}*.nc'):
        print(p.stem)
        nct = xr.open_dataset(p)
        dft = agg_global_GPP_025deg(nct, world)
        dfo.append(dft)
        nct.close(); del(nct)
    dfo = pd.concat(dfo, axis = 0)
    return dfo

def get_grid_NIRv(p, FAPAR_ok = False, reproj_ok = False):
    rnc = rxr.open_rasterio(p, band_as_variable = True)
    if reproj_ok:
        rnc = rnc.rio.reproject("EPSG:4326")
    assert (rnc.rio.crs == 'EPSG:4326'), rnc.rio.crs
    name_dict = dict(zip(rnc.keys(), ['R', 'NIR']))
    name_dict.update({'x': 'longitude', 'y': 'latitude'})
    rnc = rnc.rename(name_dict)
    R = rnc['R']
    NIR = rnc['NIR']
    if np.nanmean(NIR.data) > 100:
        NIR = NIR / 10000
        R = R / 10000
    NIRv = (NIR - R) / (NIR + R) * NIR
    NIRv.name = 'NIRv'
    if FAPAR_ok:
        EVI = 2.5 * (NIR - R) / (NIR + 2.4 * R + 1)
        FAPAR = (EVI - 0.1 + 0.1) * 1.25
        FAPAR.name = 'FAPAR'
        return NIRv, FAPAR
    else:
        return NIRv

def get_Amazon_patch(nct, minx = -35.2, miny = -9, maxx = -35, maxy = -6):
    cond = (nct.longitude > minx) & (nct.longitude < maxx) & (nct.latitude > miny) & (nct.latitude < maxy)
    return ~cond

"""## Train"""

meta = pd.read_csv((r'https://github.com/soonyenju/scitbx/blob/master/scitbx/data/fluxnet_meta_212.csv?raw=true'),index_col = 0)

nc_ec = xr.open_dataset(root_proj_platform.joinpath('FLUXNET2015_DD.nc'))

nc_era5 = xr.open_dataset(root_proj_platform.joinpath('0tower_level/ERA5-Land-daily-var19.nc'), engine = 'netcdf4')
df_era5 = nc_era5.to_dataframe()
df_era5['dewpoint_temperature_2m'] -= 273.15
df_era5['temperature_2m'] -= 273.15
df_era5['soil_temperature_level_1'] -= 273.15

df_era5['surface_latent_heat_flux_sum'] /= 86400
df_era5['surface_sensible_heat_flux_sum'] /= 86400
df_era5['surface_solar_radiation_downwards_sum'] /= 86400
df_era5['surface_thermal_radiation_downwards_sum'] /= 86400
df_era5['surface_net_solar_radiation_sum'] /= 86400
df_era5['surface_net_thermal_radiation_sum'] /= 86400

df_era5['surface_pressure'] /= 100

df_era5['temperature_2m_min'] -= 273.15
df_era5['temperature_2m_max'] -= 273.15

df_era5['VPD'] = saturation_vapor_pressure(df_era5['temperature_2m']) - saturation_vapor_pressure(df_era5['dewpoint_temperature_2m'])
df_era5['surface_net_radiation_sum'] = df_era5['surface_net_solar_radiation_sum'] + df_era5['surface_net_thermal_radiation_sum']

# era5r = df_era5[[
#     'temperature_2m', 'soil_temperature_level_1',
#     'snow_cover', 'volumetric_soil_water_layer_1', 'forecast_albedo',
#     'surface_latent_heat_flux_sum', 'surface_sensible_heat_flux_sum',
#     'surface_solar_radiation_downwards_sum',
#     'surface_thermal_radiation_downwards_sum',
#     'surface_net_solar_radiation_sum',
#     'surface_net_thermal_radiation_sum',
#     'evaporation_from_the_top_of_canopy_sum', 'surface_pressure',
#     'total_precipitation_sum', 'temperature_2m_min', 'temperature_2m_max',
#     'u_component_of_wind_10m', 'v_component_of_wind_10m', 'VPD'
# ]].to_xarray()

era5r = df_era5[[
    'temperature_2m', 'soil_temperature_level_1',
    'volumetric_soil_water_layer_1',
    'surface_solar_radiation_downwards_sum',
    'surface_thermal_radiation_downwards_sum',
    'surface_net_radiation_sum',
    'surface_pressure',
    'total_precipitation_sum', 'VPD'
]].to_xarray()

root_proj_lucc = root.joinpath("workspace/project_data/LUCC")
df_c4 = pd.read_csv(root_proj_lucc.joinpath(f'0Training/c4map_tower.csv'), index_col = 0)
df_co2 = load_pickle(root_proj_lucc.joinpath(f'0Training/CO2_tower.pkl'))
df_koppen = pd.read_csv(root_proj_lucc.joinpath(f'0Training/KOPPEN_tower.csv'), index_col = 0)
df_dem = pd.read_csv(root_proj_lucc.joinpath(f'0Training/DEM_tower.csv'), index_col = 0)
df_fpar = pd.read_csv(root_proj_lucc.joinpath('0Training/MODIS_Fpar_tower.csv'), index_col = 0)
df_fpar.index = pd.to_datetime(df_fpar.index, format = '%Y-%m-%d')
df_fpar = df_fpar.resample('1MS').mean()

# Load reflectance

nc_sat_high = xr.open_dataset(root_proj_platform.joinpath('0tower_level/Satellite_surface_reflectance_NIR_R_high-resolution.nc'))

# # ----------------------------------------------------------------------------
# Calibrate Landsat

# Harmonise Landsat satellites
# https://developers.google.com/earth-engine/tutorials/community/landsat-etm-to-oli-harmonization
# https://openprairie.sdstate.edu/cgi/viewcontent.cgi?referer=https://scholar.google.com/&httpsredir=1&article=1035&context=gsce_pubs

# Red: OLI = 0.0061 + 0.9047 ETM+
# NIR: OLI = 0.0412 + 0.8462 ETM+

dft = nc_sat_high.to_dataframe()
index = dft.index.get_level_values(2)
dft.loc[index == 'Landsat5', 'R'] = dft.loc[index == 'Landsat5', 'R'] * 0.9047 + 0.0061
dft.loc[index == 'Landsat5', 'NIR'] = dft.loc[index == 'Landsat5', 'NIR'] * 0.8462 + 0.0412
dft.loc[index == 'Landsat7', 'R'] = dft.loc[index == 'Landsat7', 'R'] * 0.9047 + 0.0061
dft.loc[index == 'Landsat7', 'NIR'] = dft.loc[index == 'Landsat7', 'NIR'] * 0.8462 + 0.0412
nc_sat_high = dft.to_xarray()

# # ----------------------------------------------------------------------------
# Calculate VI

# ndvi_high, evi_high, nirv_high, kndvi_high = get_4VIs(nc_sat_high, 'R', 'NIR', as_one = False)
vi_high = get_4VIs(nc_sat_high, 'R', 'NIR')

era5 = deepcopy(era5r)
nc_ec_mon = nc_ec.resample(time = '1MS').mean()
vi_mon_high = vi_high.resample(time = '1MS').mean()
era5_mon = era5.resample(time = '1MS').mean()

nc_ec_mon = nc_ec_mon.where((nc_ec_mon['time.year'] > 1999) & (nc_ec_mon['time.year'] < 2015), drop = True)
vi_mon_high = vi_mon_high.where((vi_mon_high['time.year'] > 1999) & (vi_mon_high['time.year'] < 2015), drop = True)
era5_mon = era5_mon.where((era5_mon['time.year'] > 1999) & (era5_mon['time.year'] < 2015), drop = True)

df_ml = []
for site in nc_ec.ID.data:
    df_ec = nc_ec_mon.sel(ID = site).drop_vars('ID').to_dataframe()

    df_ec = df_ec[df_ec['GPP_NT_VUT_REF'] >= 0]
    df_ec = df_ec[(df_ec['GPP_NT_VUT_REF'] - df_ec['GPP_DT_VUT_REF']).abs() <= 3]

    df_ec = df_ec[df_ec['RECO_NT_VUT_REF'] >= 0]
    df_ec = df_ec[(df_ec['RECO_NT_VUT_REF'] - df_ec['RECO_DT_VUT_REF']).abs() <= 3]

    df_vi_high = []
    for vi_name in ['NDVI', 'EVI', 'NIRv', 'kNDVI']:
        # dft = vi_mon_high.sel(ID = site).to_dataframe()[vi_name].rename(vi_name + '_high').reset_index().pivot(index = 'time', columns = 'satellite')
        dft = vi_mon_high.sel(ID = site).to_dataframe()[vi_name].reset_index().pivot(index = 'time', columns = 'satellite')
        dft.columns = dft.columns.map('_'.join).str.strip('_')
        df_vi_high.append(dft)
    df_vi_high = pd.concat(df_vi_high, axis = 1)

    df_era5 = era5_mon.sel(ID = site).drop_vars('ID').to_dataframe()

    dft = pd.concat([df_ec, df_vi_high, df_era5], axis = 1)

    dft = pd.concat([dft, df_co2[df_co2.index.get_level_values(1) == site].droplevel('ID')], axis = 1)
    dft = pd.concat([dft, df_fpar[site].rename('FAPAR_MODIS_PROD')], axis = 1)

    # dft_cold = dft[dft['temperature_2m'] <= -30]
    # dft = dft[dft['temperature_2m'] > -30]
    dft.loc[dft['temperature_2m'] < -30, 'temperature_2m'] = -30

    tc = dft['temperature_2m'].values
    co2 = dft['co2'].values
    patm = dft['surface_pressure'].values * 100
    vpd = dft['VPD'].values * 100
    do_ftemp_kphio = True

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, do_ftemp_kphio, c4 = True, limitation_factors = 'none')
    # ----------------------------------------------------------------------------------------------------------------------------
    dft['C3_LUE'] = c3_lue
    dft['IWUE'] = iwue
    dft['C4_LUE'] = c4_lue
    # dft = pd.concat([dft, dft_cold], axis = 0)
    dft['C4_RATIO'] = df_c4.loc[site, 'C4_area'] # df_c4 has no NaN, min is 0
    dft['KOPPEN'] = df_koppen.loc[site, 'KOPPEN']
    dft['DEM'] = df_dem.loc[site, 'DEM']
    dft['ID'] = site

    dft['YEAR'] = dft.index.year
    dft['MONTH'] = dft.index.month
    dft['LAT'] = meta.loc[site, 'LAT']
    dft['LON'] = meta.loc[site, 'LON']

    df_ml.append(dft)

df_ml = pd.concat(df_ml, axis = 0)
for c in [c for c in df_ml.columns if c.startswith('EVI')]:
    df_ml[c.replace('EVI_', 'FAPAR_')] = (df_ml[c] - 0.1) * 1.25

df_ml['IGBP'] = [MODIS_IGBP_dict[igbp] for igbp in meta.loc[df_ml['ID'], 'IGBP'].values]
df_ml['El-Nino-La-Nina'] = df_el.loc[df_ml.index.year, 'El-Nino-La-Nina'].values
# df_ml = df_ml.reset_index()
df_mlr = df_ml.copy()

df_ml = df_mlr.copy()
# df_ml = df_ml[df_ml['NEE_VUT_REF_QC'] < 0.8]
# df_ml = df_ml[(df_ml['GPP_NT_VUT_REF'] - df_ml['GPP_DT_VUT_REF']) < 3]

# GPPm = (df_ml['C3_LUE'] * (1 - df_ml['C4_RATIO'] / 100) + df_ml['C4_LUE']  * df_ml['C4_RATIO'] / 100) * df_ml['FAPAR_MODIS'] * df_ml['surface_solar_radiation_downwards_sum'] * 2.3
GPPm = (df_ml['C3_LUE'] * (1 - df_ml['C4_RATIO'] / 100) + df_ml['C4_LUE']  * df_ml['C4_RATIO'] / 100) * df_ml['EVI_MODIS'] * 1.25 * df_ml['surface_solar_radiation_downwards_sum'] * 2.3
RECOm = respiration.calculate_ecosystem_respiration(df_ml['temperature_2m'], df_ml['NIRv_MODIS'] * 15, Q10 = 1.5, Tref = 10)
LEm = evapotranspiration.mmday2Wm2(evapotranspiration.Priestley_Taylor_JPL(df_ml['surface_net_radiation_sum'], df_ml['temperature_2m'], df_ml['NDVI_MODIS']))

LEm = LEm.rename('LEm')
GPPm = GPPm.rename('GPPm')
RECOm = RECOm.rename('RECOm')

dd_fit = {}
fac = get_scaling_factor(df_ml['GPP_NT_VUT_REF'], GPPm); dd_fit['GPP'] = {'slope': fac[0], 'intercept': fac[1]}
GPPm = (GPPm - fac[1]) / fac[0]
fac = get_scaling_factor(df_ml['RECO_NT_VUT_REF'], RECOm); dd_fit['RECO'] = {'slope': fac[0], 'intercept': fac[1]}
RECOm = (RECOm - fac[1]) / fac[0]
fac = get_scaling_factor(df_ml['LE_F_MDS'], LEm); dd_fit['LE'] = {'slope': fac[0], 'intercept': fac[1]}
LEm = (LEm - fac[1]) / fac[0]
NEEm = (RECOm - GPPm).rename('NEEm')

dfi = df_ml.copy()
dfi = pd.concat([dfi, LEm, GPPm, RECOm, NEEm], axis = 1)
# dfi[['GPP_NT_VUT_REF', 'GPPm']].dropna().plot.scatter(x = 'GPP_NT_VUT_REF', y = 'GPPm')
# dfi[['RECO_NT_VUT_REF', 'RECOm']].dropna().plot.scatter(x = 'RECO_NT_VUT_REF', y = 'RECOm')
# dfi[['NEE_VUT_REF', 'NEEm']].dropna().plot.scatter(x = 'NEE_VUT_REF', y = 'NEEm')
dfi[['LE_F_MDS', 'LEm']].dropna().plot.scatter(x = 'LE_F_MDS', y = 'LEm', xlim = (-10, 250), ylim = (-10, 250))
# dfi[['RECO_NT_VUT_REF', 'GPP_NT_VUT_REF']].dropna().plot.scatter(x = 'RECO_NT_VUT_REF', y = 'GPP_NT_VUT_REF')
# df_ml.columns LE_F_MDS

# dfi = df_ml.copy()
# dfi = pd.concat([dfi, GPPm, RECOm, NEEm, LEm], axis = 1)
# dfi.index.name = 'time'
# dfi = dfi.reset_index().set_index(['ID', 'time'])

# dfi.to_parquet('/content/drive/My Drive/workspace/project_data/urban_sust/training_data.parquet')

# dd_fit

root_proj_trendy.joinpath(f'3_output/UFLUXv2_model').mkdir(exist_ok = True, parents = True)

savefile = root_proj_trendy.joinpath(f'3_output/UFLUXv2_model/UFLUXv2.pkl')
dfi = df_ml.copy()
dfi = pd.concat([dfi, GPPm, RECOm, NEEm, LEm], axis = 1)
dfi.index.name = 'time'
dfi = dfi.reset_index().set_index(['ID', 'time'])

X_names = [
    'temperature_2m', 'surface_solar_radiation_downwards_sum','VPD',
    'soil_temperature_level_1', 'surface_thermal_radiation_downwards_sum', 'total_precipitation_sum',
    'surface_net_radiation_sum',
    # 'temperature_2m_min', 'temperature_2m_max',
    # 'u_component_of_wind_10m', 'v_component_of_wind_10m', 'volumetric_soil_water_layer_1',
    # 'co2', 'C3_LUE', 'IWUE', 'C4_LUE', 'C4_RATIO',
    'KOPPEN', 'DEM', 'IGBP', 'El-Nino-La-Nina', 'IWUE',
    # 'MONTH',
    'MONTH', 'LAT', 'LON',
    # 'YEAR', 'MONTH', 'LAT', 'LON'
]

y_names = {
    'GPP': ['GPP_NT_VUT_REF', 'GPPm', 'GPPd', 'GPPmML'],
    'RECO': ['RECO_NT_VUT_REF', 'RECOm', 'RECOd', 'RECOmML'],
    'NEE': ['NEE_VUT_REF', 'NEEm', 'NEEd', 'NEEmML'],
    'LE': ['LE_F_MDS', 'LEm', 'LEd', 'LEmML']
}

df_res = []; dfm = []; dd_out = {}

for flux in ['GPP', 'RECO', 'NEE', 'LE']:
    y_name = y_names[flux][0]
    y_mname = y_names[flux][1]
    y_dname = y_names[flux][2]
    y_mlname = y_names[flux][3]
    vi_name = 'NIRv_MODIS'
    evi_name = vi_name.replace('NIRv', 'EVI')
    dft = dfi.loc[dfi[[vi_name, evi_name] + X_names + [y_name]].dropna().index, :]
    dft = dft[dft[vi_name] > 1e-9]

    dft[y_dname] = dft[y_name].values - dft[y_mname].values
    dft = dft[(dft[y_dname] / dft[y_mname]).abs() < 3]
    df_pm = dft[[y_mname]]
    dft = dft[X_names + [y_dname] + [vi_name]].dropna()

    Xs = dft[X_names + [vi_name]].rename(columns = {vi_name: 'NIRv'})
    ys = dft[[y_dname]]
    # # --------------------------------------------------------------------------
    # # method 1:
    # X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.33, random_state=42)
    # --------------------------------------------------------------------------
    # method 2:
    test_index = dfi[dfi['NIRv_MODIS'] > 1e-9]['NIRv_MODIS'].sample(frac = 0.33, random_state = 42).index.intersection(dft.index)
    train_index = dfi[dfi['NIRv_MODIS'] > 1e-9]['NIRv_MODIS'].index.difference(test_index).intersection(dft.index)
    X_train = Xs.loc[train_index]
    X_test = Xs.loc[test_index]
    y_train = ys.loc[train_index]
    y_test = ys.loc[test_index]
    # --------------------------------------------------------------------------

    regr, dfo, res = run_ml(X_train, X_test, y_train, y_test)
    res.index = [flux]
    df_res.append(res)
    dfo['IGBP'] = meta.loc[dfo.index.get_level_values('ID'), 'IGBP'].values
    dfo[y_name] = dfi.loc[dfo.index, y_name]
    dfo[y_mname] = df_pm.loc[dfo.index, y_mname]
    dfo[y_mlname] = dfo['pred'] + dfo[y_mname]

    df_feat_imp = pd.DataFrame(regr.feature_importances_, columns = [flux], index = X_names + ['NIRv']).sort_values(by = flux, ascending = False)
    dd_ot = {
        'regr': regr,
        'X_test': X_test,
        'X_names': X_names,
        'df_feature_importance': df_feat_imp,
        'dfo': dfo
    }
    dd_out[flux] = dd_ot
    dfm.append(pd.concat([get_dfm(dfo, y_mname, y_name, y_mname), get_dfm(dfo, y_mlname, y_name, y_mlname)]))
dd_out['scaling_factor'] = dd_fit
df_res = pd.concat(df_res, axis = 0)
dfm = pd.concat(dfm, axis = 0)
dd_out['df_res'] = df_res
dd_out['dfm'] = dfm
dump_pickle(dd_out, savefile)
print(df_res)

"""## Application"""

dd_out = load_pickle(root_proj_trendy.joinpath(f'3_output/UFLUXv2_model/UFLUXv2.pkl'))
dd_fit = dd_out['scaling_factor']
print(dd_out.keys())

def get_lue_iwue_grid(nct):
    nct = nct.copy()
    tc =  nct['temperature_2m']
    tc = tc.where(tc >= -30, -30)
    tc = tc.values
    co2 = nct['co2'].values
    patm = nct['surface_pressure'].values * 100
    vpd = nct['VPD'].values * 100

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, True, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, True, c4 = True, limitation_factors = 'none')

    c3_lue = xr.DataArray(c3_lue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('C3_LUE')
    c4_lue = xr.DataArray(c4_lue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('C4_LUE')
    iwue = xr.DataArray(iwue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('IWUE')
    lue = (c3_lue * (1 - nct['C4_area'].fillna(0) / 100) + c4_lue  * nct['C4_area'].fillna(0) / 100)
    return lue, iwue

def fill_negtive_da(nct, nct_masked = None):
    if nct_masked is None: nct_masked = nct.where(nct >= 1e-9)
    interpolated_nct = nct_masked.interpolate_na(dim = ['longitude', 'latitude'], method="linear")
    result = interpolated_nct.where(~np.isnan(nct), np.nan)
    return result

root_proj_trendy.joinpath(f'3_output/UFLUXv2').mkdir(exist_ok = True)
for dt in tqdm(df_path_MODIS.index):
    savefile = root_proj_trendy.joinpath('3_output/UFLUXv2').joinpath(f"{dt.year}-UFLUXv2-ECup.nc")
    if savefile.exists(): continue

    p_sat = df_path_MODIS.loc[dt, 'path']
    p_era5 = df_path_era5.loc[dt, 'path']

    if dt.year == 2000:
        lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
    else:
        lucc = luccr.sel(time = dt)
    monthly_time = pd.date_range(
        dt.strftime('%Y-%m-%d'),
        (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
        freq = "MS"
    )
    lucc = lucc.expand_dims(time = monthly_time).rename({'LC_Type1': 'IGBP'})
    nc_geo = nc_geor.interp(latitude = grids.latitude, longitude = grids.longitude).expand_dims(time = monthly_time)
    nc_geo['C4_area'] = nc_geo['C4_area'].fillna(0)
    co2 = co2r.sel(time = dt).interp(latitude = grids.latitude, longitude = grids.longitude).expand_dims(time = monthly_time)

    v = df_el.loc[dt.year, 'El-Nino-La-Nina']
    el_grid = xr.DataArray(np.ones((len(grids.latitude), len(grids.longitude))), dims=["latitude", "longitude"], coords={"latitude": grids.latitude, "longitude": grids.longitude}) * v
    el_grid = el_grid.expand_dims(time = monthly_time).rename('El-Nino-La-Nina')

    sat = xr.open_dataset(p_sat)
    sat['Nadir_Reflectance_Band1'] /= 10000
    sat['Nadir_Reflectance_Band2'] /= 10000
    sat['NDVI'] = get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('NDVI')
    sat['NIRv'] = get_NIRv(get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']), sat['Nadir_Reflectance_Band2']).rename('NIRv')
    sat['EVI'] = get_EVI2band(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('EVI')
    sat = sat.where(sat['NIRv'] > 1e-9, np.nan)

    era5 = load_era5(p_era5, engine = 'netcdf4', drop_variables = drop_variables)

    nct = xr.merge([sat, era5, lucc, nc_geo, co2, el_grid])
    del(sat); del(era5); del(lucc); del(nc_geo); del(co2)
    lue, iwue = get_lue_iwue_grid(nct)

    months = xr.DataArray(monthly_time.month, dims=["time"], coords={"time": monthly_time}).expand_dims(latitude = grids.latitude, longitude = grids.longitude).rename('MONTH')
    years = xr.DataArray(monthly_time.year, dims=["time"], coords={"time": monthly_time}).expand_dims(latitude = grids.latitude, longitude = grids.longitude).rename('YEAR')
    lons, lats = np.meshgrid(grids.longitude.data, grids.latitude.data)
    lats = xr.DataArray(lats, dims=["latitude", "longitude"], coords={"latitude": grids.latitude, "longitude": grids.longitude}).expand_dims(time = monthly_time).rename('LAT')
    lons = xr.DataArray(lons, dims=["latitude", "longitude"], coords={"latitude": grids.latitude, "longitude": grids.longitude}).expand_dims(time = monthly_time).rename('LON')
    nct = xr.merge([nct, iwue, months, years, lons, lats])
    # nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]))
    del(months); del(lats); del(lons)

    GPPm = lue * nct['EVI'] * 1.25 * nct['surface_solar_radiation_downwards_sum'] * 2.3
    RECOm = respiration.calculate_ecosystem_respiration(nct['temperature_2m'], nct['NIRv'] * 15, Q10 = 1.5, Tref = 10)
    LEm = evapotranspiration.mmday2Wm2(evapotranspiration.Priestley_Taylor_JPL(nct['surface_net_radiation_sum'], nct['temperature_2m'], nct['NDVI']))

    # GPPm = dd_fit['GPP']['slope'] * GPPm.rename('GPPm') + dd_fit['GPP']['intercept']
    # RECOm = dd_fit['RECO']['slope'] * RECOm.rename('RECOm') + dd_fit['RECO']['intercept']
    # LEm = dd_fit['LE']['slope'] * LEm.rename('LEm') + dd_fit['LE']['intercept']

    GPPm = (GPPm.rename('GPPm') - dd_fit['GPP']['intercept']) / dd_fit['GPP']['slope']
    RECOm = (RECOm.rename('RECOm') - dd_fit['RECO']['intercept']) / dd_fit['RECO']['slope']
    LEm = (LEm.rename('LEm') - dd_fit['LE']['intercept']) / dd_fit['LE']['slope']
    NEEm = (RECOm - GPPm).rename('NEEm')

    # ------------------------------------------------------------------------------
    regr = dd_out['GPP']['regr']
    pred_residue = regr.predict(nct.to_dataframe()[dd_out['GPP']['X_names'] + ['NIRv']])
    pred_residue = pd.DataFrame(pred_residue, columns = ['residue'], index = nct.to_dataframe().index).to_xarray()
    GPPmML = (GPPm + pred_residue['residue']).rename('GPPmML')
    GPPmML = GPPmML.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)

    regr = dd_out['RECO']['regr']
    pred_residue = regr.predict(nct.to_dataframe()[dd_out['RECO']['X_names'] + ['NIRv']])
    pred_residue = pd.DataFrame(pred_residue, columns = ['residue'], index = nct.to_dataframe().index).to_xarray()
    RECOmML = (RECOm + pred_residue['residue']).rename('RECOmML')
    RECOmML = RECOmML.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)

    regr = dd_out['LE']['regr']
    pred_residue = regr.predict(nct.to_dataframe()[dd_out['LE']['X_names'] + ['NIRv']])
    pred_residue = pd.DataFrame(pred_residue, columns = ['residue'], index = nct.to_dataframe().index).to_xarray()
    LEmML = (LEm + pred_residue['residue']).rename('LEmML')
    LEmML = LEmML.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)


    regr = dd_out['NEE']['regr']
    pred_residue = regr.predict(nct.to_dataframe()[dd_out['NEE']['X_names'] + ['NIRv']])
    pred_residue = pd.DataFrame(pred_residue, columns = ['residue'], index = nct.to_dataframe().index).to_xarray()
    NEEmML = (NEEm + pred_residue['residue']).rename('NEEmML')
    NEEmML = NEEmML.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
    NEEmML_mask = NEEmML.where((GPPmML >= 1e-9) & (RECOmML >= 1e-9)).to_dataset()

    GPPmML = fill_negtive_da(GPPmML.to_dataset())['GPPmML']
    RECOmML = fill_negtive_da(RECOmML.to_dataset())['RECOmML']
    NEEmML = fill_negtive_da(NEEmML.to_dataset(), nct_masked = NEEmML_mask)['NEEmML']
    # NEEmML = (RECOmML - GPPmML).rename('NEEmML')

    nco = xr.merge([GPPm, GPPmML, RECOm, RECOmML, LEm, LEmML, NEEm, NEEmML, nct['NIRv']])

    nco.to_netcdf(savefile)
    del(GPPm); del(RECOm); del(LEm); del(NEEm); del(GPPmML); del(RECOmML); del(LEmML); del(NEEmML); del(nct); del(nco)

"""## Application 0.1 deg"""

root_proj_trendy.joinpath('3_output/UFLUXv2_01deg').mkdir(exist_ok = True)

dd_out = load_pickle(root_proj_trendy.joinpath(f'3_output/UFLUXv2_model/UFLUXv2.pkl'))
dd_regr = dd_out['regr']
X_names = dd_out['X_names']
dd_fit = dd_out['scaling_factor']

def get_lue_iwue_grid(nct):
    nct = nct.copy()
    tc =  nct['temperature_2m']
    tc = tc.where(tc >= -30, -30)
    tc = tc.values
    co2 = nct['co2'].values
    patm = nct['surface_pressure'].values * 100
    vpd = nct['VPD'].values * 100

    ca = photosynthesis.calc_co2_to_ca(co2, patm)
    c3_lue, iwue = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, True, c4 = False, limitation_factors = 'wang17')
    c4_lue, _ = photosynthesis.calc_light_water_use_efficiency(tc, patm, ca, vpd, True, c4 = True, limitation_factors = 'none')

    c3_lue = xr.DataArray(c3_lue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('C3_LUE')
    c4_lue = xr.DataArray(c4_lue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('C4_LUE')
    iwue = xr.DataArray(iwue, dims = ["time", "latitude", "longitude"], coords={"time": nct.time, "latitude": nct.latitude, "longitude": nct.longitude}).rename('IWUE')
    lue = (c3_lue * (1 - nct['C4_area'].fillna(0) / 100) + c4_lue  * nct['C4_area'].fillna(0) / 100)
    return lue, iwue

def fill_negtive_da(nct):
    nct_masked = nct.where(nct >= 1e-9)
    interpolated_nct = nct_masked.interpolate_na(dim = ['longitude', 'latitude'], method="linear")
    result = interpolated_nct.where(~np.isnan(nct), np.nan)
    return result

for cnt in tqdm(range(len(df_path_era5))):
    dt = df_path_era5.index[cnt]
    p_era5 = df_path_era5.iloc[cnt, :]['path']
    sub_k = df_path_era5.iloc[cnt, :]['sub_k']
    sub_grids = subregions[sub_k]
    savefile = root_proj_trendy.joinpath('3_output/UFLUXv2_01deg').joinpath(f"{dt.year}-UFLUXv2-ECup-{sub_k}.nc")
    if savefile.exists(): continue

    p_sat = df_path_MODIS.loc[dt, 'path']

    era5 = xr.open_dataset(p_era5, engine = 'netcdf4')

    if dt.year == 2000:
        lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
    else:
        lucc = luccr.sel(time = dt)
    monthly_time = pd.date_range(
        dt.strftime('%Y-%m-%d'),
        (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
        freq = "MS"
    )
    lucc = lucc.interp(latitude = era5.latitude, longitude = era5.longitude).expand_dims(time = monthly_time).rename({'LC_Type1': 'IGBP'})
    nc_geo = nc_geor.interp(latitude = era5.latitude, longitude = era5.longitude).expand_dims(time = monthly_time)
    nc_geo['C4_area'] = nc_geo['C4_area'].fillna(0)
    co2 = co2r.sel(time = dt).interp(latitude = era5.latitude, longitude = era5.longitude).expand_dims(time = monthly_time)

    v = df_el.loc[dt.year, 'El-Nino-La-Nina']
    el_grid = xr.DataArray(np.ones((len(era5.latitude), len(era5.longitude))), dims=["latitude", "longitude"],
                           coords={"latitude": era5.latitude, "longitude": era5.longitude}) * v
    el_grid = el_grid.expand_dims(time = monthly_time).rename('El-Nino-La-Nina')

    sat = xr.open_dataset(p_sat)
    sat['Nadir_Reflectance_Band1'] /= 10000
    sat['Nadir_Reflectance_Band2'] /= 10000
    sat['NDVI'] = get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('NDVI')
    sat['NIRv'] = get_NIRv(get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']), sat['Nadir_Reflectance_Band2']).rename('NIRv')
    sat['EVI'] = get_EVI2band(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('EVI')
    sat = sat.interp(latitude = era5.latitude, longitude = era5.longitude)

    nct = xr.merge([sat, era5, lucc, nc_geo, co2, el_grid])
    del(sat); del(lucc); del(nc_geo); del(co2)
    lue, iwue = get_lue_iwue_grid(nct)

    months = (xr.DataArray(np.ones((len(era5.latitude), len(era5.longitude))), dims=["latitude", "longitude"],
    coords={"latitude": era5.latitude, "longitude": era5.longitude}) * dt.month).expand_dims(time = monthly_time).rename('MONTH')
    lons, lats = np.meshgrid(era5.longitude.data, era5.latitude.data)
    lats = xr.DataArray(lats, dims=["latitude", "longitude"], coords={"latitude": era5.latitude, "longitude": era5.longitude}).expand_dims(time = monthly_time).rename('LAT')
    lons = xr.DataArray(lons, dims=["latitude", "longitude"], coords={"latitude": era5.latitude, "longitude": era5.longitude}).expand_dims(time = monthly_time).rename('LON')
    nct = xr.merge([nct, iwue, months, lons, lats])
    # nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]))
    del(months); del(lats); del(lons); del(era5)

    GPPm = lue * nct['EVI'] * 1.25 * nct['surface_solar_radiation_downwards_sum'] * 2.3
    RECOm = respiration.calculate_ecosystem_respiration(nct['temperature_2m'], nct['NIRv'] * 15, Q10 = 1.5, Tref = 10)
    LEm = evapotranspiration.mmday2Wm2(evapotranspiration.Priestley_Taylor_JPL(nct['surface_net_radiation_sum'], nct['temperature_2m'], nct['NDVI']))

    # GPPm = dd_fit['GPP']['slope'] * GPPm.rename('GPPm') + dd_fit['GPP']['intercept']
    # RECOm = dd_fit['RECO']['slope'] * RECOm.rename('RECOm') + dd_fit['RECO']['intercept']
    # LEm = dd_fit['LE']['slope'] * LEm.rename('LEm') + dd_fit['LE']['intercept']

    GPPm = (GPPm.rename('GPPm') - dd_fit['GPP']['intercept']) / dd_fit['GPP']['slope']
    RECOm = (RECOm.rename('RECOm') - dd_fit['RECO']['intercept']) / dd_fit['RECO']['slope']
    LEm = (LEm.rename('LEm') - dd_fit['LE']['intercept']) / dd_fit['LE']['slope']
    NEEm = (RECOm - GPPm).rename('NEEm')

    # ------------------------------------------------------------------------------
    regr = dd_regr['GPP']
    pred_residue = regr.predict(nct.to_dataframe()[X_names + ['NIRv']])
    pred_residue = pd.DataFrame(pred_residue, columns = ['residue'], index = nct.to_dataframe().index).to_xarray()
    GPPmML = (GPPm + pred_residue['residue']).rename('GPPmML')
    GPPmML = GPPmML.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
    GPPmML = fill_negtive_da(GPPmML.to_dataset())['GPPmML']

    regr = dd_regr['RECO']
    pred_residue = regr.predict(nct.to_dataframe()[X_names + ['NIRv']])
    pred_residue = pd.DataFrame(pred_residue, columns = ['residue'], index = nct.to_dataframe().index).to_xarray()
    RECOmML = (RECOm + pred_residue['residue']).rename('RECOmML')
    RECOmML = RECOmML.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
    RECOmML = fill_negtive_da(RECOmML.to_dataset())['RECOmML']

    regr = dd_regr['LE']
    pred_residue = regr.predict(nct.to_dataframe()[X_names + ['NIRv']])
    pred_residue = pd.DataFrame(pred_residue, columns = ['residue'], index = nct.to_dataframe().index).to_xarray()
    LEmML = (LEm + pred_residue['residue']).rename('LEmML')
    LEmML = LEmML.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)

    NEEmML = (GPPmML - RECOmML).rename('NEEmML')

    nco = xr.merge([GPPm, GPPmML, RECOm, RECOmML, LEm, LEmML, NEEm, NEEmML]) # , nct['NIRv'], nct['IGBP']

    nco.to_netcdf(savefile)
    del(GPPm); del(RECOm); del(LEm); del(NEEm); del(GPPmML); del(RECOmML); del(LEmML); del(NEEmML); del(nct); del(nco)
    gc.collect()
    time.sleep(0.1)

"""## Sanity check"""

nct = xr.open_dataset(root_proj_trendy.joinpath('3_output/UFLUXv2').joinpath(f"2020-UFLUXv2-ECup.nc"), engine = 'netcdf4')
nct = nct.mean(dim = 'time')

nct['GPPmML'].plot(vmin = 0)

"""# Emulator v2

## Functions
"""

def get_exp_scaling_factor(daa, dab, f_fixed = None):
    r_ = daa.mean() / dab.mean()
    if r_ == 0:
        factor_ = 1
    else:
        factor_ = 10**int(np.log10(abs(r_)))

    if factor_ < 1: factor_ = 1
    if f_fixed is not None: factor_ = f_fixed
    return factor_

# get_exp_scaling_factor(np.arange(1000, 1010), np.arange(10), f_fixed = None)

"""## Prepare features"""

from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from scitbx.regress2 import regress2

savefeatures = root_proj_trendy.joinpath('2_preproc').joinpath(f'features_05deg.nc')

scale = 2
lons = np.arange(-179, 179 + scale, scale)
lats = np.arange(-89, 89 + scale, scale)
grids_oi = xr.DataArray(
    data=np.zeros([len(lons), len(lats)]),
    dims=["longitude", "latitude"],
    coords=dict(
        longitude=(["longitude"], lons),
        latitude=(["latitude"], lats),
    ),
    attrs=dict(
        description = f"{scale} grids.",
    ),
)

if savefeatures.exists():
    nc_features = xr.open_dataset(savefeatures, engine = 'netcdf4')
else:
    nc_features = []
    for dt in tqdm(df_path_MODIS.index):
        p_sat = df_path_MODIS.loc[dt, 'path']
        p_era5 = df_path_era5.loc[dt, 'path']

        if dt.year == 2000:
            lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
        else:
            lucc = luccr.sel(time = dt)
        monthly_time = pd.date_range(
            dt.strftime('%Y-%m-%d'),
            (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
            freq = "MS"
        )
        lucc = lucc.rename({'LC_Type1': 'IGBP'})
        lucc = lucc.where(~lucc['IGBP'].isin([13, 15, 16, 17]), np.nan)
        lucc = lucc.expand_dims(time = monthly_time).interp(latitude = grids_oi.latitude, longitude = grids_oi.longitude)
        nc_geo = nc_geor.interp(latitude = grids.latitude, longitude = grids.longitude).expand_dims(time = monthly_time).interp(latitude = grids_oi.latitude, longitude = grids_oi.longitude)
        nc_geo['C4_area'] = nc_geo['C4_area'].fillna(0)
        co2 = co2r.sel(time = dt).interp(latitude = grids.latitude, longitude = grids.longitude).expand_dims(time = monthly_time).interp(latitude = grids_oi.latitude, longitude = grids_oi.longitude)
        sat = xr.open_dataset(p_sat).interp(latitude = grids_oi.latitude, longitude = grids_oi.longitude)
        era5 = load_era5(p_era5, engine = 'netcdf4', drop_variables = drop_variables).interp(latitude = grids_oi.latitude, longitude = grids_oi.longitude)

        sat = sat.where((sat['Nadir_Reflectance_Band1'] != 0) & (sat['Nadir_Reflectance_Band2'] != 0))
        sat['NIRv'] = (sat['Nadir_Reflectance_Band2'] - sat['Nadir_Reflectance_Band1']) / (sat['Nadir_Reflectance_Band2'] + sat['Nadir_Reflectance_Band1']) * sat['Nadir_Reflectance_Band2'] / 1e4
        sat = sat[['NIRv']]

        nct = xr.merge([sat, era5, lucc, nc_geo, co2])
        del(sat, era5, lucc, nc_geo, co2)
        nct = nct.where((nct['NIRv'] > 1e-9) & (~nct['IGBP'].isnull()), np.nan)
        nc_features.append(nct)
    nc_features = xr.merge(nc_features)
    del(nct)
    gc.collect()
    nc_features.to_netcdf(savefeatures)

# ==============================================================================
months = xr.DataArray(
    np.tile(nc_features['time.month'], (len(nc_features.longitude), len(nc_features.latitude), 1)).T,
    coords=nc_features.coords,
    dims=nc_features.dims,
    name="month"
)
years = xr.DataArray(
    np.tile(nc_features['time.year'], (len(nc_features.longitude), len(nc_features.latitude), 1)).T,
    coords=nc_features.coords,
    dims=nc_features.dims,
    name="year"
)

lats = xr.DataArray(
    np.tile(nc_features.latitude, (len(nc_features.longitude), len(nc_features.time), 1)).T.swapaxes(0, 1),
    coords=nc_features.coords,
    dims=nc_features.dims,
    name="lat"
)

lons = xr.DataArray(
    np.tile(nc_features.longitude, (len(nc_features.latitude), len(nc_features.time), 1)).swapaxes(0, 1),
    coords=nc_features.coords,
    dims=nc_features.dims,
    name="lon"
)

nc_features = xr.merge([nc_features, months, years, lats, lons])
del(months, years, lats, lons)

# df_path_UFLUXv2 = []
# for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2').glob('*.nc'):
#     df_path_UFLUXv2.append([pd.to_datetime(p.stem.split('-')[0], format = '%Y'), p])
# df_path_UFLUXv2 = pd.DataFrame(df_path_UFLUXv2, columns = ['time', 'path']).set_index('time')

# ufluxv2 = []
# for dt in df_path_UFLUXv2.index:
#     p = df_path_UFLUXv2.loc[dt, 'path']
#     nct = xr.open_dataset(p, engine = 'netcdf4')[['GPPmML', 'RECOmML', 'LEmML']].rename({'GPPmML': 'gpp', 'RECOmML': 'reco', 'LEmML': 'le'})
#     nct['gpp'] = nct['gpp'].where(nct['gpp'] > 1e-9, np.nan)
#     nct['reco'] = nct['reco'].where(nct['reco'] > 1e-9, np.nan)
#     nct = nct.interp(latitude = grids_oi.latitude, longitude = grids_oi.longitude)
#     ufluxv2.append(nct); del(nct)
# ufluxv2 = xr.merge(ufluxv2)
# gc.collect()
# # ufluxv2['gpp'].drop_vars('spatial_ref').mean(dim = ['latitude', 'longitude']).to_dataframe().resample('1YS').mean().plot()

savefile_gpp = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_gpp_05deg_monthly_2000-2022.nc')
savefile_ra = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_ra_05deg_monthly_2000-2022.nc')
savefile_rh = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_rh_05deg_monthly_2000-2022.nc')

gpp_in = xr.open_dataset(savefile_gpp,  engine="netcdf4")['gpp']
gpp_in = gpp_in * 1000 * 86400
gpp_in = gpp_in.where(gpp_in > 1e-9, np.nan)
gpp_in = gpp_in.interp(latitude = grids_oi.latitude, longitude = grids_oi.longitude)

ra_in = xr.open_dataset(savefile_ra,  engine="netcdf4")
ra_in = ra_in * 1000 * 86400
ra_in = ra_in.where(ra_in > 1e-9, np.nan)
ra_in = ra_in.interp(latitude = grids_oi.latitude, longitude = grids_oi.longitude)

rh_in = xr.open_dataset(savefile_rh,  engine="netcdf4")
rh_in = rh_in * 1000 * 86400
rh_in = rh_in.where(rh_in > 1e-9, np.nan)
rh_in = rh_in.interp(latitude = grids_oi.latitude, longitude = grids_oi.longitude)

reco_in = (ra_in['ra'] + rh_in['rh']).rename('reco')
del(ra_in); del(rh_in)

trendy = xr.merge([gpp_in, reco_in])
del(reco_in, gpp_in)

# nc_month = xr.merge([nc_features, ufluxv2])
nc_month = xr.merge([nc_features, trendy])

pivot_vars = ['gpp', 'reco', 'temperature_2m', 'VPD', 'soil_temperature_level_1', 'volumetric_soil_water_layer_1']
nc_year = [nc_month[[vname for vname in list(nc_month.data_vars) if vname not in pivot_vars]].resample(time = '1YS').mean()]
for vname in pivot_vars:
    nc_year.append(xr.merge([d.rename(f'{vname}{m}').assign_coords(time = [pd.to_datetime(y, format = '%Y') for y in d['time.year'].data]) for m, d in nc_month[vname].groupby('time.month')]))
nc_year = xr.merge(nc_year)

"""## Training"""

# ['NIRv', 'temperature_2m', 'soil_temperature_level_1', 'volumetric_soil_water_layer_1',
#  'surface_solar_radiation_downwards_sum', 'surface_thermal_radiation_downwards_sum', 'surface_pressure',
#  'total_precipitation_sum', 'VPD', 'surface_net_radiation_sum', 'IGBP', 'DEM',
#  'KOPPEN', 'C4_area', 'co2', 'month', 'year', 'lat', 'lon', 'gpp', 'reco', 'le']

X_names_month = [
    'NIRv', 'temperature_2m', 'soil_temperature_level_1',
    'volumetric_soil_water_layer_1',
    'surface_solar_radiation_downwards_sum', 'surface_thermal_radiation_downwards_sum',
    'surface_pressure', 'total_precipitation_sum', 'surface_net_radiation_sum',
    'VPD', 'IGBP', 'DEM', 'KOPPEN', 'C4_area', 'co2',
    'gpp', 'reco',
    'year', 'month', 'lon', 'lat'
    # 'year', 'month'
]
X_names_year = [
    'NIRv', 'surface_solar_radiation_downwards_sum', 'surface_thermal_radiation_downwards_sum',
    'surface_pressure', 'total_precipitation_sum', 'surface_net_radiation_sum', 'IGBP', 'DEM', 'KOPPEN',
    'C4_area', 'co2', 'gpp1', 'gpp2', 'gpp3', 'gpp4', 'gpp5', 'gpp6', 'gpp7', 'gpp8', 'gpp9', 'gpp10', 'gpp11', 'gpp12',
    'reco1', 'reco2', 'reco3', 'reco4', 'reco5', 'reco6', 'reco7', 'reco8', 'reco9', 'reco10', 'reco11', 'reco12',
    'temperature_2m1', 'temperature_2m2', 'temperature_2m3', 'temperature_2m4', 'temperature_2m5', 'temperature_2m6',
    'temperature_2m7', 'temperature_2m8', 'temperature_2m9', 'temperature_2m10', 'temperature_2m11', 'temperature_2m12',
    'VPD1', 'VPD2', 'VPD3', 'VPD4', 'VPD5', 'VPD6', 'VPD7', 'VPD8', 'VPD9', 'VPD10', 'VPD11', 'VPD12',
    'soil_temperature_level_11', 'soil_temperature_level_12', 'soil_temperature_level_13', 'soil_temperature_level_14',
    'soil_temperature_level_15', 'soil_temperature_level_16', 'soil_temperature_level_17', 'soil_temperature_level_18',
    'soil_temperature_level_19', 'soil_temperature_level_110', 'soil_temperature_level_111', 'soil_temperature_level_112',
    'volumetric_soil_water_layer_11', 'volumetric_soil_water_layer_12', 'volumetric_soil_water_layer_13',
    'volumetric_soil_water_layer_14', 'volumetric_soil_water_layer_15', 'volumetric_soil_water_layer_16',
    'volumetric_soil_water_layer_17', 'volumetric_soil_water_layer_18', 'volumetric_soil_water_layer_19',
    'volumetric_soil_water_layer_110', 'volumetric_soil_water_layer_111', 'volumetric_soil_water_layer_112',
    'year', 'month', 'lon', 'lat'
    # 'year', 'month'
]

root_proj_trendy.joinpath(f'3_output/UFLUXv2e_model').mkdir(exist_ok = True)

for cname in tqdm(['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood'] + ['npp', 'nbp', 'ra', 'rh', 'fGrazing']):
    savefile = root_proj_trendy.joinpath(f'3_output/UFLUXv2e_model/{cname}.pkl')
    # if savefile.exists(): continue
    if cname in ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']:
        cfreq = 'yearly'
    elif cname in ['gpp', 'npp', 'nbp', 'ra', 'rh', 'fGrazing']:
        cfreq = 'monthly'
    else:
        raise ValueError('Unknown cname')
        cfreq = None
    ncc_in = xr.open_dataset(root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_{cfreq}_2000-2022.nc'),  engine="netcdf4")
    ncc_in = ncc_in.interp(latitude = grids_oi.latitude, longitude = grids_oi.longitude)
    ncc_in = ncc_in.where(ncc_in[cname] != 0, np.nan)

    dd_out = {}
    for label in [cname, cname + '_25th', cname + '_75th']:
        ncc = ncc_in[[label]]
        if cfreq == 'monthly':
            X_names = X_names_month
            ncc = ncc * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
            if cname in ['ra', 'rh']:
                f_scaling = get_exp_scaling_factor(ncc[label], trendy['reco'], f_fixed = 1)
                ncc = ncc / f_scaling
                ncc = (ncc[label] / trendy['reco']).rename(label)
            else:
                f_scaling = get_exp_scaling_factor(ncc[label], trendy['gpp'], f_fixed = 1)
                ncc = ncc / f_scaling
                ncc = (ncc[label] / trendy['gpp']).rename(label)
            ncc = xr.merge([ncc, nc_month[X_names]])
        else:
            X_names = X_names_year
            ncc = ncc * 1000 # kg m-2 => g m-2
            f_scaling = get_exp_scaling_factor(ncc[label], trendy['gpp'], f_fixed = 1)
            ncc = ncc / f_scaling
            ncc = (ncc[label] / trendy['gpp'].resample(time = '1YS').mean()).rename(label)
            ncc = xr.merge([ncc, nc_year[X_names]])

        lats, lons = np.meshgrid(ncc.latitude, ncc.longitude)
        pixels = list(zip(lats.ravel(), lons.ravel())); del(lats, lons)
        pixels = pd.DataFrame(pixels, columns = ['latitude', 'longitude'])
        test_pixels = pixels.sample(frac = 0.33, random_state = 42)
        train_pixels = pixels.loc[pixels.index.difference(test_pixels.index), :]
        assert list(train_pixels.index.intersection(test_pixels.index)) == [], 'ERROR: overlaps between training/test indexes'
        train_pixels = train_pixels.set_index(['latitude', 'longitude']).index
        test_pixels = test_pixels.set_index(['latitude', 'longitude']).index
        # ----------------------------------------------------------------------
        dfc = ncc.to_dataframe()[[label] + X_names].dropna().reset_index().set_index(['latitude', 'longitude'])
        df_train = dfc.loc[dfc.index.intersection(train_pixels), :]
        df_test = dfc.loc[dfc.index.intersection(test_pixels), :]
        X_train = df_train[X_names]; y_train = df_train[[label]]
        X_test = df_test[X_names]; y_test = df_test[[label]]
        del(df_train, df_test)
        # ----------------------------------------------------------------------
        xgb_params = {
            "objective": "reg:squarederror",
            "random_state": 0,
            'seed': 0,
        }

        # regr = RandomForestRegressor()
        regr = XGBRegressor(**xgb_params)
        regr.fit(X_train, y_train)
        y_pred = regr.predict(X_test)
        # print(regr.score(X_train, y_train))
        # print(regr.score(X_test, y_test))

        dfo = y_test.copy()#.to_frame()
        dfo.columns = ['truth']
        dfo['pred'] = y_pred

        eval_res = regress2(dfo['truth'].values, dfo['pred'].values)
        rvalue = eval_res['r']
        slope = eval_res['slope']
        intercept = eval_res['intercept']
        mae = (dfo['pred'] - dfo['truth']).abs().mean()
        # print(eval_res)

        r2 = eval_res['r']**2
        rmse = get_rmse(dfo['truth'].values, dfo['pred'].values)
        dfm = pd.DataFrame([[label, regr.score(X_test, y_test), r2, rmse, slope, intercept, dfo['truth'].mean(), mae]], columns = ['name', 'R2', 'r2', 'RMSE', 'slope', 'intercept', 'Mean', 'MAE']).set_index('name')
        df_feat_imp = pd.DataFrame(regr.feature_importances_, columns = [label], index = X_names).sort_values(by = label, ascending = False)
        # dfo.plot.scatter(x = 'truth', y = 'pred')

        dd_ot = {
            'trained_model': regr,
            'X_names': X_names,
            'X_test': X_test,
            'dfo': dfo,
            'feature_importance': df_feat_imp,
            'performance_metrics': dfm,
            'scaling_factor': f_scaling
        }
        del(f_scaling)
        dd_out[label] = dd_ot
    dump_pickle(dd_out, savefile)
    gc.collect()
    time.sleep(0.1)

"""## Application"""

# UFLUXv2:
df_path_UFLUXv2 = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2').glob('*.nc'):
    df_path_UFLUXv2.append([pd.to_datetime(p.stem.split('-')[0], format = '%Y'), p])
df_path_UFLUXv2 = pd.DataFrame(df_path_UFLUXv2, columns = ['time', 'path']).set_index('time')
# ------------------------------------------------------------------------------
# UFLUXv1:
df_path_UFLUXv1 = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv1').glob('*.nc'):
    df_path_UFLUXv1.append([pd.to_datetime(p.stem.split('-')[0], format = '%Y'), p])
df_path_UFLUXv1 = pd.DataFrame(df_path_UFLUXv1, columns = ['time', 'path']).set_index('time')

dd_train = {}; dfm = []
# for cname in tqdm(['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood'] + ['gpp', 'npp', 'nbp', 'ra', 'rh', 'fGrazing']):
for cname in tqdm(['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood'] + ['npp', 'nbp', 'ra', 'rh', 'fGrazing']):
    dd_out = load_pickle(root_proj_trendy.joinpath(f'3_output/UFLUXv2e_model/{cname}.pkl'))
    regr_list = [dd_out[cname + '_25th']['trained_model'], dd_out[cname]['trained_model'], dd_out[cname + '_75th']['trained_model']]
    X_names = dd_out[cname]['X_names']
    f_scaling = [dd_out[cname + '_25th']['scaling_factor'], dd_out[cname]['scaling_factor'], dd_out[cname + '_75th']['scaling_factor']]
    # print(cname, f_scaling)
    dd_train[cname] = [regr_list, X_names, f_scaling]
    dfm.append(dd_out[cname]['performance_metrics'])
    del(dd_out)

dfm = pd.concat(dfm, axis = 0)
dfm

# UFLUXv2
root_proj_trendy.joinpath(f'3_output/UFLUXv2e').mkdir(exist_ok = True)

for dt in tqdm(df_path_MODIS.index):
    p_sat = df_path_MODIS.loc[dt, 'path']
    p_era5 = df_path_era5.loc[dt, 'path']
    p_ufluxv2 = df_path_UFLUXv2.loc[dt, 'path']

    ufluxv2 = xr.open_dataset(p_ufluxv2, engine = 'netcdf4')[['GPPmML', 'RECOmML']].rename({'GPPmML': 'gpp', 'RECOmML': 'reco'})
    ufluxv2 = ufluxv2.where((ufluxv2['gpp'] > 1e-9) & (ufluxv2['reco'] > 1e-9), np.nan)

    if dt.year == 2000:
        lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
    else:
        lucc = luccr.sel(time = dt)
    monthly_time = pd.date_range(
        dt.strftime('%Y-%m-%d'),
        (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
        freq = "MS"
    )
    lucc = lucc.expand_dims(time = monthly_time)#.rename({'LC_Type1': 'IGBP'})
    lucc = lucc.rename({'LC_Type1': 'IGBP'})
    lucc = lucc.where(~lucc['IGBP'].isin([13, 15, 16, 17]), np.nan)
    nc_geo = nc_geor.interp(latitude = ufluxv2.latitude, longitude = ufluxv2.longitude).expand_dims(time = monthly_time)
    nc_geo['C4_area'] = nc_geo['C4_area'].fillna(0)
    co2 = co2r.sel(time = dt).interp(latitude = ufluxv2.latitude, longitude = ufluxv2.longitude).expand_dims(time = monthly_time)
    sat = xr.open_dataset(p_sat, engine = 'netcdf4')
    sat['Nadir_Reflectance_Band1'] /= 10000
    sat['Nadir_Reflectance_Band2'] /= 10000
    sat['NDVI'] = get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('NDVI')
    sat['NIRv'] = get_NIRv(get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']), sat['Nadir_Reflectance_Band2']).rename('NIRv')
    sat['EVI'] = get_EVI2band(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('EVI')
    sat = sat[['NIRv']]

    era5 = load_era5(p_era5, engine = 'netcdf4', drop_variables = drop_variables)
    assert (sat.latitude.values == ufluxv2.latitude.values).all() & (era5.latitude.values == ufluxv2.latitude.values).all(), 'ERROR: dims not match'

    months = (xr.DataArray(np.ones((len(ufluxv2.latitude), len(ufluxv2.longitude))), dims=["latitude", "longitude"], coords={"latitude": ufluxv2.latitude, "longitude": ufluxv2.longitude}) * dt.month).expand_dims(time = monthly_time).rename('month')
    years = (xr.DataArray(np.ones((len(ufluxv2.latitude), len(ufluxv2.longitude))), dims=["latitude", "longitude"], coords={"latitude": ufluxv2.latitude, "longitude": ufluxv2.longitude}) * dt.year).expand_dims(time = monthly_time).rename('year')
    lons, lats = np.meshgrid(ufluxv2.longitude.data, ufluxv2.latitude.data)
    lats = xr.DataArray(lats, dims=["latitude", "longitude"], coords={"latitude": ufluxv2.latitude, "longitude": ufluxv2.longitude}).expand_dims(time = monthly_time).rename('lat')
    lons = xr.DataArray(lons, dims=["latitude", "longitude"], coords={"latitude": ufluxv2.latitude, "longitude": ufluxv2.longitude}).expand_dims(time = monthly_time).rename('lon')

    nc_month = xr.merge([ufluxv2, sat, era5, lucc, co2, nc_geo, months, years, lats, lons])
    del(ufluxv2, sat, era5, lucc, co2, nc_geo, months, years, lats, lons)
    nc_month['gpp'] = nc_month['gpp'].where(nc_month['gpp'] >= 0, 0)
    nc_month['reco'] = nc_month['reco'].where(nc_month['reco'] >= 0, 0)

    pivot_vars = ['gpp', 'reco', 'temperature_2m', 'VPD', 'soil_temperature_level_1', 'volumetric_soil_water_layer_1']
    nc_year = [nc_month[[vname for vname in list(nc_month.data_vars) if vname not in pivot_vars]].resample(time = '1YS').mean()]
    for vname in pivot_vars:
        nc_year.append(xr.merge([d.rename(f'{vname}{m}').assign_coords(time = [dt]) for m, d in nc_month[vname].groupby('time.month')]))
    nc_year = xr.merge(nc_year)
    if dt.year == 2000: nc_year['gpp1'] = nc_year['gpp2']

    # ==========================================================================
    for cname in dd_train.keys():
        savefile = root_proj_trendy.joinpath(f'3_output/UFLUXv2e/{dt.year}-{cname}-TRENDYe.nc')
        if savefile.exists(): continue
        regr_list, X_names, f_scalings = dd_train[cname]

        if cname in ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']:
            nct = nc_year
            cfreq = 'yearly'
            base_ = nc_month[['gpp', 'reco']].resample(time = '1YS').mean()
        elif cname in ['npp', 'nbp', 'ra', 'rh', 'fGrazing']:
            nct = nc_month
            cfreq = 'monthly'
            base_ = nc_month[['gpp', 'reco']]
        else:
            raise ValueError('Unknown cname')
        regr = regr_list[0]
        pred = regr.predict(nct[X_names].to_dataframe()[X_names])
        pred_25 = pd.DataFrame(pred, columns = [cname + '_25th'], index = nct.to_dataframe().index).to_xarray()
        pred_25 = pred_25 * f_scalings[0]

        regr = regr_list[1]
        pred = regr.predict(nct[X_names].to_dataframe()[X_names])
        pred_mean = pd.DataFrame(pred, columns = [cname], index = nct.to_dataframe().index).to_xarray()
        pred_mean = pred_mean * f_scalings[1]

        regr = regr_list[2]
        pred = regr.predict(nct[X_names].to_dataframe()[X_names])
        pred_75 = pd.DataFrame(pred, columns = [cname + '_75th'], index = nct.to_dataframe().index).to_xarray()
        pred_75 = pred_75 * f_scalings[2]

        nco = xr.merge([pred_25, pred_mean, pred_75]); del(pred_25, pred_mean, pred_75)
        if cname in ['ra', 'rh']:
            nco = nco * base_['reco']
        else:
            nco = nco * base_['gpp']
        nco = nco.where(~nct['IGBP'].isin([13, 15, 16, 17])).where(~nct['NIRv'].isnull(), np.nan).where(~nct['IGBP'].isnull(), np.nan)

        if cfreq == 'yearly': nco = xr.merge([nct['IGBP'], nco])
        nco.to_netcdf(savefile)
        del(nco)
        gc.collect()

# UFLUXv1:
root_proj_trendy.joinpath(f'3_output/UFLUXv1e').mkdir(exist_ok = True)

for dt in tqdm(df_path_MODIS.index):
    p_sat = df_path_MODIS.loc[dt, 'path']
    p_era5 = df_path_era5.loc[dt, 'path']
    p_ufluxv1 = df_path_UFLUXv1.loc[dt, 'path']

    ufluxv1 = xr.open_dataset(p_ufluxv1, engine = 'netcdf4')[['GPPmML', 'RECOmML']].rename({'GPPmML': 'gpp', 'RECOmML': 'reco'})
    ufluxv1 = ufluxv1.where((ufluxv1['gpp'] > 1e-9) & (ufluxv1['reco'] > 1e-9), np.nan)
    # For UFLUXv1 only:
    ufluxv1 = xr.Dataset(
        {
            "gpp": (["time", "latitude", "longitude"], ufluxv1['gpp'].data.swapaxes(0, 2).swapaxes(1, 2)),
            "reco": (["time", "latitude", "longitude"], ufluxv1['reco'].data.swapaxes(0, 2).swapaxes(1, 2)),
        },
        coords={
            "time": ufluxv1.time,
            "latitude": ufluxv1.latitude,
            "longitude": ufluxv1.longitude
        }
    )

    if dt.year == 2000:
        lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
    else:
        lucc = luccr.sel(time = dt)
    monthly_time = pd.date_range(
        dt.strftime('%Y-%m-%d'),
        (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
        freq = "MS"
    )
    lucc = lucc.expand_dims(time = monthly_time)#.rename({'LC_Type1': 'IGBP'})
    lucc = lucc.rename({'LC_Type1': 'IGBP'})
    lucc = lucc.where(~lucc['IGBP'].isin([13, 15, 16, 17]), np.nan)
    nc_geo = nc_geor.interp(latitude = ufluxv1.latitude, longitude = ufluxv1.longitude).expand_dims(time = monthly_time)
    nc_geo['C4_area'] = nc_geo['C4_area'].fillna(0)
    co2 = co2r.sel(time = dt).interp(latitude = ufluxv1.latitude, longitude = ufluxv1.longitude).expand_dims(time = monthly_time)
    sat = xr.open_dataset(p_sat, engine = 'netcdf4')
    sat['Nadir_Reflectance_Band1'] /= 10000
    sat['Nadir_Reflectance_Band2'] /= 10000
    sat['NDVI'] = get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('NDVI')
    sat['NIRv'] = get_NIRv(get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']), sat['Nadir_Reflectance_Band2']).rename('NIRv')
    sat['EVI'] = get_EVI2band(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('EVI')
    sat = sat[['NIRv']]

    era5 = load_era5(p_era5, engine = 'netcdf4', drop_variables = drop_variables)
    assert (sat.latitude.values == ufluxv1.latitude.values).all() & (era5.latitude.values == ufluxv1.latitude.values).all(), 'ERROR: dims not match'

    months = (xr.DataArray(np.ones((len(ufluxv1.latitude), len(ufluxv1.longitude))), dims=["latitude", "longitude"], coords={"latitude": ufluxv1.latitude, "longitude": ufluxv1.longitude}) * dt.month).expand_dims(time = monthly_time).rename('month')
    years = (xr.DataArray(np.ones((len(ufluxv1.latitude), len(ufluxv1.longitude))), dims=["latitude", "longitude"], coords={"latitude": ufluxv1.latitude, "longitude": ufluxv1.longitude}) * dt.year).expand_dims(time = monthly_time).rename('year')
    lons, lats = np.meshgrid(ufluxv1.longitude.data, ufluxv1.latitude.data)
    lats = xr.DataArray(lats, dims=["latitude", "longitude"], coords={"latitude": ufluxv1.latitude, "longitude": ufluxv1.longitude}).expand_dims(time = monthly_time).rename('lat')
    lons = xr.DataArray(lons, dims=["latitude", "longitude"], coords={"latitude": ufluxv1.latitude, "longitude": ufluxv1.longitude}).expand_dims(time = monthly_time).rename('lon')

    nc_month = xr.merge([ufluxv1, sat, era5, lucc, co2, nc_geo, months, years, lats, lons])
    del(ufluxv1, sat, era5, lucc, co2, nc_geo, months, years, lats, lons)
    nc_month['gpp'] = nc_month['gpp'].where(nc_month['gpp'] >= 0, 0)
    nc_month['reco'] = nc_month['reco'].where(nc_month['reco'] >= 0, 0)

    pivot_vars = ['gpp', 'reco', 'temperature_2m', 'VPD', 'soil_temperature_level_1', 'volumetric_soil_water_layer_1']
    nc_year = [nc_month[[vname for vname in list(nc_month.data_vars) if vname not in pivot_vars]].resample(time = '1YS').mean()]
    for vname in pivot_vars:
        nc_year.append(xr.merge([d.rename(f'{vname}{m}').assign_coords(time = [dt]) for m, d in nc_month[vname].groupby('time.month')]))
    nc_year = xr.merge(nc_year)
    if dt.year == 2000: nc_year['gpp1'] = nc_year['gpp2']

    # ==========================================================================
    for cname in dd_train.keys():
        savefile = root_proj_trendy.joinpath(f'3_output/UFLUXv1e/{dt.year}-{cname}-TRENDYe.nc')
        if savefile.exists(): continue
        regr_list, X_names, f_scalings = dd_train[cname]

        if cname in ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']:
            nct = nc_year
            cfreq = 'yearly'
            base_ = nc_month[['gpp', 'reco']].resample(time = '1YS').mean()
        elif cname in ['npp', 'nbp', 'ra', 'rh', 'fGrazing']:
            nct = nc_month
            cfreq = 'monthly'
            base_ = nc_month[['gpp', 'reco']]
        else:
            raise ValueError('Unknown cname')
        regr = regr_list[0]
        pred = regr.predict(nct[X_names].to_dataframe()[X_names])
        pred_25 = pd.DataFrame(pred, columns = [cname + '_25th'], index = nct.to_dataframe().index).to_xarray()
        pred_25 = pred_25 * f_scalings[0]

        regr = regr_list[1]
        pred = regr.predict(nct[X_names].to_dataframe()[X_names])
        pred_mean = pd.DataFrame(pred, columns = [cname], index = nct.to_dataframe().index).to_xarray()
        pred_mean = pred_mean * f_scalings[1]

        regr = regr_list[2]
        pred = regr.predict(nct[X_names].to_dataframe()[X_names])
        pred_75 = pd.DataFrame(pred, columns = [cname + '_75th'], index = nct.to_dataframe().index).to_xarray()
        pred_75 = pred_75 * f_scalings[2]

        nco = xr.merge([pred_25, pred_mean, pred_75]); del(pred_25, pred_mean, pred_75)
        if cname in ['ra', 'rh']:
            nco = nco * base_['reco']
        else:
            nco = nco * base_['gpp']
        nco = nco.where(~nct['IGBP'].isin([0, 13, 15, 16, 17])).where(~nct['NIRv'].isnull(), np.nan).where(~nct['IGBP'].isnull(), np.nan)

        if cfreq == 'yearly': nco = xr.merge([nct['IGBP'], nco])
        nco.to_netcdf(savefile)
        del(nco)
        gc.collect()

# if root_proj_trendy.joinpath(f'3_output/UFLUXv2e_temp').exists():
#     root_proj_trendy.joinpath(f'3_output/UFLUXv2e').mkdir(exist_ok = True)
#     df_ufluxv2e_path = []
#     for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2e_temp').glob('*.nc'):
#         year, cname, _ = p.stem.split('-')
#         df_ufluxv2e_path.append([cname, pd.to_datetime(year, format = '%Y'), p])
#     df_ufluxv2e_path = pd.DataFrame(df_ufluxv2e_path, columns = ['cname', 'year', 'path']).pivot(index = 'year', columns = 'cname').droplevel(0, axis = 1)

#     for dt in df_ufluxv2e_path.index:
#         savepool = root_proj_trendy.joinpath(f'3_output/UFLUXv2e').joinpath(f'{dt.year}-cPool-TRENDYe.nc')
#         if not savepool.exists():
#             nco_pool = xr.merge([xr.open_dataset(p, engine = 'netcdf4') for p in df_ufluxv2e_path.loc[dt, ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']]])
#             nco_pool.to_netcdf(savepool)
#             del(nco_pool)
#         saveflux = root_proj_trendy.joinpath(f'3_output/UFLUXv2e').joinpath(f'{dt.year}-cFlux-TRENDYe.nc')
#         if not saveflux.exists():
#             nco_flux = xr.merge([xr.open_dataset(p, engine = 'netcdf4') for p in df_ufluxv2e_path.loc[dt, ['npp', 'nbp', 'ra', 'rh', 'fGrazing']]])
#             nco_flux.to_netcdf(saveflux)
#             del(nco_flux)
#         gc.collect()

"""## Sanity check"""

nco.mean(dim = 'time')['cVeg'].plot(vmin = 0)

# ncc_in = xr.open_dataset(root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_yearly_2000-2022.nc'),  engine="netcdf4")
# ncc_in = ncc_in.where(ncc_in[cname] != 0, np.nan)
# ncc = ncc_in.interp(latitude = nco.latitude, longitude = nco.longitude)

ncc['cVeg'].mean(dim = 'time').plot(vmin = 0)

df_path = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2e').glob('*.nc'):
    cname, year, _ = p.stem.split('-')
    df_path.append([cname, pd.to_datetime(year, format = '%Y'), p])
df_path = pd.DataFrame(df_path, columns = ['cname', 'year', 'path']).pivot(index = 'year', columns = 'cname').droplevel(0, axis = 1)

xr.open_dataset(df_path['cWood'][0]).mean(dim = 'time')['cWood'].rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).plot(vmin = 0)

"""## Applicaiton 0.1 deg"""

df_path_UFLUXv2 = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2_01deg').glob('*.nc'):
    sub_k = f"{p.stem.split('-')[-1]}"
    df_path_UFLUXv2.append([pd.to_datetime(p.stem.split('-')[0], format = '%Y'), sub_k, p])
df_path_UFLUXv2 = pd.DataFrame(df_path_UFLUXv2, columns = ['time', 'sub_k', 'path']).set_index('time')

dd_train = {}; dfm = []
for cname in tqdm(['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood'] + ['gpp', 'npp', 'nbp', 'ra', 'rh', 'fGrazing']):
    dd_out = load_pickle(root_proj_trendy.joinpath(f'3_output/UFLUXv2e_model/{cname}.pkl'))
    regr_list = [dd_out[cname + '_25th']['trained_model'], dd_out[cname]['trained_model'], dd_out[cname + '_75th']['trained_model']]
    X_names = dd_out[cname]['X_names']
    dd_train[cname] = [regr_list, X_names]
    dfm.append(dd_out[cname]['performance_metrics'])
    del(dd_out)

dfm = pd.concat(dfm, axis = 0)
dfm

df_path_ufluxv2e_01deg = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2e_01deg_temp').glob('*.nc'):
    year, cname, sub_k, _ = p.stem.split('-')
    df_path_ufluxv2e_01deg.append([cname, pd.to_datetime(year, format = '%Y'), sub_k, p])
df_path_ufluxv2e_01deg = pd.DataFrame(df_path_ufluxv2e_01deg, columns = ['cname', 'year', 'sub_k', 'path'])#.set_index(['cname', 'year'])
df_path_ufluxv2e_01deg = df_path_ufluxv2e_01deg.pivot(index = ['year', 'cname'], columns = 'sub_k')
df_path_ufluxv2e_01deg.columns = df_path_ufluxv2e_01deg.columns.droplevel(0)
df_path_ufluxv2e_01deg

root_proj_trendy.joinpath(f'3_output/UFLUXv2e_01deg_temp').mkdir(exist_ok = True)

for cnt in tqdm(range(len(df_path_era5))):
    dt = df_path_era5.index[cnt]
    p_era5 = df_path_era5.iloc[cnt, :]['path']
    sub_k = df_path_era5.iloc[cnt, :]['sub_k']
    sub_grids = subregions[sub_k]
    # --------------------------------------------------------------------------
    if sub_k in df_path_ufluxv2e_01deg.columns:
        df_finished = df_path_ufluxv2e_01deg[df_path_ufluxv2e_01deg.index.get_level_values('year') == dt][sub_k]
        finished = list(df_finished.dropna().index.get_level_values('cname'))
        if len(finished) == len(list(dd_train.keys())): continue
    # --------------------------------------------------------------------------

    p_sat = df_path_MODIS.loc[dt, 'path']
    p_ufluxv2 = df_path_UFLUXv2[df_path_UFLUXv2['sub_k'] == sub_k].loc[dt, 'path']

    ufluxv2 = xr.open_dataset(p_ufluxv2, engine = 'netcdf4')[['GPPmML', 'RECOmML']].rename({'GPPmML': 'gpp', 'RECOmML': 'reco'})

    if dt.year == 2000:
        lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
    else:
        lucc = luccr.sel(time = dt)
    monthly_time = pd.date_range(
        dt.strftime('%Y-%m-%d'),
        (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
        freq = "MS"
    )
    lucc = lucc.expand_dims(time = monthly_time)#.rename({'LC_Type1': 'IGBP'})
    lucc = lucc.rename({'LC_Type1': 'IGBP'})
    lucc = lucc.where(~lucc['IGBP'].isin([13, 15, 16, 17]), np.nan)
    lucc = lucc.interp(latitude = ufluxv2.latitude, longitude = ufluxv2.longitude)
    nc_geo = nc_geor.interp(latitude = ufluxv2.latitude, longitude = ufluxv2.longitude).expand_dims(time = monthly_time)
    nc_geo['C4_area'] = nc_geo['C4_area'].fillna(0)
    co2 = co2r.sel(time = dt).interp(latitude = ufluxv2.latitude, longitude = ufluxv2.longitude).expand_dims(time = monthly_time)
    sat = xr.open_dataset(p_sat, engine = 'netcdf4')
    sat['Nadir_Reflectance_Band1'] /= 10000
    sat['Nadir_Reflectance_Band2'] /= 10000
    sat['NDVI'] = get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('NDVI')
    sat['NIRv'] = get_NIRv(get_NDVI(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']), sat['Nadir_Reflectance_Band2']).rename('NIRv')
    sat['EVI'] = get_EVI2band(sat['Nadir_Reflectance_Band1'], sat['Nadir_Reflectance_Band2']).rename('EVI')
    sat = sat[['NIRv']]
    sat = sat.interp(latitude = ufluxv2.latitude, longitude = ufluxv2.longitude)

    era5 = xr.open_dataset(p_era5, engine = 'netcdf4')
    # assert all(sub_grids.latitude == era5.latitude) & all(sub_grids.longitude == era5.longitude), 'ERROR: coordinates not matching'

    assert (sat.latitude.values == ufluxv2.latitude.values).all() & (era5.latitude.values == ufluxv2.latitude.values).all(), 'ERROR: dims not match'

    months = (xr.DataArray(np.ones((len(ufluxv2.latitude), len(ufluxv2.longitude))), dims=["latitude", "longitude"], coords={"latitude": ufluxv2.latitude, "longitude": ufluxv2.longitude}) * dt.month).expand_dims(time = monthly_time).rename('month')
    years = (xr.DataArray(np.ones((len(ufluxv2.latitude), len(ufluxv2.longitude))), dims=["latitude", "longitude"], coords={"latitude": ufluxv2.latitude, "longitude": ufluxv2.longitude}) * dt.year).expand_dims(time = monthly_time).rename('year')
    lons, lats = np.meshgrid(ufluxv2.longitude.data, ufluxv2.latitude.data)
    lats = xr.DataArray(lats, dims=["latitude", "longitude"], coords={"latitude": ufluxv2.latitude, "longitude": ufluxv2.longitude}).expand_dims(time = monthly_time).rename('lat')
    lons = xr.DataArray(lons, dims=["latitude", "longitude"], coords={"latitude": ufluxv2.latitude, "longitude": ufluxv2.longitude}).expand_dims(time = monthly_time).rename('lon')

    nc_month = xr.merge([ufluxv2, sat, era5, lucc, co2, nc_geo, months, years, lats, lons])
    del(ufluxv2, sat, era5, lucc, co2, nc_geo, months, years, lats, lons)
    nc_month['gpp'] = nc_month['gpp'].where(nc_month['gpp'] >= 0, 0)
    nc_month['reco'] = nc_month['reco'].where(nc_month['reco'] >= 0, 0)

    pivot_vars = ['gpp', 'reco', 'temperature_2m', 'VPD', 'soil_temperature_level_1', 'volumetric_soil_water_layer_1']
    nc_year = [nc_month[[vname for vname in list(nc_month.data_vars) if vname not in pivot_vars]].resample(time = '1YS').mean()]
    for vname in pivot_vars:
        nc_year.append(xr.merge([d.rename(f'{vname}{m}').assign_coords(time = [dt]) for m, d in nc_month[vname].groupby('time.month')]))
    nc_year = xr.merge(nc_year)
    if dt.year == 2000: nc_year['gpp1'] = nc_year['gpp2']

    # ==========================================================================
    for cname in dd_train.keys():
    # for cname in ['ra', 'rh']:
        savefile = root_proj_trendy.joinpath(f'3_output/UFLUXv2e_01deg_temp/{dt.year}-{cname}-{sub_k}-TRENDYe.nc')
        if savefile.exists(): continue
        regr_list, X_names = dd_train[cname]

        if cname in ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']:
            nct = nc_year
            cfreq = 'yearly'
        elif cname in ['gpp', 'npp', 'nbp', 'ra', 'rh', 'fGrazing']:
            nct = nc_month
            cfreq = 'monthly'
        else:
            raise ValueError('Unknown cname')

        regr = regr_list[0]
        pred = regr.predict(nct[X_names].to_dataframe()[X_names])
        pred_25 = pd.DataFrame(pred, columns = [cname + '_25th'], index = nct.to_dataframe().index).to_xarray()

        regr = regr_list[1]
        pred = regr.predict(nct[X_names].to_dataframe()[X_names])
        pred_mean = pd.DataFrame(pred, columns = [cname], index = nct.to_dataframe().index).to_xarray()

        regr = regr_list[2]
        pred = regr.predict(nct[X_names].to_dataframe()[X_names])
        pred_75 = pd.DataFrame(pred, columns = [cname + '_75th'], index = nct.to_dataframe().index).to_xarray()

        nco = xr.merge([pred_25, pred_mean, pred_75]); del(pred_25, pred_mean, pred_75)
        nco = nco.where(~nct['IGBP'].isin([13, 15, 16, 17])).where(~nct['NIRv'].isnull(), np.nan).where(~nct['IGBP'].isnull(), np.nan)
        if cfreq == 'yearly': nco = xr.merge([nct['IGBP'], nco])

        nco.to_netcdf(savefile)
        del(nco); del(nct); del(regr); del(pred)
        gc.collect()
        time.sleep(0.1)

"""# Analyse stats

## UFLUXv2 stats and two-way NEE
"""

dd_out = load_pickle(root_proj_trendy.joinpath(f'3_output/UFLUXv2_model/UFLUXv2.pkl'))
roundit(dd_out['dfm'])
# google.download_file(dd_out['dfm'], 'UFLUXv2-validation.csv')

# Option 1: GPP
flux = 'GPP'
yname = 'GPP_NT_VUT_REF'
y1name = 'GPPm'
y2name = 'GPPmML'
unit = '$gC \, m^{-2} \ d^{-1}$'

# # Option2: RECO
# flux = 'RECO'
# yname = 'RECO_NT_VUT_REF'
# y1name = 'RECOm'
# y2name = 'RECOmML'
# unit = '$gC \, m^{-2} \ d^{-1}$'

# # Option3: NEE
# flux = 'NEE'
# yname = 'NEE_VUT_REF'
# y1name = 'NEEm'
# y2name = 'NEEmML'
# unit = '$gC \, m^{-2} \ d^{-1}$'

# # Option3: LE
# flux = 'LE'
# yname = 'LE_F_MDS'
# y1name = 'LEm'
# y2name = 'LEmML'
# unit = '$W \, m^{-2}$'

dfp = deepcopy(dd_out[flux]['dfo'])

if flux in ['GPP', 'RECO']:
    dfp.loc[dfp[yname] < 0, yname] = np.nan
    dfp.loc[dfp[y1name] < 0, y1name] = np.nan
    dfp.loc[dfp[y2name] < 0, y2name] = np.nan

# ------------------------------------------------------------------------------

fig, ax = setup_canvas(1, 1, figsize = (5, 5), fontsize = 10, labelsize = 10)

# dfp.plot.scatter(x = yname, y = y1name, ax = ax)
# dfp.plot.scatter(x = yname, y = y2name, ax = ax, c = colors[1])

ax.scatter(dfp[yname], dfp[y1name], s = 10, c = colors[0], alpha = 0.7, marker = 'x', label = 'baseline')
ax.scatter(dfp[yname], dfp[y2name], s = 10, c = colors[1], edgecolor = 'k', alpha = 0.3, label = 'UFLUX')

xl = np.linspace(np.floor(dfp[yname].min()), np.ceil(dfp[yname].max()), 1000)
ax.plot(xl, xl, ls = '-.', color = 'k')

m1 = get_dfm(dfp[[yname, y1name]].dropna(), 'flux', yname, y1name).T['flux']
ax.plot(xl, xl * m1['slope'] + m1['intercept'], ls = '--', color = colors[0])
m1 = roundit(m1, 2)
add_text(
    ax, 0.05, 0.95, f"$y_b$ = {m1['slope']}x + {m1['intercept']}; r2: {m1['r2']}; RMSE: {m1['RMSE']}",
    color = colors[0], horizontalalignment = 'left', verticalalignment = 'center',
    if_background = True, bg_facecolor = 'white', bg_alpha = 0.1, bg_edgecolor='None'
)

m2 = get_dfm(dfp[[yname, y2name]].dropna(), 'flux', yname, y2name).T['flux']
ax.plot(xl, xl * m2['slope'] + m2['intercept'], ls = '--', color = colors[1])
m2 = roundit(m2, 2)
add_text(
    ax, 0.05, 0.9, f"$y_u$ = {m2['slope']}x + {m2['intercept']}; r2: {m2['r2']}; RMSE: {m2['RMSE']}",
    color = colors[1], horizontalalignment = 'left', verticalalignment = 'center',
    if_background = True, bg_facecolor = 'white', bg_alpha = 0.1, bg_edgecolor='None'
)

ax.set_xlim([xl[0] - (xl[-1] - xl[0]) / 20, xl[-1] + (xl[-1] - xl[0]) / 20])
ax.set_ylim([xl[0] - (xl[-1] - xl[0]) / 20, xl[-1] + (xl[-1] - xl[0]) / 20])
ax.set_xlabel(f'{flux} EC' + f' ({unit})')
ax.set_ylabel(f'{flux} est.' + f' ({unit})')

# upper_legend(ax)
ax.legend(loc = 'lower right') #, frameon=False, facecolor='none', edgecolor='none')

dfm1m2 = pd.concat([
    m1.rename('process model'),
    m2.rename('hybrid model')
], axis = 1).T
dfm1m2['rRMSE'] = dfm1m2['RMSE'] / dfm1m2['Mean']

google.download_file(fig, f'{flux}-scatter.png')
# google.download_file(dfm1m2, f'{flux}-metrics.csv')
dfm1m2

NEE = (dd_out['RECO']['dfo']['RECO_NT_VUT_REF'] - dd_out['GPP']['dfo']['GPP_NT_VUT_REF']).rename('NEE')
NEEmML_dir = dd_out['NEE']['dfo']['NEEmML'].rename('NEEmML').dropna()
NEEmML_ind = (dd_out['RECO']['dfo']['RECOmML'] - dd_out['GPP']['dfo']['GPPmML']).rename('NEEmML').dropna()
dfp = pd.concat([NEE, NEEmML_dir, NEEmML_ind], axis = 1)
dfp.columns = ['NEE', 'NEEe_dir', 'NEEe_ind']

# mutidx = NEEmML_dir.index.intersection(NEEmML_ind.index)
# stats.linregress(NEEmML_dir[mutidx], NEEmML_ind[mutidx])

dfm_nee = pd.concat([
    get_dfm(dfp[['NEEe_dir', 'NEEe_ind']].dropna(), 'NEE_dir_ind', 'NEEe_dir', 'NEEe_ind'),
    get_dfm(dfp[['NEE', 'NEEe_dir']].dropna(), 'NEE_EC_dir', 'NEE', 'NEEe_dir'),
    get_dfm(dfp[['NEE', 'NEEe_ind']].dropna(), 'NEE_EC_ind', 'NEE', 'NEEe_ind')
], axis = 0)

unit = '$gC \, m^{-2} \ d^{-1}$'

# ==============================================================================

fig, axes = setup_canvas(1, 3, figsize = (12, 4), wspace = 0.1, fontsize = 10, labelsize = 10)

# dfp.plot.scatter(x = 'NEEe_dir', y = 'NEEe_ind', ax = ax)
# dfp.plot.scatter(x = 'NEE', y = 'NEEe_dir', ax = ax, color = colors[1])
# dfp.plot.scatter(x = 'NEE', y = 'NEEe_ind', ax = ax, color = colors[2])

# # kde_scatter(ax, dfp, 'NEEe_dir', 'NEEe_ind', frac = 0.3, v_scale = 0.1, cmap = 'RdYlBu_r')

# ------------------------------------------------------------------------------
ax = axes[0]
ax.scatter(dfp['NEE'], dfp['NEEe_dir'], s = 10, c = colors[0], edgecolor = 'k', alpha = 0.3)

xl = np.linspace(np.floor(dfp.min().min()), np.ceil(dfp.max().max()), 1000)
ax.plot(xl, xl, ls = '-.', color = 'k')

m1 = get_dfm(dfp[['NEE', 'NEEe_dir']].dropna(), 'flux', 'NEE', 'NEEe_dir').T['flux']
ax.plot(xl, xl * m1['slope'] + m1['intercept'], ls = '--', color = colors[0])
m1 = roundit(m1, 2)
add_text(
    ax, 0.05, 0.9, f"y = {m1['slope']}x + {m1['intercept']}; r2: {m1['r2']}",
    color = colors[0], horizontalalignment = 'left', verticalalignment = 'center',
    if_background = True, bg_facecolor = 'white', bg_alpha = 0.7, bg_edgecolor='None'
)
ax.set_xlabel('NEE EC'+ f' ({unit})')
ax.set_ylabel('NEE dir.'+ f' ({unit})')
# ------------------------------------------------------------------------------
ax = axes[1]
ax.scatter(dfp['NEE'], dfp['NEEe_ind'], s = 10, c = colors[1], edgecolor = 'k', alpha = 0.3)

xl = np.linspace(np.floor(dfp.min().min()), np.ceil(dfp.max().max()), 1000)
ax.plot(xl, xl, ls = '-.', color = 'k')

m1 = get_dfm(dfp[['NEE', 'NEEe_ind']].dropna(), 'flux', 'NEE', 'NEEe_ind').T['flux']
ax.plot(xl, xl * m1['slope'] + m1['intercept'], ls = '--', color = colors[1])
m1 = roundit(m1, 2)
add_text(
    ax, 0.05, 0.9, f"y = {m1['slope']}x + {m1['intercept']}; r2: {m1['r2']}",
    color = colors[1], horizontalalignment = 'left', verticalalignment = 'center',
    if_background = True, bg_facecolor = 'white', bg_alpha = 0.7, bg_edgecolor='None'
)
ax.set_xlabel('NEE EC'+ f' ({unit})')
ax.set_ylabel('NEE ind.' + f' ({unit})')
# ------------------------------------------------------------------------------
ax = axes[2]
ax.scatter(dfp['NEEe_dir'], dfp['NEEe_ind'], s = 10, c = colors[2], edgecolor = 'k', alpha = 0.3)

xl = np.linspace(np.floor(dfp.min().min()), np.ceil(dfp.max().max()), 1000)
ax.plot(xl, xl, ls = '-.', color = 'k')

m1 = get_dfm(dfp[['NEEe_dir', 'NEEe_ind']].dropna(), 'flux', 'NEEe_dir', 'NEEe_ind').T['flux']
ax.plot(xl, xl * m1['slope'] + m1['intercept'], ls = '--', color = colors[2])
m1 = roundit(m1, 2)
add_text(
    ax, 0.05, 0.9, f"y = {m1['slope']}x + {m1['intercept']}; r2: {m1['r2']}",
    color = colors[2], horizontalalignment = 'left', verticalalignment = 'center',
    if_background = True, bg_facecolor = 'white', bg_alpha = 0.7, bg_edgecolor='None'
)
ax.set_xlabel('NEE dir.'+ f' ({unit})')
ax.set_ylabel('NEE ind.'+ f' ({unit})')

ax.set_xlim([xl[0] - (xl[-1] - xl[0]) / 20, xl[-1] + (xl[-1] - xl[0]) / 20])
ax.set_ylim([xl[0] - (xl[-1] - xl[0]) / 20, xl[-1] + (xl[-1] - xl[0]) / 20])

# google.download_file(fig, f'NEE_balance-scatter.png')
# google.download_file(dfm_nee, f'NEE_balance-metrics.csv')
dfm_nee

# dft = pd.concat([df_ml['NEE_VUT_REF'], (df_ml['RECO_NT_VUT_REF'] - df_ml['GPP_NT_VUT_REF']).rename('NEE_NT_calc')], axis = 1).dropna()
# stats.linregress(dft['NEE_VUT_REF'], dft['NEE_NT_calc'])

"""## Emulator stats"""

dfp = []

for p in root_proj_trendy.joinpath('3_output/UFLUXv2e_model').glob('*.pkl'):
    # print(p.stem)
    dd_out = load_pickle(p)
    for k in dd_out.keys():
        dft = dd_out[k]['performance_metrics']
        dft['cname'] = k.split('_')[0]
        dft['nRMSE'] = dft['RMSE'] / dft['Mean']
        dfp.append(dft)
dfp = pd.concat(dfp)
# google.download_file(dfp, 'emulator-metrics-stats.csv')
# google.download_file(dfp.groupby('cname').mean(), 'emulator-metrics-stats-avg_by_cname.csv')
dfp.groupby('cname').mean()

# cnames = ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood', 'gpp', 'npp', 'nbp', 'ra', 'rh', 'fGrazing']
cnames = ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood', 'ra', 'rh']
x = dfp.loc[cnames, 'nRMSE']
y = dfp.loc[cnames, 'r2']
x_25th = dfp.loc[[c + '_25th' for c in cnames], 'nRMSE']
y_25th = dfp.loc[[c + '_25th' for c in cnames], 'r2']
x_75th = dfp.loc[[c + '_75th' for c in cnames], 'nRMSE']
y_75th = dfp.loc[[c + '_75th' for c in cnames], 'r2']

x_25th.index = [i.split('_')[0] for i in x_25th.index]
y_25th.index = [i.split('_')[0] for i in y_25th.index]
x_75th.index = [i.split('_')[0] for i in x_75th.index]
y_75th.index = [i.split('_')[0] for i in y_75th.index]

dd_angle = {
    'cVeg': [-0.02, -0.04, 30, -80],
    'cLitter': [-0.03, 0.04, 0, -90],
    'cSoil': [0.02, 0.04, 30, -70],
    'cLeaf': [0.00, -0.03, 30, -80],
    'cWood': [0.02, 0.02, 30, 70],
    'ra': [0.00, 0.02, 30, -80],
    'rh': [0.00, -0.03, 30, -80]
}

dd_marker = {
    'cVeg': 'o',
    'cLitter': 's',
    'cSoil': 'P',
    'cLeaf': 'X',
    'cWood': 'D',
    'ra': 'v',
    'rh': '>'
}

fig, ax = setup_canvas(1, 1, figsize = (5, 5), fontsize = 10, labelsize = 10)
for cname in cnames:
    ax.scatter(x[cname], y[cname], s = 40, c = colors[0], edgecolor = 'k', zorder = 10, marker = dd_marker[cname])
    ax.scatter(x_25th[cname], y_25th[cname], s = 40, c = colors[1], edgecolor = 'k', zorder = 10, marker = dd_marker[cname])
    ax.scatter(x_75th[cname], y_75th[cname], s = 40, c = colors[2], edgecolor = 'k', zorder = 10, marker = dd_marker[cname])
    ax.plot([x[cname], x_25th[cname]], [y[cname], y_25th[cname]], ls = '--', color = colors[1])
    ax.plot([x[cname], x_75th[cname]], [y[cname], y_75th[cname]], ls = '--', color = colors[2])
    xloc = x[cname]; yloc = y[cname]
    ax.annotate(
        cname, (xloc, yloc),
        xytext=(xloc + dd_angle[cname][0], yloc + dd_angle[cname][1]),
        arrowprops = dict(arrowstyle = "->", connectionstyle = f"angle3,angleA={dd_angle[cname][2]},angleB={dd_angle[cname][3]}")
    )


x_center = 0
y_center = 1

# Draw quarter-circle grid by adding circles at the upper-left corner
for r in [0.05, 0.1, 0.15, 0.2, 0.3]:  # Radius values
    circle = plt.Circle((x_center, y_center), r, color='gray', fill=False, linestyle='--', linewidth=0.8)
    ax.add_artist(circle)

for cname in cnames:
    ax.scatter([], [], s = 40, c = 'grey', edgecolor = 'k', zorder = 10, marker = dd_marker[cname], label = cname)
ax.scatter([], [], s = 40, c = colors[0], edgecolor = 'k', zorder = 10, marker = 'o', label = 'mean')
ax.scatter([], [], s = 40, c = colors[1], edgecolor = 'k', zorder = 10, marker = 'o', label = '25th')
ax.scatter([], [], s = 40, c = colors[2], edgecolor = 'k', zorder = 10, marker = 'o', label = '75th')

# # Style the axes
# ax.spines['top'].set_visible(False)
# ax.spines['right'].set_visible(False)
# ax.spines['left'].set_position(('data', 0))
# ax.spines['bottom'].set_position(('data', 0))

# Set aspect ratio to ensure circles appear circular
ax.set_aspect('equal', adjustable='datalim')

ax.set_xlabel('nRMSE', fontsize = 10)
ax.set_ylabel('r2', fontsize = 10)

# ax.set_xlim(0, 0.33)
# ax.set_ylim(0.83, 1.01)

upper_legend(ax, nrows = 2, yloc = 1.15)

# google.download_file(fig, 'emulator-validation.png')

"""# Analyse IAV

## Load IAV data
"""

df_path_ufluxv2 = []
for p in root_proj_trendy.joinpath('3_output/UFLUXv2').glob('*.nc'):
    year, _, _ = p.stem.split('-')
    df_path_ufluxv2.append([pd.to_datetime(year, format = '%Y'), p])
df_path_ufluxv2 = pd.DataFrame(df_path_ufluxv2, columns = ['time', 'path']).set_index('time')

# ------------------------------------------------------------------------------

'''
Load rescaled:
'''
df_path_ufluxv2e_rescale = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2e_rescale').glob('*.nc'):
    year, cname, _ = p.stem.split('-')
    if cname == 'npp': continue
    df_path_ufluxv2e_rescale.append([pd.to_datetime(year, format = '%Y'), cname, p])
df_path_ufluxv2e_rescale = pd.DataFrame(df_path_ufluxv2e_rescale, columns = ['time', 'cname', 'path']).pivot(index = 'time', columns = ['cname']).droplevel(0, axis = 1)

df_path_ufluxv2e = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2e').glob('*.nc'):
    year, cname, _ = p.stem.split('-')
    if cname in ['fGrazing', 'nbp', 'ra', 'rh']: continue
    df_path_ufluxv2e.append([pd.to_datetime(year, format = '%Y'), cname, p])
df_path_ufluxv2e = pd.DataFrame(df_path_ufluxv2e, columns = ['time', 'cname', 'path']).pivot(index = 'time', columns = ['cname']).droplevel(0, axis = 1)
df_path_ufluxv2e = pd.concat([df_path_ufluxv2e, df_path_ufluxv2e_rescale], axis = 1)
'''
Otherwise:
'''
# df_path_ufluxv2e = []
# for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2e').glob('*.nc'):
#     year, cname, _ = p.stem.split('-')
#     df_path_ufluxv2e.append([pd.to_datetime(year, format = '%Y'), cname, p])
# df_path_ufluxv2e = pd.DataFrame(df_path_ufluxv2e, columns = ['time', 'cname', 'path']).pivot(index = 'time', columns = ['cname']).droplevel(0, axis = 1)

# ------------------------------------------------------------------------------

savefolder = root_proj_trendy.joinpath(f'4_analysis/annual_dynamics'); savefolder.mkdir(exist_ok = True)
# ------------------------------------------------------------------------------
savefile = savefolder.joinpath(f'ufluxv2.nc')
if not savefile.exists():
    ufluxv2 = []
    for dt in tqdm(df_path_ufluxv2.index):
        p = df_path_ufluxv2.loc[dt, 'path']
        nct = xr.open_dataset(p, engine = 'netcdf4')
        # nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
        # nct = nct.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).resample(time = '1YS').mean()
        ufluxv2.append(nct.resample(time = '1YS').mean())
    ufluxv2 = xr.merge(ufluxv2)
    ufluxv2 = ufluxv2.where(~ufluxv2['GPPmML'].isnull(), np.nan)
    ufluxv2.to_netcdf(savefile)
    del(ufluxv2)
# ------------------------------------------------------------------------------
savefolder.joinpath('UFLUXv2e').mkdir(exist_ok = True)
for cname in df_path_ufluxv2e.columns:
    savefile = savefolder.joinpath(f'UFLUXv2e/{cname}.nc')
    if savefile.exists(): continue
    print(cname)
    ufluxv2et = []
    for dt in tqdm(df_path_ufluxv2e.index):
        p = df_path_ufluxv2e.loc[dt, cname]
        nct = xr.open_dataset(p, engine = 'netcdf4')
        nct = nct.resample(time = '1YS').mean()
        ufluxv2et.append(nct)
        time.sleep(0.1)
    ufluxv2et = xr.merge(ufluxv2et)
    ufluxv2et.to_netcdf(savefile, engine = 'scipy')
    del(ufluxv2et); gc.collect(); time.sleep(1)
# ------------------------------------------------------------------------------

# UFLUXv1

df_path_ufluxv1 = []
for p in root_proj_trendy.joinpath('3_output/UFLUXv1').glob('*.nc'):
    year, _, _ = p.stem.split('-')
    df_path_ufluxv1.append([pd.to_datetime(year, format = '%Y'), p])
df_path_ufluxv1 = pd.DataFrame(df_path_ufluxv1, columns = ['time', 'path']).set_index('time')

df_path_ufluxv1_ensemble = []
for p in root_proj_trendy.joinpath('3_output/UFLUXv1-ensemble').glob('*.nc'):
    year, _, _ = p.stem.split('-')
    df_path_ufluxv1_ensemble.append([pd.to_datetime(year, format = '%Y'), p])
df_path_ufluxv1_ensemble = pd.DataFrame(df_path_ufluxv1_ensemble, columns = ['time', 'path']).set_index('time')

# ------------------------------------------------------------------------------

'''
Load rescaled:
'''
df_path_ufluxv1e_rescale = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv1e_rescale').glob('*.nc'):
    year, cname, _ = p.stem.split('-')
    if cname == 'npp': continue
    df_path_ufluxv1e_rescale.append([pd.to_datetime(year, format = '%Y'), cname, p])
df_path_ufluxv1e_rescale = pd.DataFrame(df_path_ufluxv1e_rescale, columns = ['time', 'cname', 'path']).pivot(index = 'time', columns = ['cname']).droplevel(0, axis = 1)

df_path_ufluxv1e = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv1e').glob('*.nc'):
    year, cname, _ = p.stem.split('-')
    if cname in ['fGrazing', 'nbp', 'ra', 'rh']: continue
    df_path_ufluxv1e.append([pd.to_datetime(year, format = '%Y'), cname, p])
df_path_ufluxv1e = pd.DataFrame(df_path_ufluxv1e, columns = ['time', 'cname', 'path']).pivot(index = 'time', columns = ['cname']).droplevel(0, axis = 1)
df_path_ufluxv1e = pd.concat([df_path_ufluxv1e, df_path_ufluxv1e_rescale], axis = 1)
'''
Otherwise:
'''
# df_path_ufluxv1e = []
# for p in root_proj_trendy.joinpath(f'3_output/UFLUXv1e').glob('*.nc'):
#     year, cname, _ = p.stem.split('-')
#     df_path_ufluxv1e.append([pd.to_datetime(year, format = '%Y'), cname, p])
# df_path_ufluxv1e = pd.DataFrame(df_path_ufluxv1e, columns = ['time', 'cname', 'path']).pivot(index = 'time', columns = ['cname']).droplevel(0, axis = 1)

savefolder = root_proj_trendy.joinpath(f'4_analysis/annual_dynamics'); savefolder.mkdir(exist_ok = True)
# ------------------------------------------------------------------------------

savefile = savefolder.joinpath(f'ufluxv1.nc')
if not savefile.exists():
    ufluxv1 = []
    for dt in tqdm(df_path_ufluxv1.index):
        p = df_path_ufluxv1.loc[dt, 'path']
        nct = xr.open_dataset(p, engine = 'netcdf4')
        # nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
        # nct = nct.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).resample(time = '1YS').mean()
        ufluxv1.append(nct.resample(time = '1YS').mean())
    ufluxv1 = xr.merge(ufluxv1)
    ufluxv1 = ufluxv1.where(~ufluxv1['GPPmML'].isnull(), np.nan)
    ufluxv1.to_netcdf(savefile)
    del(ufluxv1)
# ------------------------------------------------------------------------------

savefile = savefolder.joinpath(f'ufluxv1-ensemble.nc')
if not savefile.exists():
    ufluxv1_ensemble = []
    for dt in tqdm(df_path_ufluxv1_ensemble.index):
        p = df_path_ufluxv1_ensemble.loc[dt, 'path']
        nct = xr.open_dataset(p, engine = 'netcdf4')
        # nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
        # nct = nct.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).resample(time = '1YS').mean()
        ufluxv1_ensemble.append(nct.resample(time = '1YS').mean())
    ufluxv1_ensemble = xr.merge(ufluxv1_ensemble)
    ufluxv1_ensemble = ufluxv1_ensemble.where(~ufluxv1_ensemble['GPPmML_mean'].isnull(), np.nan)
    ufluxv1_ensemble.to_netcdf(savefile)
    del(ufluxv1_ensemble)
# ------------------------------------------------------------------------------

savefolder.joinpath('UFLUXv1e').mkdir(exist_ok = True)
for cname in df_path_ufluxv1e.columns:
    if cname in ['fGrazing', 'nbp']: continue
    savefile = savefolder.joinpath(f'UFLUXv1e/{cname}.nc')
    if savefile.exists(): continue
    print(cname)
    ufluxv1et = []
    for dt in tqdm(df_path_ufluxv1e.index):
        p = df_path_ufluxv1e.loc[dt, cname]
        nct = xr.open_dataset(p, engine = 'netcdf4')
        nct = nct.resample(time = '1YS').mean()
        ufluxv1et.append(nct)
        time.sleep(0.1)
    ufluxv1et = xr.merge(ufluxv1et)
    ufluxv1et.to_netcdf(savefile, engine = "scipy")
    del(ufluxv1et); gc.collect(); time.sleep(1)

# # ------------------------------------------------------------------------------
# savefile = savefolder.joinpath(f'UFLUXv1e/npp_unscaled.nc')
# if not savefile.exists():
#     npp_orig = []
#     for p in root_proj_trendy.joinpath(f'3_output/UFLUXv1e').glob('*npp*.nc'):
#         nct = xr.open_dataset(p).resample(time = '1YS').mean()
#         npp_orig.append(nct)
#     npp_orig = xr.merge(npp_orig)

#     npp_orig.to_netcdf(savefile, engine = "scipy")

def get_Trendy_gpp():
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_gpp_05deg_monthly_2000-2022.nc')
    gppT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    gppT = gppT.where(gppT['gpp'] != 0, np.nan)
    gppT = gppT.resample(time = '1YS').mean()
    return gppT.rename({'gpp': 'GPP(t)', 'gpp_25th': 'GPP(t)_25th', 'gpp_75th': 'GPP(t)_75th'})

def get_Trendy_npp():
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_npp_05deg_monthly_2000-2022.nc')
    nppT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    nppT = nppT.where(nppT['npp'] != 0, np.nan)
    nppT = nppT.resample(time = '1YS').mean()
    return nppT.rename({'npp': 'NPP(t)', 'npp_25th': 'NPP(t)_25th', 'npp_75th': 'NPP(t)_75th'})

def get_Trendy_ra():
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_ra_05deg_monthly_2000-2022.nc')
    raT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    raT = raT.where(raT['ra'] != 0, np.nan)
    raT = raT.resample(time = '1YS').mean()
    return raT.rename({'ra': 'Ra(t)', 'ra_25th': 'Ra(t)_25th', 'ra_75th': 'Ra(t)_75th'})

def get_Trendy_rh():
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_rh_05deg_monthly_2000-2022.nc')
    rhT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    rhT = rhT.where(rhT['rh'] != 0, np.nan)
    rhT = rhT.resample(time = '1YS').mean()
    return rhT.rename({'rh': 'Rh(t)', 'rh_25th': 'Rh(t)_25th', 'rh_75th': 'Rh(t)_75th'})

'''
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# !!!!WRONG CODE NEE != -NPP; NEE = -NEP = -(NPP - Rh)!!!!!
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
def get_Trendy_nee():
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_npp_05deg_monthly_2000-2022.nc')
    nppT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    nppT = nppT.where(nppT['npp'] != 0, np.nan)
    nppT = nppT.resample(time = '1YS').mean()

    neeT = xr.merge([
        nppT['npp'].rename('Trendy_NEE') * -1,
        nppT['npp_25th'].rename('Trendy_NEE_25th') * -1,
        nppT['npp_75th'].rename('Trendy_NEE_75th') * -1
    ])

    neeT = (nppT * -1).rename({'npp': 'NEE(t)', 'npp_25th': 'NEE(t)_25th', 'npp_75th': 'NEE(t)_75th'})
    return neeT
'''

'''
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# !!!!WRONG CODE HERE: NPP 75th may not correspond to RH 75th!!!!
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
def get_Trendy_nep():
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_npp_05deg_monthly_2000-2022.nc')
    nppT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    nppT = nppT.where(nppT['npp'] != 0, np.nan)

    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_rh_05deg_monthly_2000-2022.nc')
    rhT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    rhT = rhT.where(rhT['rh'] > 1e-9, np.nan)

    nepT = (nppT['npp'] - rhT['rh']).rename('NEP(t)').resample(time = '1YS').mean()
    nepT_25th = (nppT['npp_25th'] - rhT['rh_25th']).rename('NEP(t)_25th').resample(time = '1YS').mean()
    nepT_75th = (nppT['npp_75th'] - rhT['rh_75th']).rename('NEP(t)_75th').resample(time = '1YS').mean()

    return xr.merge([nepT, nepT_25th, nepT_75th])
'''

def get_Trendy_nep():
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_nep_05deg_monthly_2000-2022.nc')
    nepT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    nepT = nepT.where(nepT['nep'] != 0, np.nan)
    nepT = nepT.resample(time = '1YS').mean()
    return nepT.rename({'nep': 'NEP(t)', 'nep_25th': 'NEP(t)_25th', 'nep_75th': 'NEP(t)_75th'})

def get_Trendy_reco():
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_ra_05deg_monthly_2000-2022.nc')
    raT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    # raT = raT.resample(time = '1YS').mean()
    raT = raT.where(raT['ra'] > 1e-9, np.nan)
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_rh_05deg_monthly_2000-2022.nc')
    rhT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    # rhT = rhT.resample(time = '1YS').mean()
    rhT = rhT.where(rhT['rh'] > 1e-9, np.nan)

    recoT = (raT['ra'] + rhT['rh']).rename('RECO(t)').resample(time = '1YS').mean()
    recoT_25th = (raT['ra_25th'] + rhT['rh_25th']).rename('RECO(t)_25th').resample(time = '1YS').mean()
    recoT_75th = (raT['ra_75th'] + rhT['rh_75th']).rename('RECO(t)_75th').resample(time = '1YS').mean()

    return xr.merge([recoT, recoT_25th, recoT_75th])

savefolder = root_proj_trendy.joinpath(f'4_analysis/annual_dynamics/Trendy'); savefolder.mkdir(exist_ok = True)

savefile = savefolder.joinpath(f'gpp.nc')
if not savefile.exists():
    gppT = get_Trendy_gpp()
    gppT.to_netcdf(savefile)
    del(gppT)

savefile = savefolder.joinpath(f'npp.nc')
if not savefile.exists():
    nppT = get_Trendy_npp()
    nppT.to_netcdf(savefile)
    del(nppT)

savefile = savefolder.joinpath(f'ra.nc')
if not savefile.exists():
    raT = get_Trendy_ra()
    raT.to_netcdf(savefile)
    del(raT)

savefile = savefolder.joinpath(f'rh.nc')
if not savefile.exists():
    rhT = get_Trendy_rh()
    rhT.to_netcdf(savefile)
    del(rhT)

savefile = savefolder.joinpath(f'reco.nc')
if not savefile.exists():
    recoT = get_Trendy_reco()
    recoT.to_netcdf(savefile)
    del(recoT)

savefile = savefolder.joinpath(f'nep.nc')
if not savefile.exists():
    nepT = get_Trendy_nep()
    nepT.to_netcdf(savefile)

    neeT = xr.merge([
        nepT['NEP(t)'].rename('NEE(t)') * -1,
        nepT['NEP(t)_25th'].rename('NEE(t)_25th') * -1,
        nepT['NEP(t)_75th'].rename('NEE(t)_75th') * -1
    ])
    neeT.to_netcdf(savefolder.joinpath(f'nee.nc'))
    del(nepT); del(neeT)

p = root_proj_trendy.joinpath(f'4_analysis/annual_dynamics/ufluxv2.nc')
ufluxv2_a = xr.open_dataset(p, engine = 'netcdf4')
ufluxv2_a = ufluxv2_a.rename(
    {k: k.replace('mML', '') + '(u)' for k in ufluxv2_a.data_vars if k != 'NIRv'}
)

ufluxv2_a = ufluxv2_a.rename({'NEE(u)': 'NEE(u)_dir', 'NEEm(u)': 'NEEm(u)_dir'})
ufluxv2_a['NEP(u)_dir'] = ufluxv2_a['NEE(u)_dir'] * -1
ufluxv2_a['NEPm(u)_dir'] = ufluxv2_a['NEEm(u)_dir'] * -1
ufluxv2_a['NEE(u)_ind'] = (ufluxv2_a['RECO(u)'] - ufluxv2_a['GPP(u)'])
ufluxv2_a['NEEm(u)_ind'] = (ufluxv2_a['RECOm(u)'] - ufluxv2_a['GPPm(u)'])
ufluxv2_a['NEP(u)_ind'] = ufluxv2_a['NEE(u)_ind'] * -1
ufluxv2_a['NEPm(u)_ind'] = ufluxv2_a['NEEm(u)_ind'] * -1

# ==============================================================================

# UFLUXv1:
p = root_proj_trendy.joinpath(f'4_analysis/annual_dynamics/ufluxv1.nc')
ufluxv1_a = xr.open_dataset(p, engine = 'netcdf4')
ufluxv1_a = ufluxv1_a.rename(
    {k: k.replace('mML', '') + '(u1)' for k in ufluxv1_a.data_vars if k != 'NIRv'}
)

ufluxv1_a = ufluxv1_a.rename({'NEE(u1)': 'NEE(u1)_dir'})
ufluxv1_a['NEP(u1)_dir'] = ufluxv1_a['NEE(u1)_dir'] * -1
ufluxv1_a['NEE(u1)_ind'] = (ufluxv1_a['RECO(u1)'] - ufluxv1_a['GPP(u1)'])
ufluxv1_a['NEP(u1)_ind'] = ufluxv1_a['NEE(u1)_ind'] * -1


# UFLUXv1-ensemble:
p = root_proj_trendy.joinpath(f'4_analysis/annual_dynamics/ufluxv1-ensemble.nc')
ufluxv1_a_ensemble = xr.open_dataset(p, engine = 'netcdf4')
ufluxv1_a_ensemble = ufluxv1_a_ensemble.rename(
    {k: k.replace('mML', '(u1)') for k in ufluxv1_a_ensemble.data_vars if k != 'NIRv'}
)
ufluxv1_a_ensemble = ufluxv1_a_ensemble.rename(
    {k: k.replace('_mean', '') for k in ufluxv1_a_ensemble.data_vars if k != 'NIRv'}
)

ufluxv1_a_ensemble = ufluxv1_a_ensemble.rename({'NEE(u1)_min': 'NEE(u1)_min_dir', 'NEE(u1)': 'NEE(u1)_dir', 'NEE(u1)_max': 'NEE(u1)_max_dir'})

ufluxv1_a_ensemble['NEP(u1)_min_dir'] = ufluxv1_a_ensemble['NEE(u1)_min_dir'] * -1
ufluxv1_a_ensemble['NEP(u1)_dir'] = ufluxv1_a_ensemble['NEE(u1)_dir'] * -1
ufluxv1_a_ensemble['NEP(u1)_max_dir'] = ufluxv1_a_ensemble['NEE(u1)_max_dir'] * -1

# Calculating RECO max - GPP max is meaningless, as it's not euqal to NEE max
ufluxv1_a_ensemble['NEE(u1)_ind'] = (ufluxv1_a_ensemble['RECO(u1)'] - ufluxv1_a_ensemble['GPP(u1)'])
ufluxv1_a_ensemble['NEP(u1)_ind'] = ufluxv1_a_ensemble['NEE(u1)_ind'] * -1

# ==============================================================================

ufluxv2e_dd = {}
for p in root_proj_trendy.joinpath(f'4_analysis/annual_dynamics/UFLUXv2e').glob('*.nc'):
    name = p.stem
    if name in ['ufluxv2_nongapfill', 'fGrazing', 'nbp']:
        continue
    else:
        ufluxv2e_dd[name] = xr.open_dataset(p, engine = 'netcdf4')

ufluxv2e_dd['reco'] = xr.merge([
    (ufluxv2e_dd['ra']['ra'] + ufluxv2e_dd['rh']['rh']).rename('reco'),
    (ufluxv2e_dd['ra']['ra_25th'] + ufluxv2e_dd['rh']['rh_25th']).rename('reco_25th'),
    (ufluxv2e_dd['ra']['ra_75th'] + ufluxv2e_dd['rh']['rh_75th']).rename('reco_75th')
])

for cname, ufluxv2e in ufluxv2e_dd.items():
    if cname in ['npp', 'reco', 'gpp']:
        ufluxv2e_dd[cname] = ufluxv2e.rename(
            {k: k.split('_')[0].upper() + '(e)_' + k.split('_')[1] if '_' in k else k.upper() + '(e)' for k in ufluxv2e.data_vars}
        )
    elif cname in ['cLeaf', 'cLitter', 'cSoil', 'cVeg', 'cWood']:
        ufluxv2e_dd[cname] = ufluxv2e.rename(
            {k: k.split('_')[0] + '(e)_' + k.split('_')[1] if '_' in k else k + '(e)' for k in ufluxv2e.data_vars if k != 'IGBP'}
        )
    elif cname in ['ra', 'rh']:
        ufluxv2e_dd[cname] = ufluxv2e.rename(
            {k: k.split('_')[0].capitalize() + '(e)_' + k.split('_')[1] if '_' in k else k.capitalize() + '(e)' for k in ufluxv2e.data_vars if k != 'IGBP'}
        )
    else:
        pass

ufluxv2e_dd['nep'] = xr.merge([
    (ufluxv2e_dd['npp']['NPP(e)_25th'] - ufluxv2e_dd['rh']['Rh(e)_25th']).rename('NEP(e)_25th'),
    (ufluxv2e_dd['npp']['NPP(e)'] - ufluxv2e_dd['rh']['Rh(e)']).rename('NEP(e)'),
    (ufluxv2e_dd['npp']['NPP(e)_75th'] - ufluxv2e_dd['rh']['Rh(e)_75th']).rename('NEP(e)_75th'),
])

ufluxv2e_dd['nee'] = (ufluxv2e_dd['nep'] * -1).rename({
    'NEP(e)': 'NEE(e)',
    'NEP(e)_25th': 'NEE(e)_25th',
    'NEP(e)_75th': 'NEE(e)_75th',
})

print('UFLUXv2e:', list(ufluxv2e_dd.keys()))
gc.collect(); time.sleep(0.1)

# ==============================================================================

# UFLUX1e
ufluxv1e_dd = {}
for p in root_proj_trendy.joinpath(f'4_analysis/annual_dynamics/UFLUXv1e').glob('*.nc'):
    name = p.stem
    if name in ['ufluxv1_nongapfill', 'fGrazing', 'nbp', 'npp_unscaled']:
        continue
    else:
        ufluxv1e_dd[name] = xr.open_dataset(p, engine = 'netcdf4')

ufluxv1e_dd['reco'] = xr.merge([
    (ufluxv1e_dd['ra']['ra'] + ufluxv1e_dd['rh']['rh']).rename('reco'),
    (ufluxv1e_dd['ra']['ra_25th'] + ufluxv1e_dd['rh']['rh_25th']).rename('reco_25th'),
    (ufluxv1e_dd['ra']['ra_75th'] + ufluxv1e_dd['rh']['rh_75th']).rename('reco_75th')
])

for cname, UFLUXv1e in ufluxv1e_dd.items():
    if cname in ['npp', 'reco', 'gpp']:
        ufluxv1e_dd[cname] = UFLUXv1e.rename(
            {k: k.split('_')[0].upper() + '(e)_' + k.split('_')[1] if '_' in k else k.upper() + '(e)' for k in UFLUXv1e.data_vars}
        )
    elif cname in ['cLeaf', 'cLitter', 'cSoil', 'cVeg', 'cWood']:
        ufluxv1e_dd[cname] = UFLUXv1e.rename(
            {k: k.split('_')[0] + '(e)_' + k.split('_')[1] if '_' in k else k + '(e)' for k in UFLUXv1e.data_vars if k != 'IGBP'}
        )
    elif cname in ['ra', 'rh']:
        ufluxv1e_dd[cname] = UFLUXv1e.rename(
            {k: k.split('_')[0].capitalize() + '(e)_' + k.split('_')[1] if '_' in k else k.capitalize() + '(e)' for k in UFLUXv1e.data_vars if k != 'IGBP'}
        )
    else:
        pass

ufluxv1e_dd['nep'] = xr.merge([
    (ufluxv1e_dd['npp']['NPP(e)_25th'] - ufluxv1e_dd['rh']['Rh(e)_25th']).rename('NEP(e)_25th'),
    (ufluxv1e_dd['npp']['NPP(e)'] - ufluxv1e_dd['rh']['Rh(e)']).rename('NEP(e)'),
    (ufluxv1e_dd['npp']['NPP(e)_75th'] - ufluxv1e_dd['rh']['Rh(e)_75th']).rename('NEP(e)_75th'),
])

ufluxv1e_dd['nee'] = (ufluxv1e_dd['nep'] * -1).rename({
    'NEP(e)': 'NEE(e)',
    'NEP(e)_25th': 'NEE(e)_25th',
    'NEP(e)_75th': 'NEE(e)_75th',
})

if 'npp_unscaled' in ufluxv1e_dd.keys():
    ufluxv1e_dd['npp_unscaled'] = ufluxv1e_dd['npp_unscaled'].rename({
        'npp_25th': 'NPP(e)_25th',
        'npp': 'NPP(e)',
        'npp_75th': 'NPP(e)_75th'
    })

print('UFLUXv1e:', list(ufluxv1e_dd.keys()))
gc.collect(); time.sleep(0.1)

# ==============================================================================

trendy_dd = {}
for p in root_proj_trendy.joinpath(f'4_analysis/annual_dynamics/Trendy').glob('*.nc'):
    name = p.stem
    trendy_dd[name] = xr.open_dataset(p, engine = 'netcdf4')

print('Trendy:', list(trendy_dd.keys()))
gc.collect(); time.sleep(0.1)

coef_mat = xr.DataArray(
    deg2m(ufluxv2_a.longitude, ufluxv2_a.latitude, 0.5, 0.5),
    dims = ['latitude', 'longitude'],
    coords = {'longitude': ufluxv2_a.longitude, 'latitude': ufluxv2_a.latitude}
).expand_dims(time = ufluxv2_a.time)

'''
CONSIDER HERE: if only consider where GPP & NPP > 0: not consider => agree with other upscaling studies, but they should be > 0 (otherwise vegetation would die)
'''
cond_trendy = (~(trendy_dd['reco']['RECO(t)'].isnull()))

cond_ufluxv2 = (~(ufluxv2_a['RECO(u)'].isnull()))\
     & (ufluxv2_a['RECO(u)'] > 0)\
     & (ufluxv2_a['GPP(u)'] > 0)\
     & (ufluxv2_a['GPP(u)'] < 12)\
     & (ufluxv2_a['RECO(u)'] < 12)\
     & (ufluxv2_a['NEE(u)_ind'] > -12)\
     & (ufluxv2_a['NEE(u)_ind'] < 6)

cond_ufluxv1 = (~(ufluxv1_a['RECO(u1)'].isnull()))\
     & (ufluxv1_a['RECO(u1)'] > 0)\
     & (ufluxv1_a['GPP(u1)'] > 0)\
     & (ufluxv1_a['GPP(u1)'] < 12)\
     & (ufluxv1_a['RECO(u1)'] < 12)\
     & (ufluxv1_a['NEE(u1)_ind'] > -12)\
     & (ufluxv1_a['NEE(u1)_ind'] < 6)

cond_ufluxv2e = (~(ufluxv2e_dd['reco']['RECO(e)'].isnull()))\
     & (ufluxv2e_dd['gpp']['GPP(e)'] > 0)\
     & (ufluxv2e_dd['npp']['NPP(e)'] > 0)\
     & (ufluxv2e_dd['reco']['RECO(e)'] < 12)\
     & (ufluxv2e_dd['nee']['NEE(e)'] > -12)\
     & (ufluxv2e_dd['nee']['NEE(e)'] < 6)\
     & (ufluxv2e_dd['cWood']['cWood(e)'] > 0)\
     & (ufluxv2e_dd['cLeaf']['cLeaf(e)'] > 0)\
     & (ufluxv2e_dd['cVeg']['cVeg(e)'] > 0)\
     & (ufluxv2e_dd['cLitter']['cLitter(e)'] > 0)\
     & (ufluxv2e_dd['cSoil']['cSoil(e)'] > 0)

cond_ufluxv1e = (~(ufluxv1e_dd['reco']['RECO(e)'].isnull()))\
     & (ufluxv1e_dd['gpp']['GPP(e)'] > 0)\
     & (ufluxv1e_dd['npp']['NPP(e)'] > 0)\
     & (ufluxv1e_dd['reco']['RECO(e)'] < 12)\
     & (ufluxv1e_dd['nee']['NEE(e)'] > -12)\
     & (ufluxv1e_dd['nee']['NEE(e)'] < 6)\
     & (ufluxv1e_dd['cWood']['cWood(e)'] > 0)\
     & (ufluxv1e_dd['cLeaf']['cLeaf(e)'] > 0)\
     & (ufluxv1e_dd['cVeg']['cVeg(e)'] > 0)\
     & (ufluxv1e_dd['cLitter']['cLitter(e)'] > 0)\
     & (ufluxv1e_dd['cSoil']['cSoil(e)'] > 0)

cond = cond_trendy & cond_ufluxv2 & cond_ufluxv2e & cond_ufluxv1 & cond_ufluxv1e

ufluxv2_a = ufluxv2_a.where(cond, np.nan)
ufluxv1_a_ensemble = ufluxv1_a_ensemble.where(cond, np.nan)
for k in ufluxv2e_dd.keys():
    ufluxv2e_dd[k] = ufluxv2e_dd[k].where(cond, np.nan)

for k in trendy_dd.keys():
    trendy_dd[k] = trendy_dd[k].where(cond, np.nan)

ufluxv1_a = ufluxv1_a.where(cond, np.nan)
for k in ufluxv1e_dd.keys():
    ufluxv1e_dd[k] = ufluxv1e_dd[k].where(cond, np.nan)

"""## Rescale factors"""

df_ufluxv2 = (ufluxv2_a.where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe().copy()
df_ufluxv2 = df_ufluxv2[['GPPm(u)', 'GPP(u)', 'RECOm(u)', 'RECO(u)', 'NEEm(u)_dir', 'NEE(u)_dir', 'NEP(u)_dir', 'NEPm(u)_dir', 'NEE(u)_ind', 'NEEm(u)_ind', 'NEP(u)_ind', 'NEPm(u)_ind']]
df_ufluxv2 = df_ufluxv2 / coef_PgC_gC * 365
# ------------------------------------------------------------------------------
df_ufluxe = []
for k in ['npp', 'ra', 'rh', 'reco', 'nep', 'nee']:
    dft = (ufluxv2e_dd[k].where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365
    df_ufluxe.append(dft)
    del(dft)
df_ufluxe = pd.concat(df_ufluxe, axis = 1)
# ------------------------------------------------------------------------------
df_trendy = []
for k in trendy_dd.keys():
    dft = (trendy_dd[k].where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365
    df_trendy.append(dft)
    del(dft)
df_trendy = pd.concat(df_trendy, axis = 1)

print([c for c in df_ufluxv2.columns if 'm(u)' not in c])
print([c for c in df_ufluxe.columns if not (c.endswith('_25th') or c.endswith('_75th'))])
print([c for c in df_trendy.columns if not (c.endswith('_25th') or c.endswith('_75th'))])

scale_respiration = (df_ufluxv2['RECO(u)'] / (df_ufluxe['Ra(e)'] + df_ufluxe['Rh(e)'])).mean()
scale_productivity = ((df_ufluxv2['GPP(u)'] - df_ufluxe['Ra(e)']) / df_ufluxe['NPP(e)']).mean()

print(scale_respiration, scale_productivity)

scale_respiration_ra = 1.06
scale_respiration_rh = 1.1
scale_productivity = 1.4

"""## Rescale Reco and NPP
Update: DO NOT RESCALE NPP
"""

'''
OBSOLETE CODE
'''
# df_path_ufluxv2 = []
# for p in root_proj_trendy.joinpath('3_output/UFLUXv2').glob('*.nc'):
#     year, _, _ = p.stem.split('-')
#     df_path_ufluxv2.append([pd.to_datetime(year, format = '%Y'), p])
# df_path_ufluxv2 = pd.DataFrame(df_path_ufluxv2, columns = ['time', 'path']).set_index('time')

# # ------------------------------------------------------------------------------

# df_path_ufluxv2e = []
# for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2e').glob('*.nc'):
#     year, cname, _ = p.stem.split('-')
#     df_path_ufluxv2e.append([pd.to_datetime(year, format = '%Y'), cname, p])
# df_path_ufluxv2e = pd.DataFrame(df_path_ufluxv2e, columns = ['time', 'cname', 'path']).pivot(index = 'time', columns = ['cname']).droplevel(0, axis = 1)

# # ------------------------------------------------------------------------------
# savefolder = root_proj_trendy.joinpath(f'3_output/UFLUXv2e_rescale')
# savefolder.mkdir(exist_ok = True)

# for dt in tqdm(df_path_ufluxv2.index):
#     savefile_gpp = savefolder.joinpath(f'{dt.year}-gpp-TRENDYe.nc')
#     if savefile_gpp.exists(): continue

#     p_ufluxv2 = df_path_ufluxv2.loc[dt, 'path']
#     p_ufluxv2e_ra = df_path_ufluxv2e.loc[dt, 'ra']
#     p_ufluxv2e_rh = df_path_ufluxv2e.loc[dt, 'rh']
#     p_ufluxv2e_npp = df_path_ufluxv2e.loc[dt, 'npp']

#     ufluxv2 = xr.open_dataset(p_ufluxv2, engine = 'netcdf4')
#     uflux2e_ra = xr.open_dataset(p_ufluxv2e_ra, engine = 'netcdf4')
#     uflux2e_rh = xr.open_dataset(p_ufluxv2e_rh, engine = 'netcdf4')
#     uflux2e_npp = xr.open_dataset(p_ufluxv2e_npp, engine = 'netcdf4')

#     uflux2e_reco = (uflux2e_ra['ra'] + uflux2e_rh['rh'])
#     f_arr = ufluxv2['RECOmML'].mean() / uflux2e_reco.mean()

#     uflux2e_ra_rescale = uflux2e_ra * f_arr
#     uflux2e_rh_rescale = uflux2e_rh * f_arr

#     ufluxv2_npp = ufluxv2['GPPmML'] - uflux2e_ra_rescale['ra']
#     f_arr = ufluxv2_npp.mean() / uflux2e_npp['npp'].mean()
#     uflux2e_npp_rescale = uflux2e_npp * f_arr

#     uflux2e_gpp_rescale = xr.merge([
#         (uflux2e_npp_rescale['npp_25th'] + uflux2e_ra_rescale['ra_25th']).rename('gpp_25th'),
#         (uflux2e_npp_rescale['npp'] + uflux2e_ra_rescale['ra']).rename('gpp'),
#         (uflux2e_npp_rescale['npp_75th'] + uflux2e_ra_rescale['ra_75th']).rename('gpp_75th')
#     ])
#     savefile = savefile_gpp
#     if not savefile.exists(): uflux2e_gpp_rescale.to_netcdf(savefile)
#     savefile = savefolder.joinpath(f'{dt.year}-npp-TRENDYe.nc')
#     if not savefile.exists(): uflux2e_npp_rescale.to_netcdf(savefile)
#     savefile = savefolder.joinpath(f'{dt.year}-ra-TRENDYe.nc')
#     if not savefile.exists(): uflux2e_ra_rescale.to_netcdf(savefile)
#     savefile = savefolder.joinpath(f'{dt.year}-rh-TRENDYe.nc')
#     if not savefile.exists(): uflux2e_rh_rescale.to_netcdf(savefile)

#     del(uflux2e_gpp_rescale, uflux2e_npp_rescale, uflux2e_ra_rescale, uflux2e_rh_rescale)
#     del(savefile_gpp, savefile)

# del(savefolder)

df_path_ufluxv2 = []
for p in root_proj_trendy.joinpath('3_output/UFLUXv2').glob('*.nc'):
    year, _, _ = p.stem.split('-')
    df_path_ufluxv2.append([pd.to_datetime(year, format = '%Y'), p])
df_path_ufluxv2 = pd.DataFrame(df_path_ufluxv2, columns = ['time', 'path']).set_index('time')

# ------------------------------------------------------------------------------

df_path_ufluxv2e = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv2e').glob('*.nc'):
    year, cname, _ = p.stem.split('-')
    df_path_ufluxv2e.append([pd.to_datetime(year, format = '%Y'), cname, p])
df_path_ufluxv2e = pd.DataFrame(df_path_ufluxv2e, columns = ['time', 'cname', 'path']).pivot(index = 'time', columns = ['cname']).droplevel(0, axis = 1)

# ------------------------------------------------------------------------------

scale_respiration_ra = 1.06
scale_respiration_rh = 1.1
scale_productivity = 1.4

# ------------------------------------------------------------------------------

savefolder = root_proj_trendy.joinpath(f'3_output/UFLUXv2e_rescale')
savefolder.mkdir(exist_ok = True)

for dt in tqdm(df_path_ufluxv2.index):
    savefile_gpp = savefolder.joinpath(f'{dt.year}-gpp-TRENDYe.nc')
    if savefile_gpp.exists(): continue

    p_ufluxv2 = df_path_ufluxv2.loc[dt, 'path']
    p_ufluxv2e_ra = df_path_ufluxv2e.loc[dt, 'ra']
    p_ufluxv2e_rh = df_path_ufluxv2e.loc[dt, 'rh']
    p_ufluxv2e_npp = df_path_ufluxv2e.loc[dt, 'npp']

    ufluxv2 = xr.open_dataset(p_ufluxv2, engine = 'netcdf4')
    uflux2e_ra = xr.open_dataset(p_ufluxv2e_ra, engine = 'netcdf4')
    uflux2e_rh = xr.open_dataset(p_ufluxv2e_rh, engine = 'netcdf4')
    uflux2e_npp = xr.open_dataset(p_ufluxv2e_npp, engine = 'netcdf4')


    uflux2e_ra_rescale = uflux2e_ra * scale_respiration_ra
    uflux2e_rh_rescale = uflux2e_rh * scale_respiration_rh

    uflux2e_npp_rescale = uflux2e_npp * scale_productivity

    uflux2e_gpp_rescale = xr.merge([
        (uflux2e_npp_rescale['npp_25th'] + uflux2e_ra_rescale['ra_25th']).rename('gpp_25th'),
        (uflux2e_npp_rescale['npp'] + uflux2e_ra_rescale['ra']).rename('gpp'),
        (uflux2e_npp_rescale['npp_75th'] + uflux2e_ra_rescale['ra_75th']).rename('gpp_75th')
    ])
    savefile = savefile_gpp
    if not savefile.exists(): uflux2e_gpp_rescale.to_netcdf(savefile)
    # savefile = savefolder.joinpath(f'{dt.year}-npp-TRENDYe.nc')
    # if not savefile.exists(): uflux2e_npp_rescale.to_netcdf(savefile)
    savefile = savefolder.joinpath(f'{dt.year}-ra-TRENDYe.nc')
    if not savefile.exists(): uflux2e_ra_rescale.to_netcdf(savefile)
    savefile = savefolder.joinpath(f'{dt.year}-rh-TRENDYe.nc')
    if not savefile.exists(): uflux2e_rh_rescale.to_netcdf(savefile)

    del(uflux2e_gpp_rescale, uflux2e_npp_rescale, uflux2e_ra_rescale, uflux2e_rh_rescale)
    del(savefile_gpp, savefile)

del(savefolder)

# UFLUXv1:
df_path_ufluxv1 = []
for p in root_proj_trendy.joinpath('3_output/UFLUXv1').glob('*.nc'):
    year, _, _ = p.stem.split('-')
    df_path_ufluxv1.append([pd.to_datetime(year, format = '%Y'), p])
df_path_ufluxv1 = pd.DataFrame(df_path_ufluxv1, columns = ['time', 'path']).set_index('time')

# ------------------------------------------------------------------------------

df_path_ufluxv1e = []
for p in root_proj_trendy.joinpath(f'3_output/UFLUXv1e').glob('*.nc'):
    year, cname, _ = p.stem.split('-')
    df_path_ufluxv1e.append([pd.to_datetime(year, format = '%Y'), cname, p])
df_path_ufluxv1e = pd.DataFrame(df_path_ufluxv1e, columns = ['time', 'cname', 'path']).pivot(index = 'time', columns = ['cname']).droplevel(0, axis = 1)

# ------------------------------------------------------------------------------

scale_respiration_ra = 1.06
scale_respiration_rh = 1.1
scale_productivity = 1.4

# ------------------------------------------------------------------------------

savefolder = root_proj_trendy.joinpath(f'3_output/UFLUXv1e_rescale')
savefolder.mkdir(exist_ok = True)

for dt in tqdm(df_path_ufluxv1.index):
    savefile_gpp = savefolder.joinpath(f'{dt.year}-gpp-TRENDYe.nc')
    if savefile_gpp.exists(): continue

    p_ufluxv1 = df_path_ufluxv1.loc[dt, 'path']
    p_ufluxv1e_ra = df_path_ufluxv1e.loc[dt, 'ra']
    p_ufluxv1e_rh = df_path_ufluxv1e.loc[dt, 'rh']
    p_ufluxv1e_npp = df_path_ufluxv1e.loc[dt, 'npp']

    ufluxv1 = xr.open_dataset(p_ufluxv1, engine = 'netcdf4')
    uflux1e_ra = xr.open_dataset(p_ufluxv1e_ra, engine = 'netcdf4')
    uflux1e_rh = xr.open_dataset(p_ufluxv1e_rh, engine = 'netcdf4')
    uflux1e_npp = xr.open_dataset(p_ufluxv1e_npp, engine = 'netcdf4')

    uflux1e_ra_rescale = uflux1e_ra * scale_respiration_ra
    uflux1e_rh_rescale = uflux1e_rh * scale_respiration_rh

    uflux1e_npp_rescale = uflux1e_npp * scale_productivity

    uflux1e_gpp_rescale = xr.merge([
        (uflux1e_npp_rescale['npp_25th'] + uflux1e_ra_rescale['ra_25th']).rename('gpp_25th'),
        (uflux1e_npp_rescale['npp'] + uflux1e_ra_rescale['ra']).rename('gpp'),
        (uflux1e_npp_rescale['npp_75th'] + uflux1e_ra_rescale['ra_75th']).rename('gpp_75th')
    ])
    savefile = savefile_gpp
    # if not savefile.exists(): uflux1e_gpp_rescale.to_netcdf(savefile, engine = 'scipy')
    # savefile = savefolder.joinpath(f'{dt.year}-npp-TRENDYe.nc')
    if not savefile.exists(): uflux1e_npp_rescale.to_netcdf(savefile, engine = 'scipy')
    savefile = savefolder.joinpath(f'{dt.year}-ra-TRENDYe.nc')
    if not savefile.exists(): uflux1e_ra_rescale.to_netcdf(savefile, engine = 'scipy')
    savefile = savefolder.joinpath(f'{dt.year}-rh-TRENDYe.nc')
    if not savefile.exists(): uflux1e_rh_rescale.to_netcdf(savefile, engine = 'scipy')

    del(uflux1e_gpp_rescale, uflux1e_npp_rescale, uflux1e_ra_rescale, uflux1e_rh_rescale)
    del(savefile_gpp, savefile)

del(savefolder)

"""## UFLUXv2, UFLUXe, and Trendy IAV (for testing rescaling)"""

ufluxv2 = ufluxv2_a[['GPP(u)']].drop_vars('spatial_ref')
trendy = trendy_dd['gpp']

# ------------------------------------------------------------------------------

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

# dfp = pd.concat([
#     ufluxv2.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     trendy.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
# ], axis = 1)

dfp = pd.concat([
    (ufluxv2.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (ufluxv2e.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (trendy.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
], axis = 1)

dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]
print(dfp.columns)

# # colorful:
# ax.plot(dfp.index, dfp['GPP(u)'], c = colors[0], label = 'GPP(u)')
# ax.plot(dfp.index, dfp['GPP(t)'], c = colors[2], label = 'GPP(t)')
# ax.fill_between(dfp.index, dfp['GPP(t)_25th'], dfp['GPP(t)_75th'], color = colors[2], alpha = 0.2)
# grey:
ax.plot(dfp.index, dfp['GPP(u)'], 'k-', label = 'GPP(u)')
ax.plot(dfp.index, dfp['GPP(t)'], 'k:', label = 'GPP(t)')
# ax.set_ylim(0, 5)
# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('GPP (gC m-2d-1)')

ufluxv2 = ufluxv2_a[['GPP(u)']].drop_vars('spatial_ref')
ufluxv2e = ufluxv2e_dd['gpp'].drop_vars('spatial_ref')
trendy = trendy_dd['gpp']

# ------------------------------------------------------------------------------

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

# dfp = pd.concat([
#     ufluxv2.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     ufluxv2e.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     trendy.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
# ], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

dfp = pd.concat([
    (ufluxv2.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (ufluxv2e.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (trendy.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

# dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]
print(dfp.columns)

# colorful:
ax.plot(dfp.index, dfp['GPP(u)'], c = colors[0], label = 'GPP(u)')
ax.plot(dfp.index, dfp['GPP(e)'], c = colors[1], label = 'GPP(e)')
ax.fill_between(dfp.index, dfp['GPP(e)_25th'], dfp['GPP(e)_75th'], color = colors[1], alpha = 0.2)
ax.plot(dfp.index, dfp['GPP(t)'], c = colors[2], label = 'GPP(t)')
ax.fill_between(dfp.index, dfp['GPP(t)_25th'], dfp['GPP(t)_75th'], color = colors[2], alpha = 0.2)
# # grey:
# ax.plot(dfp.index, dfp['GPP(u)'], 'k-', label = 'GPP(u)')
# ax.plot(dfp.index, dfp['GPP(e)'], 'k--', label = 'GPP(e)')
# ax.plot(dfp.index, dfp['GPP(t)'], 'k:', label = 'GPP(t)')
# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('GPP (gC m-2d-1)')

ufluxv2 = ufluxv2_a[['RECO(u)']].drop_vars('spatial_ref')
ufluxv2e = ufluxv2e_dd['reco'].drop_vars('spatial_ref')
trendy = trendy_dd['reco']

# ------------------------------------------------------------------------------

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

# dfp = pd.concat([
#     ufluxv2.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     ufluxv2e.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     trendy.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
# ], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

dfp = pd.concat([
    (ufluxv2.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (ufluxv2e.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (trendy.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

# dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]
print(dfp.columns)

# colorful:
ax.plot(dfp.index, dfp['RECO(u)'], c = colors[0], label = 'RECO(u)')
ax.plot(dfp.index, dfp['RECO(e)'], c = colors[1], label = 'RECO(e)')
ax.fill_between(dfp.index, dfp['RECO(e)_25th'], dfp['RECO(e)_75th'], color = colors[1], alpha = 0.2)
ax.plot(dfp.index, dfp['RECO(t)'], c = colors[2], label = 'RECO(t)')
ax.fill_between(dfp.index, dfp['RECO(t)_25th'], dfp['RECO(t)_75th'], color = colors[2], alpha = 0.2)
# # grey:
# ax.plot(dfp.index, dfp['RECO(u)'], 'k-', label = 'RECO(u)')
# ax.plot(dfp.index, dfp['RECO(e)'], 'k--', label = 'RECO(e)')
# ax.plot(dfp.index, dfp['RECO(t)'], 'k:', label = 'RECO(t)')
# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('Reco (gC m-2d-1)')

ufluxv2 = ufluxv2_a['NEP(u)_ind'].rename('NEP(u)').to_dataset().drop_vars('spatial_ref')
ufluxv2e = ufluxv2e_dd['nep'].drop_vars('spatial_ref')
# ufluxv2e = ufluxv2e.where(ufluxv2e['NEP(e)'] > -10, np.nan)
trendy = trendy_dd['nep'] # NEP direct from Trendy
# trendy = xr.merge([
#     (trendy_dd['gpp']['GPP(t)'] - trendy_dd['reco']['RECO(t)']).rename('NEP(t)').to_dataset(),
#     trendy_dd['gpp']['GPP(t)_25th'] - trendy_dd['reco']['RECO(t)_25th'].rename('NEP(t)_25th').to_dataset(),
#     trendy_dd['gpp']['GPP(t)_75th'] - trendy_dd['reco']['RECO(t)_75th'].rename('NEP(t)_75th').to_dataset()
# ]) # NEP from Trendy GPP - Reco

# ------------------------------------------------------------------------------

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

# dfp = pd.concat([
#     ufluxv2.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     ufluxv2e.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     trendy.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
# ], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

dfp = pd.concat([
    (ufluxv2.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (ufluxv2e.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (trendy.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

# dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]
print(dfp.columns)

# colorful:
ax.plot(dfp.index, dfp['NEP(u)'], c = colors[0], label = 'NEP(u)')
ax.plot(dfp.index, dfp['NEP(e)'], c = colors[1], label = 'NEP(e)')
ax.fill_between(dfp.index, dfp['NEP(e)_25th'], dfp['NEP(e)_75th'], color = colors[1], alpha = 0.2)
ax.plot(dfp.index, dfp['NEP(t)'], c = colors[2], label = 'NEP(t)')
ax.fill_between(dfp.index, dfp['NEP(t)_25th'], dfp['NEP(t)_75th'], color = colors[2], alpha = 0.2)
# # grey:
# ax.plot(dfp.index, dfp['NEP(u)'], 'k-', label = 'NEP(u)')
# ax.plot(dfp.index, dfp['NEP(e)'], 'k--', label = 'NEP(e)')
# ax.plot(dfp.index, dfp['NEP(t)'], 'k:', label = 'NEP(t)')
# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('NEP (gC m-2d-1)')

# NEE_dir = ufluxv2_a['NEE(u)'].rename('NEE(u)_dir').drop_vars('spatial_ref')
# NEE_ind = (ufluxv2_a['RECO(u)'] - ufluxv2_a['GPP(u)']).rename('NEE(u)_ind').drop_vars('spatial_ref')
NEE_dir = ufluxv2_a['NEE(u)_dir'].drop_vars('spatial_ref')
NEE_ind = ufluxv2_a['NEE(u)_ind'].drop_vars('spatial_ref')
ufluxv2e = ufluxv2e_dd['nee'].drop_vars('spatial_ref')
trendy = trendy_dd['nee']

# ------------------------------------------------------------------------------

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

# dfp = pd.concat([
#     NEE_dir.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     NEE_ind.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     ufluxv2e.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     trendy.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
# ], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

dfp = pd.concat([
    (NEE_dir.to_dataset().where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (NEE_ind.to_dataset().where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (ufluxv2e.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (trendy.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

# dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]
print(dfp.columns)

# colorful:
ax.plot(dfp.index, dfp['NEE(u)_dir'], c = colors[0], ls = '-.', label = 'NEE(u)_dir')
ax.plot(dfp.index, dfp['NEE(u)_ind'], c = colors[0], label = 'NEE(u)_ind')
ax.plot(dfp.index, dfp['NEE(e)'], c = colors[1], label = 'NEE(e)')
ax.fill_between(dfp.index, dfp['NEE(e)_25th'], dfp['NEE(e)_75th'], color = colors[1], alpha = 0.2)
ax.plot(dfp.index, dfp['NEE(t)'], c = colors[2], label = 'NEE(t)')
ax.fill_between(dfp.index, dfp['NEE(t)_25th'], dfp['NEE(t)_75th'], color = colors[2], alpha = 0.2)
# # grey:
# ax.plot(dfp.index, dfp['NEE(u)_ind'], 'k-', label = 'NEE(u)')
# ax.plot(dfp.index, dfp['NEE(e)'], 'k--', label = 'NEE(e)')
# ax.plot(dfp.index, dfp['NEE(t)'], 'k:', label = 'NEE(t)')
# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('NEE (gC m-2d-1)')

trendy_npp_dir = trendy_dd['npp']
trendy_npp_ind = xr.merge([
    (trendy_dd['gpp']['GPP(t)'] - trendy_dd['ra']['Ra(t)']).rename('NPP(t)').to_dataset(),
    trendy_dd['gpp']['GPP(t)_25th'] - trendy_dd['ra']['Ra(t)_25th'].rename('NPP(t)_25th').to_dataset(),
    trendy_dd['gpp']['GPP(t)_75th'] - trendy_dd['ra']['Ra(t)_75th'].rename('NPP(t)_75th').to_dataset()
]) # NPP from Trendy GPP - Ra
ufluxv2e = ufluxv2e_dd['npp'].drop_vars('spatial_ref')

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

# dfp = pd.concat([
#     trendy_npp_dir.rename({'NPP(t)': 'NPP(t)_dir', 'NPP(t)_25th': 'NPP(t)_dir_25th', 'NPP(t)_75th': 'NPP(t)_dir_75th'}).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     trendy_npp_ind.rename({'NPP(t)': 'NPP(t)_ind', 'NPP(t)_25th': 'NPP(t)_ind_25th', 'NPP(t)_75th': 'NPP(t)_ind_75th'}).mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     ufluxv2e.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe()
# ], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

dfp = pd.concat([
    (trendy_npp_dir.rename({'NPP(t)': 'NPP(t)_dir', 'NPP(t)_25th': 'NPP(t)_dir_25th', 'NPP(t)_75th': 'NPP(t)_dir_75th'}) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (trendy_npp_ind.rename({'NPP(t)': 'NPP(t)_ind', 'NPP(t)_25th': 'NPP(t)_ind_25th', 'NPP(t)_75th': 'NPP(t)_ind_75th'}) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (ufluxv2e.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365
], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

# dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]
print(dfp.columns)

# colorful:
ax.plot(dfp.index, dfp['NPP(t)_dir'], c = colors[0], label = 'NPP(t)_dir')
ax.plot(dfp.index, dfp['NPP(t)_ind'], c = colors[1], label = 'NPP(t)_ind')
ax.fill_between(dfp.index, dfp['NPP(t)_dir_25th'], dfp['NPP(t)_dir_75th'], color = colors[0], alpha = 0.2)
ax.fill_between(dfp.index, dfp['NPP(t)_ind_25th'], dfp['NPP(t)_ind_75th'], color = colors[1], alpha = 0.2)
ax.plot(dfp.index, dfp['NPP(e)'], c = colors[2], label = 'NPP(e)')
ax.fill_between(dfp.index, dfp['NPP(e)_25th'], dfp['NPP(e)_75th'], color = colors[2], alpha = 0.2)
# # grey:
# ax.plot(dfp.index, dfp['NPP(t)_dir'], 'k-', label = 'NPP(t)_dir')
# ax.plot(dfp.index, dfp['NPP(t)_ind'], 'k:', label = 'NPP(t)_ind')
# ax.plot(dfp.index, dfp['NPP(e)'], 'k--', label = 'NPP(e)')

# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('NPP (gC m-2d-1)')

(trendy_npp_dir['NPP(t)'] - trendy_npp_ind['NPP(t)']).rename('NPP(t)d').mean(dim = 'time').plot()

def load_trendy_cpool(cname):
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_yearly_2000-2022.nc')
    trendy = xr.open_dataset(p, engine="netcdf4") * 1000 # kg m-2 => g m-2
    trendy = trendy.where(trendy[cname] != 0, np.nan)
    trendy = trendy.rename({cname: f'{cname}(t)', f'{cname}_25th': f'{cname}(t)_25th', f'{cname}_75th': f'{cname}(t)_75th'})
    return trendy

cname = 'cLeaf'
ufluxv2e = ufluxv2e_dd[cname]
trendy = load_trendy_cpool(cname)
dfp = pd.concat([
    ufluxv2e.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
    trendy.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

# dfp = pd.concat([
#     (ufluxv2e.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC,
#     (trendy.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC,
# ], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

dfp = dfp[dfp.columns[dfp.columns != 'IGBP']]
print(dfp.columns)

# colorful:
ax.plot(dfp.index, dfp[f'{cname}(t)'], c = colors[0], label = f'{cname}(t)')
ax.plot(dfp.index, dfp[f'{cname}(e)'], c = colors[1], label = f'{cname}(e)')
ax.fill_between(dfp.index, dfp[f'{cname}(t)_25th'], dfp[f'{cname}(t)_75th'], color = colors[0], alpha = 0.2)
ax.fill_between(dfp.index, dfp[f'{cname}(e)_25th'], dfp[f'{cname}(e)_75th'], color = colors[1], alpha = 0.2)
# # grey:
# ax.plot(dfp.index, dfp[f'{cname}(t)'], 'k-', label = f'{cname}(t)')
# ax.plot(dfp.index, dfp[f'{cname}(e)'], 'k:', label = f'{cname}(e)') # 'k--'

# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel(f'{cname} (gC m-2)')

def load_trendy_cpool(cname):
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_yearly_2000-2022.nc')
    trendy = xr.open_dataset(p, engine="netcdf4") * 1000 # kg m-2 => g m-2
    trendy = trendy.where(trendy[cname] != 0, np.nan)
    trendy = trendy.rename({cname: f'{cname}(t)', f'{cname}_25th': f'{cname}(t)_25th', f'{cname}_75th': f'{cname}(t)_75th'})
    return trendy

cname = 'cSoil'
ufluxv2e = ufluxv2e_dd[cname]
trendy = load_trendy_cpool(cname)
dfp = pd.concat([
    ufluxv2e.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
    trendy.where(cond, np.nan).mean(dim = ['longitude', 'latitude']).to_dataframe(),
], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

# dfp = pd.concat([
#     (ufluxv2e.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC,
#     (trendy.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC,
# ], axis = 1).drop(columns = ['spatial_ref'], axis = 1)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

dfp = dfp[dfp.columns[dfp.columns != 'IGBP']]
print(dfp.columns)

# colorful:
ax.plot(dfp.index, dfp[f'{cname}(t)'], c = colors[0], label = f'{cname}(t)')
ax.plot(dfp.index, dfp[f'{cname}(e)'], c = colors[1], label = f'{cname}(e)')
ax.fill_between(dfp.index, dfp[f'{cname}(t)_25th'], dfp[f'{cname}(t)_75th'], color = colors[0], alpha = 0.2)
ax.fill_between(dfp.index, dfp[f'{cname}(e)_25th'], dfp[f'{cname}(e)_75th'], color = colors[1], alpha = 0.2)
# # grey:
# ax.plot(dfp.index, dfp[f'{cname}(t)'], 'k-', label = f'{cname}(t)')
# ax.plot(dfp.index, dfp[f'{cname}(e)'], 'k:', label = f'{cname}(e)') # 'k--'

# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel(f'{cname} (gC m-2)')

"""## IAV for paper(s)"""

df_ufluxv2 = (ufluxv2_a.where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe().copy()
df_ufluxv2 = df_ufluxv2[['GPPm(u)', 'GPP(u)', 'RECOm(u)', 'RECO(u)', 'NEEm(u)_dir', 'NEE(u)_dir', 'NEP(u)_dir', 'NEPm(u)_dir', 'NEE(u)_ind', 'NEEm(u)_ind', 'NEP(u)_ind', 'NEPm(u)_ind']]
df_ufluxv2 = df_ufluxv2 / coef_PgC_gC * 365
# ------------------------------------------------------------------------------
df_ufluxv1 = (ufluxv1_a.where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe().copy()
df_ufluxv1 = df_ufluxv1[['GPP(u1)', 'RECO(u1)', 'NEE(u1)_dir', 'NEP(u1)_dir', 'NEE(u1)_ind', 'NEP(u1)_ind']]
df_ufluxv1 = df_ufluxv1 / coef_PgC_gC * 365
# ufluxv1 ensemble
df_ufluxv1_ensemble = (ufluxv1_a_ensemble.where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe().copy()
df_ufluxv1_ensemble = df_ufluxv1_ensemble[['GPP(u1)_min', 'GPP(u1)', 'GPP(u1)_max', 'RECO(u1)_min', 'RECO(u1)', 'RECO(u1)_max', 'NEE(u1)_min_dir', 'NEE(u1)_dir', 'NEE(u1)_max_dir', 'NEP(u1)_min_dir', 'NEP(u1)_dir', 'NEP(u1)_max_dir', 'NEE(u1)_ind', 'NEP(u1)_ind']]
df_ufluxv1_ensemble = df_ufluxv1_ensemble / coef_PgC_gC * 365

df_uflux = pd.concat([df_ufluxv1, df_ufluxv2], axis = 1)
# ------------------------------------------------------------------------------
# UFLUXv1e
df_ufluxv1e = []
for k in ['gpp', 'npp', 'ra', 'rh', 'reco', 'nep', 'nee']:
    dft = (ufluxv1e_dd[k].where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365
    dft.columns = [c.replace('(e)', '(e1)') for c in dft.columns]
    df_ufluxv1e.append(dft)
    del(dft)
df_ufluxv1e = pd.concat(df_ufluxv1e, axis = 1)
# ------------------------------------------------------------------------------
# UFLUXv2e
df_ufluxv2e = []
for k in ['gpp', 'npp', 'ra', 'rh', 'reco', 'nep', 'nee']:
    dft = (ufluxv2e_dd[k].where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365
    df_ufluxv2e.append(dft)
    del(dft)
df_ufluxv2e = pd.concat(df_ufluxv2e, axis = 1)

df_ufluxe = pd.concat([df_ufluxv1e, df_ufluxv2e], axis = 1)
# ------------------------------------------------------------------------------
df_trendy = []
for k in trendy_dd.keys():
    dft = (trendy_dd[k].where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365
    df_trendy.append(dft)
    del(dft)
df_trendy = pd.concat(df_trendy, axis = 1)

print([c for c in df_uflux.columns if 'm(u)' not in c])
print([c for c in df_ufluxe.columns if not (c.endswith('_25th') or c.endswith('_75th'))])
print([c for c in df_trendy.columns if not (c.endswith('_25th') or c.endswith('_75th'))])

df_gpp_adj = (df_ufluxv1e['NPP(e1)'] + df_ufluxe['Ra(e1)']).rename('GPP(e1)').to_frame()
df_gpp_adj = pd.concat([
    # df_ufluxv1[f'GPP(u1)'].rename('GPP(e1)').to_frame(),
    df_gpp_adj,
    (df_ufluxv1e['NPP(e1)_25th'] + df_ufluxe['Ra(e1)'] ).rename('GPP(e1)_25th').to_frame(),
    (df_ufluxv1e['NPP(e1)_75th'] + df_ufluxe['Ra(e1)'] ).rename('GPP(e1)_75th').to_frame()
], axis = 1)

# fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

# cname = 'GPP'
# dfp = pd.concat([
#     # df_ufluxv2[[f'{cname}(u)']],
#     df_ufluxv1[[f'{cname}(u1)']],
#     df_gpp_adj,
#     # df_ufluxe[[f'{cname}(e1)', f'{cname}(e1)_25th', f'{cname}(e1)_75th']],
#     df_ufluxe[[f'{cname}(e)', f'{cname}(e)_25th', f'{cname}(e)_75th']],
#     df_trendy[[f'{cname}(t)', f'{cname}(t)_25th', f'{cname}(t)_75th']]
# ], axis = 1)['2003'::]

# dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]

# chromatic = 1

# if chromatic:
#     # colorful:
#     # ax.plot(dfp.index, dfp[f'{cname}(u)'], 'k-.', label = f'{cname}(UFLUXv2)')
#     ax.plot(dfp.index, dfp[f'{cname}(u1)'], 'k:', label = f'{cname}(UFLUXv1)')
#     ax.scatter(dfp.index, dfp[f'{cname}(u1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)

#     ax.plot(dfp.index, dfp[f'{cname}(e1)'], c = colors[0], label = f'{cname}(UFLUX)')
#     ax.scatter(dfp.index, dfp[f'{cname}(e1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
#     ax.fill_between(dfp.index, dfp[f'{cname}(e1)_25th'], dfp[f'{cname}(e1)_75th'], color = colors[0], alpha = 0.2)

#     # ax.plot(dfp.index, dfp[f'{cname}(e)'], c = colors[0], label = f'{cname}(UFLUX)')
#     # ax.scatter(dfp.index, dfp[f'{cname}(e)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
#     # ax.fill_between(dfp.index, dfp[f'{cname}(e)_25th'], dfp[f'{cname}(e)_75th'], color = colors[0], alpha = 0.2)


#     ax.plot(dfp.index, dfp[f'{cname}(t)'], c = colors[1], label = f'{cname}(Trendy)')
#     ax.scatter(dfp.index, dfp[f'{cname}(t)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
#     ax.fill_between(dfp.index, dfp[f'{cname}(t)_25th'], dfp[f'{cname}(t)_75th'], color = colors[1], alpha = 0.2)
# else:
#     # grey:
#     ax.plot(dfp.index, dfp[f'{cname}(e)'], 'k-', label = f'{cname}(UFLUX)')
#     ax.plot(dfp.index, dfp[f'{cname}(t)'], 'k:', label = f'{cname}(Trendy)')
# upper_legend(ax, nrows = 1, yloc = 1.15)
# ax.set_ylabel(cname + ' ($PgC \ yr^{-1}$)')

# # google.download_file(fig, f'{cname}_IAV.png')

# ensemble runs

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

cname = 'GPP'
dfp = pd.concat([
    # df_ufluxv2[[f'{cname}(u)']],
    # df_ufluxv1[[f'{cname}(u1)']],
    df_ufluxv1_ensemble[[f'{cname}(u1)_min', f'{cname}(u1)', f'{cname}(u1)_max']],
    df_gpp_adj,
    # df_ufluxe[[f'{cname}(e1)', f'{cname}(e1)_25th', f'{cname}(e1)_75th']],
    df_ufluxe[[f'{cname}(e)', f'{cname}(e)_25th', f'{cname}(e)_75th']],
    df_trendy[[f'{cname}(t)', f'{cname}(t)_25th', f'{cname}(t)_75th']]
], axis = 1)['2003'::]

dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]

chromatic = 1

if chromatic:
    # colorful:
    # ax.plot(dfp.index, dfp[f'{cname}(u)'], 'k-.', label = f'{cname}(UFLUXv2)')
    ax.plot(dfp.index, dfp[f'{cname}(e1)'], 'k:', label = f'{cname}(NPP + Ra)')
    ax.scatter(dfp.index, dfp[f'{cname}(e1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)

    ax.plot(dfp.index, dfp[f'{cname}(u1)'], c = colors[0], label = f'{cname}(UFLUX)')
    ax.scatter(dfp.index, dfp[f'{cname}(u1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    # ax.fill_between(dfp.index, dfp[f'{cname}(e1)_25th'], dfp[f'{cname}(e1)_75th'], color = colors[0], alpha = 0.2)
    ax.fill_between(dfp.index, dfp[f'{cname}(u1)_min'], dfp[f'{cname}(u1)_max'], color = 'gray', alpha = 0.2)

    # ax.plot(dfp.index, dfp[f'{cname}(e)'], c = colors[0], label = f'{cname}(UFLUX)')
    # ax.scatter(dfp.index, dfp[f'{cname}(e)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    # ax.fill_between(dfp.index, dfp[f'{cname}(e)_25th'], dfp[f'{cname}(e)_75th'], color = colors[0], alpha = 0.2)


    ax.plot(dfp.index, dfp[f'{cname}(t)'], c = colors[1], label = f'{cname}(Trendy)')
    ax.scatter(dfp.index, dfp[f'{cname}(t)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    ax.fill_between(dfp.index, dfp[f'{cname}(t)_25th'], dfp[f'{cname}(t)_75th'], color = colors[1], alpha = 0.2)
else:
    # grey:
    ax.plot(dfp.index, dfp[f'{cname}(e)'], 'k-', label = f'{cname}(UFLUX)')
    ax.plot(dfp.index, dfp[f'{cname}(t)'], 'k:', label = f'{cname}(Trendy)')
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel(cname + ' ($PgC \ yr^{-1}$)')

lrru = stats.linregress(dfp.index.year, dfp[f'{cname}(u1)'])
print('UFLUX', roundit(lrru.slope), roundit(lrru.pvalue))
lrrt = stats.linregress(dfp.index.year, dfp[f'{cname}(t)'])
print('TRENDY', roundit(lrrt.slope), roundit(lrrt.pvalue))

roundit([
    dfp[f'{cname}(e1)'].std() / dfp[f'{cname}(e1)'].mean(),
    dfp[f'{cname}(u1)'].std() / dfp[f'{cname}(u1)'].mean(),
    dfp[f'{cname}(t)'].std() / dfp[f'{cname}(t)'].mean()
])

# google.download_file(fig, f'{cname}_IAV.png')

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

cname = 'RECO'
dfp = pd.concat([
    # df_ufluxv2[[f'{cname}(u)']],
    df_ufluxv1[[f'{cname}(u1)']],
    df_ufluxe[[f'{cname}(e1)', f'{cname}(e1)_25th', f'{cname}(e1)_75th']],
    df_ufluxe[[f'{cname}(e)', f'{cname}(e)_25th', f'{cname}(e)_75th']],
    df_trendy[[f'{cname}(t)', f'{cname}(t)_25th', f'{cname}(t)_75th']]
], axis = 1)['2003'::]

dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]

chromatic = 1

if chromatic:
    # colorful:
    # ax.plot(dfp.index, dfp[f'{cname}(u)'], 'k-.', label = f'{cname}(UFLUXv2)')
    ax.plot(dfp.index, dfp[f'{cname}(u1)'], 'k:', label = f'{cname}(Ra + Rh)')

    ax.plot(dfp.index, dfp[f'{cname}(e1)'], c = colors[0], label = f'{cname}(UFLUX)')
    ax.scatter(dfp.index, dfp[f'{cname}(e1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    ax.fill_between(dfp.index, dfp[f'{cname}(e1)_25th'], dfp[f'{cname}(e1)_75th'], color = colors[0], alpha = 0.2)

    # ax.plot(dfp.index, dfp[f'{cname}(e)'], c = colors[0], label = f'{cname}(UFLUX)')
    # ax.scatter(dfp.index, dfp[f'{cname}(e)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    # ax.fill_between(dfp.index, dfp[f'{cname}(e)_25th'], dfp[f'{cname}(e)_75th'], color = colors[0], alpha = 0.2)


    ax.plot(dfp.index, dfp[f'{cname}(t)'], c = colors[1], label = f'{cname}(Trendy)')
    ax.scatter(dfp.index, dfp[f'{cname}(t)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    ax.fill_between(dfp.index, dfp[f'{cname}(t)_25th'], dfp[f'{cname}(t)_75th'], color = colors[1], alpha = 0.2)
else:
    # grey:
    ax.plot(dfp.index, dfp[f'{cname}(e)'], 'k-', label = f'{cname}(UFLUX)')
    ax.plot(dfp.index, dfp[f'{cname}(t)'], 'k:', label = f'{cname}(Trendy)')
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel(cname + ' ($PgC \ yr^{-1}$)')

lrru = stats.linregress(dfp.index.year, dfp[f'{cname}(e1)'])
print('UFLUX', roundit(lrru.slope), roundit(lrru.pvalue))
lrrt = stats.linregress(dfp.index.year, dfp[f'{cname}(t)'])
print('TRENDY', roundit(lrrt.slope), roundit(lrrt.pvalue))

# google.download_file(fig, f'{cname}_IAV.png')

# (dfp[['GPP(t)', 'GPP(t)_25th', 'GPP(t)_75th']]).to_csv(root.joinpath("workspace/project_data/platform").joinpath('2IAV-new/GPP_Trendyv12.csv'))

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

cname = 'NEP'
dfp = pd.concat([
    # df_ufluxv2[[f'{cname}(u)']],
    # df_ufluxv1[[f'{cname}(u1)_dir']],
    df_ufluxv1_ensemble[[f'{cname}(u1)_min_dir', f'{cname}(u1)_dir', f'{cname}(u1)_max_dir']],
    df_ufluxv1_ensemble[[f'{cname}(u1)_ind']],
    df_ufluxe[[f'{cname}(e1)', f'{cname}(e1)_25th', f'{cname}(e1)_75th']],
    # df_ufluxe[[f'{cname}(e)', f'{cname}(e)_25th', f'{cname}(e)_75th']],
    df_trendy[[f'{cname}(t)', f'{cname}(t)_25th', f'{cname}(t)_75th']]
], axis = 1)['2003'::]

dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]

chromatic = 1

if chromatic:
    # colorful:
    # ax.plot(dfp.index, dfp[f'{cname}(u)'], 'k-.', label = f'{cname}(UFLUXv2)')
    ax.plot(dfp.index, dfp[f'{cname}(u1)_dir'], 'k:', label = f'{cname}(dir.)')
    ax.plot(dfp.index, dfp[f'{cname}(u1)_ind'], 'k-', label = f'{cname}(ind.)')

    ax.plot(dfp.index, dfp[f'{cname}(e1)'], c = colors[0], label = f'{cname}(UFLUX)')
    ax.scatter(dfp.index, dfp[f'{cname}(e1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    # ax.fill_between(dfp.index, dfp[f'{cname}(e1)_25th'], dfp[f'{cname}(e1)_75th'], color = colors[0], alpha = 0.2)
    ax.fill_between(dfp.index, dfp[f'{cname}(u1)_min_dir'], dfp[f'{cname}(u1)_max_dir'], color = 'gray', alpha = 0.2)

    # ax.plot(dfp.index, dfp[f'{cname}(e)'], c = colors[0], label = f'{cname}(UFLUX)')
    # ax.scatter(dfp.index, dfp[f'{cname}(e)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    # ax.fill_between(dfp.index, dfp[f'{cname}(e)_25th'], dfp[f'{cname}(e)_75th'], color = colors[0], alpha = 0.2)


    ax.plot(dfp.index, dfp[f'{cname}(t)'], c = colors[1], label = f'{cname}(Trendy)')
    ax.scatter(dfp.index, dfp[f'{cname}(t)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    ax.fill_between(dfp.index, dfp[f'{cname}(t)_25th'], dfp[f'{cname}(t)_75th'], color = colors[1], alpha = 0.2)
else:
    # grey:
    ax.plot(dfp.index, dfp[f'{cname}(e)'], 'k-', label = f'{cname}(UFLUX)')
    ax.plot(dfp.index, dfp[f'{cname}(t)'], 'k:', label = f'{cname}(Trendy)')
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel(cname + ' ($PgC \ yr^{-1}$)')

lrru = stats.linregress(dfp.index.year, dfp[f'{cname}(e1)'])
print('UFLUX', roundit(lrru.slope), roundit(lrru.pvalue))
lrrt = stats.linregress(dfp.index.year, dfp[f'{cname}(t)'])
print('TRENDY', roundit(lrrt.slope), roundit(lrrt.pvalue))

# google.download_file(fig, f'{cname}_IAV.png')

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

cname = 'NPP'
dfp = pd.concat([
    # df_ufluxv2[[f'{cname}(u)']],
    # df_ufluxv1[[f'{cname}(u1)']],
    df_ufluxe[[f'{cname}(e1)', f'{cname}(e1)_25th', f'{cname}(e1)_75th']],
    df_ufluxe[[f'{cname}(e)', f'{cname}(e)_25th', f'{cname}(e)_75th']],
    df_trendy[[f'{cname}(t)', f'{cname}(t)_25th', f'{cname}(t)_75th']]
], axis = 1)['2003'::]

dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]

chromatic = 1

if chromatic:
    # colorful:
    # ax.plot(dfp.index, dfp[f'{cname}(u)'], 'k-.', label = f'{cname}(UFLUXv2)')
    # ax.plot(dfp.index, dfp[f'{cname}(u1)'], 'k:', label = f'{cname}(UFLUXv1)')

    ax.plot(dfp.index, dfp[f'{cname}(e1)'], c = colors[0], label = f'{cname}(UFLUX)')
    ax.scatter(dfp.index, dfp[f'{cname}(e1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    ax.fill_between(dfp.index, dfp[f'{cname}(e1)_25th'], dfp[f'{cname}(e1)_75th'], color = colors[0], alpha = 0.2)

    # ax.plot(dfp.index, dfp[f'{cname}(e)'], c = colors[0], label = f'{cname}(UFLUX)')
    # ax.scatter(dfp.index, dfp[f'{cname}(e)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    # ax.fill_between(dfp.index, dfp[f'{cname}(e)_25th'], dfp[f'{cname}(e)_75th'], color = colors[0], alpha = 0.2)


    ax.plot(dfp.index, dfp[f'{cname}(t)'], c = colors[1], label = f'{cname}(Trendy)')
    ax.scatter(dfp.index, dfp[f'{cname}(t)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    ax.fill_between(dfp.index, dfp[f'{cname}(t)_25th'], dfp[f'{cname}(t)_75th'], color = colors[1], alpha = 0.2)
else:
    # grey:
    ax.plot(dfp.index, dfp[f'{cname}(e)'], 'k-', label = f'{cname}(UFLUX)')
    ax.plot(dfp.index, dfp[f'{cname}(t)'], 'k:', label = f'{cname}(Trendy)')
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel(cname + ' ($PgC \ yr^{-1}$)')

lrru = stats.linregress(dfp.index.year, dfp[f'{cname}(e1)'])
print('UFLUX', roundit(lrru.slope), roundit(lrru.pvalue))
lrrt = stats.linregress(dfp.index.year, dfp[f'{cname}(t)'])
print('TRENDY', roundit(lrrt.slope), roundit(lrrt.pvalue))

# google.download_file(fig, f'{cname}_IAV.png')

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

cname = 'Rh'
dfp = pd.concat([
    df_ufluxe[[f'{cname}(e1)', f'{cname}(e1)_25th', f'{cname}(e1)_75th']],
    df_ufluxe[[f'{cname}(e)', f'{cname}(e)_25th', f'{cname}(e)_75th']],
    df_trendy[[f'{cname}(t)', f'{cname}(t)_25th', f'{cname}(t)_75th']]
], axis = 1)['2003'::]

dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]

chromatic = 1

if chromatic:
    # colorful:

    ax.plot(dfp.index, dfp[f'{cname}(e1)'], c = colors[0], label = f'{cname}(UFLUX)')
    ax.scatter(dfp.index, dfp[f'{cname}(e1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    ax.fill_between(dfp.index, dfp[f'{cname}(e1)_25th'], dfp[f'{cname}(e1)_75th'], color = colors[0], alpha = 0.2)

    # ax.plot(dfp.index, dfp[f'{cname}(e)'], c = colors[0], label = f'{cname}(UFLUX)')
    # ax.scatter(dfp.index, dfp[f'{cname}(e)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    # ax.fill_between(dfp.index, dfp[f'{cname}(e)_25th'], dfp[f'{cname}(e)_75th'], color = colors[0], alpha = 0.2)


    ax.plot(dfp.index, dfp[f'{cname}(t)'], c = colors[1], label = f'{cname}(Trendy)')
    ax.scatter(dfp.index, dfp[f'{cname}(t)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    ax.fill_between(dfp.index, dfp[f'{cname}(t)_25th'], dfp[f'{cname}(t)_75th'], color = colors[1], alpha = 0.2)
else:
    # grey:
    ax.plot(dfp.index, dfp[f'{cname}(e)'], 'k-', label = f'{cname}(UFLUX)')
    ax.plot(dfp.index, dfp[f'{cname}(t)'], 'k:', label = f'{cname}(Trendy)')
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel(cname + ' ($PgC \ yr^{-1}$)')

# google.download_file(fig, f'{cname}_IAV.png')

def load_trendy_cpool(cname):
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_yearly_2000-2022.nc')
    trendy = xr.open_dataset(p, engine="netcdf4") * 1000 # kg m-2 => g m-2
    trendy = trendy.where(trendy[cname] != 0, np.nan)
    trendy = trendy.rename({cname: f'{cname}(t)', f'{cname}_25th': f'{cname}(t)_25th', f'{cname}_75th': f'{cname}(t)_75th'})
    return trendy

df_cpool = []
for k in ['cLeaf', 'cLitter', 'cSoil', 'cVeg', 'cWood']:
    trendy_ = load_trendy_cpool(k)
    dft_trendy = (trendy_.where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC
    del(trendy_)
    dft_ufluxv2e = (ufluxv2e_dd[k].where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC
    dft_ufluxv1e = (ufluxv1e_dd[k].where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC
    dft_ufluxv1e.columns = [c.replace('(e)', '(e1)') for c in dft_ufluxv1e.columns]
    df_cpool.append(pd.concat([dft_trendy, dft_ufluxv2e.drop('IGBP', axis = 1), dft_ufluxv1e.drop('IGBP', axis = 1)], axis = 1))
    del(dft_trendy, dft_ufluxv2e, dft_ufluxv1e)

df_cpool = pd.concat(df_cpool, axis = 1)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

cname = 'cLitter'
dfp = df_cpool[[f'{cname}(e)', f'{cname}(e)_25th', f'{cname}(e)_75th'] + [f'{cname}(t)', f'{cname}(t)_25th', f'{cname}(t)_75th'] + [f'{cname}(e1)', f'{cname}(e1)_25th', f'{cname}(e1)_75th']]
dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]['2003'::]

chromatic = 1

if chromatic:
    # colorful:
    # ax.plot(dfp.index, dfp[f'{cname}(e)'], c = colors[0], label = f'{cname}(UFLUX)')
    # ax.scatter(dfp.index, dfp[f'{cname}(e)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    # ax.fill_between(dfp.index, dfp[f'{cname}(e)_25th'], dfp[f'{cname}(e)_75th'], color = colors[0], alpha = 0.2)
    # --------------------------------------------------------------------------
    ax.plot(dfp.index, dfp[f'{cname}(e1)'], c = colors[0], label = f'{cname}(UFLUX)')
    ax.scatter(dfp.index, dfp[f'{cname}(e1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    ax.fill_between(dfp.index, dfp[f'{cname}(e1)_25th'], dfp[f'{cname}(e1)_75th'], color = colors[0], alpha = 0.2)
    # --------------------------------------------------------------------------
    ax.plot(dfp.index, dfp[f'{cname}(t)'], c = colors[1], label = f'{cname}(Trendy)')
    ax.scatter(dfp.index, dfp[f'{cname}(t)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
    ax.fill_between(dfp.index, dfp[f'{cname}(t)_25th'], dfp[f'{cname}(t)_75th'], color = colors[1], alpha = 0.2)
else:
    # grey:
    ax.plot(dfp.index, dfp[f'{cname}(e)'], 'k-', label = f'{cname}(UFLUX)')
    ax.plot(dfp.index, dfp[f'{cname}(t)'], 'k:', label = f'{cname}(Trendy)')
    # --------------------------------------------------------------------------
    ax.plot(dfp.index, dfp[f'{cname}(e1)'], 'k-.', label = f'{cname}(UFLUX)')
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel(cname + ' ($PgC$)')

lrru = stats.linregress(dfp.index.year, dfp[f'{cname}(e1)'])
print('UFLUX', roundit(lrru.slope), roundit(lrru.pvalue))
lrrt = stats.linregress(dfp.index.year, dfp[f'{cname}(t)'])
print('TRENDY', roundit(lrrt.slope), roundit(lrrt.pvalue))

# google.download_file(fig, f'{cname}_IAV.png')

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)

dfp = df_cpool[['cVeg(e1)', 'cVeg(e1)_25th', 'cVeg(e1)_75th'] + ['cLitter(e1)', 'cLitter(e1)_25th', 'cLitter(e1)_75th'] + ['cSoil(e1)', 'cSoil(e1)_25th', 'cSoil(e1)_75th']]
dfp = dfp[dfp.columns[dfp.columns != 'spatial_ref']]['2003'::]

chromatic = 1

ax.plot(dfp.index, dfp[f'cVeg(e1)'], c = colors[0], label = f'cVeg')
ax.scatter(dfp.index, dfp[f'cVeg(e1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
ax.fill_between(dfp.index, dfp[f'cVeg(e1)_25th'], dfp[f'cVeg(e1)_75th'], color = colors[0], alpha = 0.2)
# --------------------------------------------------------------------------
ax.plot(dfp.index, dfp[f'cLitter(e1)'], c = colors[1], label = f'cLitter')
ax.scatter(dfp.index, dfp[f'cLitter(e1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
ax.fill_between(dfp.index, dfp[f'cLitter(e1)_25th'], dfp[f'cLitter(e1)_75th'], color = colors[1], alpha = 0.2)
# --------------------------------------------------------------------------
ax.plot(dfp.index, dfp[f'cSoil(e1)'], c = colors[2], label = f'cSoil')
ax.scatter(dfp.index, dfp[f'cSoil(e1)'], s = 30, color = 'w', edgecolor = 'k', zorder = 2)
ax.fill_between(dfp.index, dfp[f'cSoil(e1)_25th'], dfp[f'cSoil(e1)_75th'], color = colors[2], alpha = 0.2)

upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel(' ($PgC$)')

# df_cpool[['cLeaf(e)', 'cWood(e)', 'cVeg(e)', 'cLitter(e)', 'cSoil(e)']].plot()

(df_cpool['cVeg(e1)'][-1] - df_cpool['cVeg(e1)'][3]) / 20

dfp = pd.concat([
    df_ufluxe['NEP(e1)'].rename('NEP(UFLUX)'),
    df_cpool[['cVeg(e)', 'cLitter(e)', 'cSoil(e)']].diff(1).sum(axis = 1).rename('NEP(UFLUX from stock)'),
    df_trendy['NEP(t)'].rename('NEP(Trendy)'),
    df_cpool[['cVeg(t)', 'cLitter(t)', 'cSoil(t)']].diff(1).sum(axis = 1).rename('NEP(Trendy from stock)'),
], axis = 1)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
y_tf_mean = dfp['NEP(UFLUX)'].mean()
y_t_mean = dfp['NEP(UFLUX from stock)'].mean()
# ax.axhline(y = y_tf_mean, color=colors[0], linestyle='-', linewidth = 1)
ax.axhline(y = y_t_mean, color=colors[1], linestyle='-', linewidth = 1)
ax.plot(dfp.index, dfp['NEP(UFLUX)'], linestyle='-', marker='o', color=colors[0], markersize = 5, markerfacecolor='white', markeredgecolor='black', label = "NEP(UFLUX)")
ax.plot(dfp.index, dfp['NEP(UFLUX from stock)'], linestyle=':', marker='o', color=colors[1], markersize = 5, markerfacecolor='white', markeredgecolor='black', label="NEP(UFLUX from stock)", alpha = 0.5)
# ax.text(dfp.index[2], y_t_mean, f"{roundit(y_t_mean)}", fontsize = 10, color='k', verticalalignment='bottom')

ax.annotate(f"{roundit(y_tf_mean)}", xy=(dfp.index[12], y_tf_mean), xytext=(dfp.index[12], y_tf_mean + 15),
            fontsize = 10, color=colors[0], ha='center',
            arrowprops=dict(arrowstyle="->", color='k', lw=1.5))
ax.set_ylabel('NEP $(PgC \ yr^{-1})$')

ax.annotate(f"{roundit(y_t_mean)}", xy=(dfp.index[2], y_t_mean), xytext=(dfp.index[2], y_t_mean + 15),
            fontsize = 10, color=colors[1], ha='center',
            arrowprops=dict(arrowstyle="->", color='k', lw=1.5))
ax.set_ylabel('NEP $(PgC \ yr^{-1})$')
upper_legend(ax, yloc = 1.12)

print(roundit(dfp[['NEP(UFLUX)', 'NEP(UFLUX from stock)']].mean()))

# google.download_file(fig, f'UFLUX_mass_balance.png')

dfp = pd.concat([
    df_ufluxe['NEP(e1)'].rename('NEP(UFLUX)'),
    df_cpool[['cVeg(e)', 'cLitter(e)', 'cSoil(e)']].diff(1).sum(axis = 1).rename('NEP(UFLUX from stock)'),
    df_trendy['NEP(t)'].rename('NEP(Trendy)'),
    df_cpool[['cVeg(t)', 'cLitter(t)', 'cSoil(t)']].diff(1).sum(axis = 1).rename('NEP(Trendy from stock)'),
], axis = 1)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
y_tf_mean = dfp['NEP(Trendy)'].mean()
y_t_mean = dfp['NEP(Trendy from stock)'].mean()
ax.axhline(y = y_tf_mean, color=colors[0], linestyle='-', linewidth = 1)
ax.axhline(y = y_t_mean, color=colors[1], linestyle='-', linewidth = 1)
ax.plot(dfp.index, dfp['NEP(Trendy)'], linestyle=':', marker='o', color=colors[0], markersize = 5, markerfacecolor='white', markeredgecolor='black', label = "NEP(Trendy)")
ax.plot(dfp.index, dfp['NEP(Trendy from stock)'], linestyle=':', marker='o', color=colors[1], markersize = 5, markerfacecolor='white', markeredgecolor='black', label="NEP(Trendy from stock)", alpha = 0.5)
# ax.text(dfp.index[2], y_t_mean, f"{roundit(y_t_mean)}", fontsize = 10, color='k', verticalalignment='bottom')

ax.annotate(f"{roundit(y_tf_mean)}", xy=(dfp.index[12], y_tf_mean), xytext=(dfp.index[12], y_tf_mean + 10),
            fontsize = 10, color=colors[0], ha='center',
            arrowprops=dict(arrowstyle="->", color='k', lw=1.5))
ax.set_ylabel('NEP $(PgC \ yr^{-1})$')

ax.annotate(f"{roundit(y_t_mean)}", xy=(dfp.index[1], y_t_mean), xytext=(dfp.index[1], y_t_mean + 10),
            fontsize = 10, color=colors[1], ha='center',
            arrowprops=dict(arrowstyle="->", color='k', lw=1.5))
ax.set_ylabel('NEP $(PgC \ yr^{-1})$')
upper_legend(ax, yloc = 1.12)

print(roundit(dfp[['NEP(Trendy)', 'NEP(Trendy from stock)']].mean()))

# google.download_file(fig, f'Trendy_mass_balance.png')

df_cpool['cVeg(e)'].mean(), (df_cpool['cWood(e)'] + df_cpool['cLeaf(e)']).mean(), df_cpool['cVeg(t)'].mean(), (df_cpool['cWood(t)'] + df_cpool['cLeaf(t)']).mean()

"""# Analysis maps"""

def load_trendy_cpool(cname):
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_yearly_2000-2022.nc')
    trendy = xr.open_dataset(p, engine="netcdf4") * 1000 # kg m-2 => g m-2
    trendy = trendy.where(trendy[cname] != 0, np.nan)
    trendy = trendy.rename({cname: f'{cname}(t)', f'{cname}_25th': f'{cname}(t)_25th', f'{cname}_75th': f'{cname}(t)_75th'})
    return trendy

for k in tqdm(['cLeaf', 'cLitter', 'cSoil', 'cVeg', 'cWood']):
    trendy_dd[k] = load_trendy_cpool(k)

# cname0 = 'nep'
# if cname0 in ['cWood', 'cLeaf', 'cVeg', 'cLitter', 'cSoil']:
#     cname1 = cname0
# elif cname0 in ['ra', 'rh']:
#     cname1 = cname0.capitalize()
# else:
#     cname1 = cname0.upper()

# # ufluxv2 = ufluxv2_a[[f'{cname1}(u)']].drop_vars('spatial_ref')#.where(cond, np.nan)
# ufluxv1e = ufluxv1e_dd[cname0].drop_vars('spatial_ref')#.where(cond, np.nan)
# ufluxv2e = ufluxv2e_dd[cname0].drop_vars('spatial_ref')#.where(cond, np.nan)
# trendy = trendy_dd[cname0]

# if cname0 == 'npp':
#     arrp = xr.merge([
#         ufluxv1e_dd['npp_unscaled'].drop_vars('spatial_ref')[f'{cname1}(e)'].rename(f'{cname1}(UFLUX)'),
#         trendy[f'{cname1}(t)'].rename(f'{cname1}(Trendy)')
#     ])
# elif cname0 == 'gpp': # !!!!CHECK IF YOU NEED GPPe or GPPu, comment this block if the latter!!!!
#     arrp = xr.merge([
#         (ufluxv1e_dd['npp_unscaled'].drop_vars('spatial_ref')['NPP(e)'] + ufluxv1e_dd['ra'].drop_vars('spatial_ref')['Ra(e)']).rename(f'{cname1}(UFLUX)'),
#         trendy[f'{cname1}(t)'].rename(f'{cname1}(Trendy)')
#     ])
# else:
#     arrp = xr.merge([
#         # ufluxv2[f'{cname1}(u)'].rename(f'{cname1}(UFLUX)'),
#         ufluxv1e[f'{cname1}(e)'].rename(f'{cname1}(UFLUX)'),
#         # ufluxv2e[f'{cname1}(e)'].rename(f'{cname1}(UFLUX)'),
#         trendy[f'{cname1}(t)'].rename(f'{cname1}(Trendy)')
#     ])
# if 'spatial_ref' in arrp.data_vars: arrp = arrp.drop_vars('spatial_ref')
# # arrp = arrp.where(arrp > 0, np.nan)
# arrp = arrp.where(cond, np.nan)

# # ==============================================================================

# fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
# if cname0 == 'nep':
#     arrp.mean(dim = 'time')[f'{cname1}(UFLUX)'].drop_vars('spatial_ref', errors = 'ignore').plot(ax = ax, cbar_kwargs = {'label': f'{cname1}'}, cmap = 'PRGn') #, vmin = -10, vmax = 10, cmap = 'coolwarm')
# else:
#     arrp.mean(dim = 'time')[f'{cname1}(UFLUX)'].drop_vars('spatial_ref', errors = 'ignore').plot(ax = ax, cbar_kwargs = {'label': f'{cname1}'})

# x0 = ax.get_position().x0
# x1 = ax.get_position().x1
# y0 = ax.get_position().y0
# y1 = ax.get_position().y1
# ax1 = fig.add_axes([x0, y0 - 0.25, x1 - x0, 0.2])
# # cbar = fig.colorbar(im, cax=cbar_ax, orientation = 'horizontal')
# # cbar.ax.locator_params(nbins = 5)
# # cbar.set_label(unit_dict['GPP'], labelpad = 0, fontsize = 22)
# ax1.plot(arrp.longitude, arrp.mean(dim = ['time', 'latitude'])[f'{cname1}(UFLUX)'], color = colors[0])
# ax1.plot(arrp.longitude, arrp.mean(dim = ['time', 'latitude'])[f'{cname1}(Trendy)'], color = colors[1])
# ax2 = fig.add_axes([x0 - 0.2, y0, 0.15, y1 - y0])
# ax2.plot(arrp.mean(dim = ['time', 'longitude'])[f'{cname1}(UFLUX)'], arrp.latitude, color = colors[0], label = 'UFLUX')
# ax2.plot(arrp.mean(dim = ['time', 'longitude'])[f'{cname1}(Trendy)'], arrp.latitude, color = colors[1], label = 'Trendy')

# upper_legend(ax2, xloc = 3.3, yloc = 1.13)

# ax.set_xlim(-180, 180)
# ax1.set_xlim(-180, 180)

# ax.set_ylim(-60, 80)
# ax2.set_ylim(-60, 80)

# if cname0 == 'nep':
#     ax1.set_ylim(-2, 10)
#     ax2.set_xlim(-2, 10)

# # world[world['CONTINENT'] == 'Asia'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
# # world[world['CONTINENT'] == 'Africa'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
# # world[world['CONTINENT'] == 'Europe'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
# # world[world['CONTINENT'] == 'North America'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
# # world[world['CONTINENT'] == 'South America'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)

# # google.download_file(fig, f'{cname1}_spatial.png')

cname0 = 'npp'
if cname0 in ['cWood', 'cLeaf', 'cVeg', 'cLitter', 'cSoil']:
    cname1 = cname0
    unit = '$gC \, m^{-2}$'
elif cname0 in ['ra', 'rh']:
    cname1 = cname0.capitalize()
    unit = '$gC \, m^{-2} \ d^{-1}$'
else:
    cname1 = cname0.upper()
    unit = '$gC \, m^{-2} \ d^{-1}$'

if cname0 in ['nep']:
    cmap = 'PRGn'; vmin = -3; vmax = 3
elif cname0 in ['cWood', 'cLeaf', 'cVeg', 'cSoil']:
    cmap = 'YlGn'; vmin = None; vmax = None
elif cname0 in ['cLitter']:
    cmap = 'YlOrBr'; vmin = None; vmax = None
else:
    cmap = 'viridis'
    vmin = None; vmax = None

# ufluxv2 = ufluxv2_a[[f'{cname1}(u)']].drop_vars('spatial_ref')#.where(cond, np.nan)
ufluxv1e = ufluxv1e_dd[cname0].drop_vars('spatial_ref')#.where(cond, np.nan)
ufluxv2e = ufluxv2e_dd[cname0].drop_vars('spatial_ref')#.where(cond, np.nan)
trendy = trendy_dd[cname0]

if cname0 == 'npp':
    arrp = xr.merge([
        ufluxv1e_dd['npp'].drop_vars('spatial_ref')[f'{cname1}(e)'].rename(f'{cname1}(UFLUX)'),
        trendy[f'{cname1}(t)'].rename(f'{cname1}(Trendy)')
    ])
elif cname0 == 'gpp': # !!!!CHECK IF YOU NEED GPPe or GPPu, comment this block if the latter!!!!
    arrp = xr.merge([
        (ufluxv1e_dd['npp_unscaled'].drop_vars('spatial_ref')['NPP(e)'] + ufluxv1e_dd['ra'].drop_vars('spatial_ref')['Ra(e)']).rename(f'{cname1}(UFLUX)'),
        trendy[f'{cname1}(t)'].rename(f'{cname1}(Trendy)')
    ])
else:
    arrp = xr.merge([
        # ufluxv2[f'{cname1}(u)'].rename(f'{cname1}(UFLUX)'),
        ufluxv1e[f'{cname1}(e)'].rename(f'{cname1}(UFLUX)'),
        # ufluxv2e[f'{cname1}(e)'].rename(f'{cname1}(UFLUX)'),
        trendy[f'{cname1}(t)'].rename(f'{cname1}(Trendy)')
    ])
if 'spatial_ref' in arrp.data_vars: arrp = arrp.drop_vars('spatial_ref')
# arrp = arrp.where(arrp > 0, np.nan)
arrp = arrp.where(cond, np.nan)

# ==============================================================================

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
# cbar_kwargs = {'label': f'{cname1}', 'orientation':'horizontal', 'shrink':0.6, 'aspect':40, 'label':'Percent Deviation'}
cbar_kwargs = {'shrink':0.5, 'label': cname1 + f' ({unit})'}
im = arrp.mean(dim = 'time')[f'{cname1}(UFLUX)'].drop_vars('spatial_ref', errors = 'ignore').plot(
    ax = ax, cbar_kwargs = cbar_kwargs, cmap = cmap,
    vmin = vmin, vmax = vmax
) #, vmin = -10, vmax = 10, cmap = 'coolwarm')

ax.set_xlabel(''); ax.set_ylabel('')
world.dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 0.5)

x0 = ax.get_position().x0
x1 = ax.get_position().x1
y0 = ax.get_position().y0
y1 = ax.get_position().y1
ax1 = fig.add_axes([x0, y0 - 0.17, x1 - x0, 0.15])
ax1.set_ylabel(cname1)
ax1.set_xlabel('longitude')
ax1.yaxis.set_label_position("right")
ax1.yaxis.tick_right()

# cbar = fig.colorbar(im, cax=cbar_ax, orientation = 'horizontal')
# cbar.ax.locator_params(nbins = 5)
# cbar.set_label(unit_dict['GPP'], labelpad = 0, fontsize = 22)
ax1.plot(arrp.longitude, arrp.mean(dim = ['time', 'latitude'])[f'{cname1}(UFLUX)'], color = colors[0])
ax1.plot(arrp.longitude, arrp.mean(dim = ['time', 'latitude'])[f'{cname1}(Trendy)'], color = colors[1])
ax2 = fig.add_axes([x0 - 0.16, y0 + 0.05, 0.1, y1 - y0 - 0.1])
ax2.plot(arrp.mean(dim = ['time', 'longitude'])[f'{cname1}(UFLUX)'], arrp.latitude, color = colors[0], label = 'UFLUX')
ax2.plot(arrp.mean(dim = ['time', 'longitude'])[f'{cname1}(Trendy)'], arrp.latitude, color = colors[1], label = 'Trendy')
ax2.set_xlabel(cname1)
ax2.set_ylabel('latitude')

upper_legend(ax2, xloc = 4.4, yloc = 1.2)

ax.set_xlim(-180, 180)
ax1.set_xlim(-180, 180)

ax.set_ylim(-60, 80)
ax2.set_ylim(-60, 80)

if cname0 == 'nep':
    ax1.set_ylim(-2, 2)
    ax2.set_xlim(-2, 2)

# fig.suptitle(cname1, fontsize=14, x=0.1, y=0.73)
# add_text(0.1, 0.1, cname, fontsize = 14)
ax.text(0.05, 0.1, cname1, fontsize = 14, transform = ax.transAxes, horizontalalignment = 'left', verticalalignment = 'center')

# world[world['CONTINENT'] == 'Asia'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
# world[world['CONTINENT'] == 'Africa'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
# world[world['CONTINENT'] == 'Europe'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
# world[world['CONTINENT'] == 'North America'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
# world[world['CONTINENT'] == 'South America'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)

# google.download_file(fig, f'{cname1}_spatial.png')

# world = gpd.read_file(('https://raw.githubusercontent.com/soonyenju/geoAI/main/geoAI/data/geojson/WB_countries_Admin0_10m.json?raw=true'))
# UK = world[world['NAME_EN'] == 'United Kingdom']
# UK = UK[['geometry']].dissolve()

# trendy_gpp_UK = geoface.clip(trendy_dd['gpp']['GPP(t)'].mean(dim = 'time'), UK)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
trendy_gpp_UK.drop_vars('spatial_ref').plot(ax = ax, vmin = 0)
UK.plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)

print(trendy_dd.keys())

cname = 'cWood'

if cname in ['npp', 'reco', 'gpp', 'nep', 'nee']:
    clabel = cname.upper() + '(t)'
elif cname in ['cLeaf', 'cLitter', 'cSoil', 'cVeg', 'cWood']:
    clabel = cname + '(t)'
elif cname in ['ra', 'rh']:
    clabel = cname.capitalize() + '(t)'
else:
    pass
if cname in ['nep', 'npp']:
    vmin = None
    cmap = 'PRGn'
else:
    vmin = 0
    cmap = 'viridis'

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
trendy_dd[cname].mean(dim = 'time')[clabel].drop_vars('spatial_ref', errors = 'ignore').plot(ax =ax, vmin = vmin, cmap = cmap)
# ax.axis('off')
# ax.set_xticks([]); ax.set_yticks([])
ax.set_xticklabels([]); ax.set_yticklabels([])
ax.set_xlabel(''); ax.set_ylabel('')

google.download_file(fig, f'workflow-TRENDY-{cname}.png')

df_path_trendy_all = []
for p in list(root_proj_trendy.joinpath('1_data/2_trendy_c').glob('*.nc')) + list(root_proj_trendy.joinpath('1_data/v12').glob('*.nc')) + list(root_proj_trendy.joinpath('1_data/3_AGB_grazing').glob('*.nc')):
    fmi = p.stem.split('_')
    model_ = fmi[0]
    scenario_ = fmi[1]
    fluxpool_ = fmi[-1]
    df_path_trendy_all.append([model_, scenario_, fluxpool_, p])
df_path_trendy_all = pd.DataFrame(df_path_trendy_all, columns = ['MODEL', 'SCENARIO', 'FLUXPOOL', 'PATH']).sort_values(['MODEL', 'FLUXPOOL'])
df_path_trendy_all = df_path_trendy_all.query("SCENARIO == 'S3'").drop('SCENARIO', axis = 1).pivot(index = 'MODEL', columns = 'FLUXPOOL', values = 'PATH')

# print(df_path_trendy_all['FLUXPOOL'].unique())

df_path_trendy_all.index

"""# Continental Analysis"""

fig, ax = setup_canvas(1, 1)
world[world['CONTINENT'] == 'Asia'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
world[world['CONTINENT'] == 'Africa'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
world[world['CONTINENT'] == 'Europe'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
world[world['CONTINENT'] == 'North America'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
world[world['CONTINENT'] == 'South America'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)
world[world['CONTINENT'] == 'Oceania'].dissolve().plot(ax = ax, facecolor = 'none', edgecolor = 'k', linewidth = 1)

ufluxv1e_continent = []
for cname in ['cLeaf', 'cLitter', 'cSoil', 'cVeg', 'cWood', 'ra', 'rh', 'npp', 'reco', 'nep']:
    if cname in ['npp', 'reco', 'gpp', 'nep']:
        ufluxv1e_continent.append(ufluxv1e_dd[cname][f'{cname.upper()}(e)'])
    elif cname in ['cLeaf', 'cLitter', 'cSoil', 'cVeg', 'cWood']:
        ufluxv1e_continent.append(ufluxv1e_dd[cname][f'{cname}(e)'])
    elif cname in ['ra', 'rh']:
        ufluxv1e_continent.append(ufluxv1e_dd[cname][f'{cname.capitalize()}(e)'])
    else:
        pass
ufluxv1e_continent = xr.merge(ufluxv1e_continent)

ufluxv1e_continent['GPP(e)_adj'] = ufluxv1e_continent['NPP(e)'] + ufluxv1e_continent['Ra(e)']

ufluxv1_continent = ufluxv1_a_ensemble[['GPP(u1)', 'RECO(u1)', 'NEP(u1)_ind']]

nc_continent = xr.merge([ufluxv1e_continent, ufluxv1_continent])

nc_continent = (nc_continent.where(cond, np.nan) * coef_mat / coef_PgC_gC * 365).rio.write_crs("epsg:4326", inplace = False)
nc_continent[['cLeaf(e)', 'cLitter(e)', 'cSoil(e)', 'cVeg(e)', 'cWood(e)']] = nc_continent[['cLeaf(e)', 'cLitter(e)', 'cSoil(e)', 'cVeg(e)', 'cWood(e)']] / 365

print(nc_continent.sum(dim = ['latitude', 'longitude']).to_dataframe().columns)

nc_continent['cAGB'] = nc_continent['cLeaf(e)'] + nc_continent['cWood(e)']
nc_continent['cBGB'] = nc_continent['cVeg(e)'] - nc_continent['cAGB']
nc_continent = nc_continent[['cAGB', 'cBGB', 'cLitter(e)', 'cSoil(e)', 'NEP(e)', 'GPP(e)_adj','RECO(u1)']]
nc_continent = nc_continent.rename({
    'cLitter(e)': 'cLitter',
    'cSoil(e)': 'cSoil',
    'NEP(e)': 'NEP',
    'GPP(e)_adj': 'GPP',
    'RECO(u1)': 'RECO'
})

dfca, dfcm = [], []
for cname in ['cAGB', 'cBGB', 'cLitter', 'cSoil', 'NEP', 'GPP', 'RECO']:
    dfc = []; dfm = []
    for continent in world['CONTINENT'].drop_duplicates().sort_values():
        if continent == 'Seven seas (open ocean)': continue
        dft = nc_continent[cname].rio.clip(world[world['CONTINENT'] == continent].geometry.values, world.crs).sum(dim = ['latitude', 'longitude']).to_dataframe()
        dft = dft[cname]['2003'::]

        x = np.arange(len(dft))
        y = dft
        slope, intercept, rvalue, pvalue, stderr = stats.linregress(x, y)
        dfm.append([continent, slope, pvalue])

        # dfc.append(dft.rename(continent))
        dft = dft.rename('amount_carbon').to_frame()
        dft['continent'] = continent
        dfc.append(dft)

    dfm = pd.DataFrame(dfm, columns = ['continent', 'slope', 'pvalue']).set_index('continent')
    dfc = pd.concat(dfc, axis = 0)
    dfc['cname'] = cname
    dfm['cname'] = cname
    dfca.append(dfc)
    dfcm.append(dfm)
dfca = pd.concat(dfca, axis = 0)
dfcm = pd.concat(dfcm, axis = 0)

import matplotlib.patches as mpatches  # For custom legend handles

dfca_stock = dfca[dfca['cname'].isin(['cAGB', 'cBGB', 'cLitter', 'cSoil'])].copy()
dfca_stock['trendy'] = 'insignificance'
dfca_stock['trend'] = np.nan

colors_mapping = {
    "insignificance": "gray",
    "positive": "blue",
    "negative": "red"
}


for continent in dfcm.index.unique():
    for cname in dfca_stock['cname'].unique():
        dft = dfcm[dfcm['cname'] == cname].loc[continent, :]
        if dft['pvalue'] <= 0.05:
            if dft['slope'] > 0:
                dfca_stock.loc[(dfca_stock['continent'] == continent) & (dfca_stock['cname'] == cname), 'trendy'] = 'positive'
            else:
                dfca_stock.loc[(dfca_stock['continent'] == continent) & (dfca_stock['cname'] == cname), 'trendy'] = 'negative'
            dfca_stock.loc[(dfca_stock['continent'] == continent) & (dfca_stock['cname'] == cname), 'trend'] = dft['slope']

# Create the FacetGrid
g = sns.FacetGrid(dfca_stock, col="cname", sharey=False, aspect = 1)  # `sharey=False` allows different Y-axis ranges

# Map a boxplot to each facet
g.map_dataframe(sns.boxplot, x="continent", y="amount_carbon", hue = 'trendy', palette = colors_mapping)

# Rotate X-axis labels
for ax in g.axes.flat:  # Loop through each subplot
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right")  # Rotate labels
    ax.set_ylabel('$PgC$'); ax.set_xlabel('')
    ax.set_title(ax.get_title().split(' = ')[1], fontsize = 10)

# Adjust legend
# g.add_legend(title="Trend", label_order=["insignificance", "positive", "negative"])

# Create legend manually to ensure all categories appear
legend_handles = [
    mpatches.Patch(color=colors_mapping["insignificance"], label="Insignificance"),
    mpatches.Patch(color=colors_mapping["positive"], label="Positive"),
    mpatches.Patch(color=colors_mapping["negative"], label="Negative")  # Always included
]

# Add legend to the figure
g.fig.legend(handles=legend_handles, title="Trend", loc="upper right", bbox_to_anchor=(1.15, 0.7), frameon = False)

# Function to add trend values with relative positioning
def add_trend_labels(data, ax):
    for i, continent in enumerate(data["continent"].unique()):
        subset = data[(data["continent"] == continent) & (data["trendy"] != "insignificance")]
        if not subset.empty:
            trend_value = subset["trend"].unique()[0]  # Get trend value

            # Get axis limits to scale positioning dynamically
            x_min, x_max = ax.get_xlim()
            x_offset = (x_max - x_min) * 0.03  # Offset = 3% of the axis range
            y_min, y_max = ax.get_ylim()
            y_offset = (y_max - y_min) * 0.1  # Offset = 10% of the axis range

            # Get median y position of the box
            median_y = subset["amount_carbon"].median()

            # Annotate with dynamic positioning
            ax.text(i + x_offset, median_y + y_offset, f"{trend_value:.2f}", color="black", fontsize=9, ha="center")

# Apply function to each subplot
for ax, (cname, data) in zip(g.axes.flat, dfca_stock.groupby("cname", sort=False)):
    add_trend_labels(data, ax)

# google.download_file(g.fig, f'UFLUX_stock_continental.png')

import matplotlib.patches as mpatches  # For custom legend handles

dfca_flux = dfca[dfca['cname'].isin(['NEP', 'GPP', 'RECO'])].copy()
dfca_flux['trendy'] = 'insignificance'
dfca_flux['trend'] = np.nan

colors_mapping = {
    "insignificance": "gray",
    "positive": "blue",
    "negative": "red"
}


for continent in dfcm.index.unique():
    for cname in dfca_flux['cname'].unique():
        dft = dfcm[dfcm['cname'] == cname].loc[continent, :]
        if dft['pvalue'] <= 0.05:
            if dft['slope'] > 0:
                dfca_flux.loc[(dfca_flux['continent'] == continent) & (dfca_flux['cname'] == cname), 'trendy'] = 'positive'
            else:
                dfca_flux.loc[(dfca_flux['continent'] == continent) & (dfca_flux['cname'] == cname), 'trendy'] = 'negative'
            dfca_flux.loc[(dfca_flux['continent'] == continent) & (dfca_flux['cname'] == cname), 'trend'] = dft['slope']

# Create the FacetGrid
g = sns.FacetGrid(dfca_flux, col="cname", sharey=False, aspect = 1)  # `sharey=False` allows different Y-axis ranges

# Map a boxplot to each facet
g.map_dataframe(sns.boxplot, x="continent", y="amount_carbon", hue = 'trendy', palette = colors_mapping)

# Rotate X-axis labels
for ax in g.axes.flat:  # Loop through each subplot
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right")  # Rotate labels
    ax.set_ylabel('$PgC \ yr^{-1}$'); ax.set_xlabel('')
    ax.set_title(ax.get_title().split(' = ')[1], fontsize = 10)

# Adjust legend
# g.add_legend(title="Trend", label_order=["insignificance", "positive", "negative"])

# Create legend manually to ensure all categories appear
legend_handles = [
    mpatches.Patch(color=colors_mapping["insignificance"], label="Insignificance"),
    mpatches.Patch(color=colors_mapping["positive"], label="Positive"),
    mpatches.Patch(color=colors_mapping["negative"], label="Negative")  # Always included
]

# Add legend to the figure
g.fig.legend(handles=legend_handles, title="Trend", loc="upper right", bbox_to_anchor=(1.15, 0.7), frameon = False)

# Function to add trend values with relative positioning
def add_trend_labels(data, ax):
    for i, continent in enumerate(data["continent"].unique()):
        subset = data[(data["continent"] == continent) & (data["trendy"] != "insignificance")]
        if not subset.empty:
            trend_value = subset["trend"].unique()[0]  # Get trend value

            # Get axis limits to scale positioning dynamically
            x_min, x_max = ax.get_xlim()
            x_offset = (x_max - x_min) * 0.03  # Offset = 3% of the axis range
            y_min, y_max = ax.get_ylim()
            y_offset = (y_max - y_min) * 0.1  # Offset = 10% of the axis range

            # Get median y position of the box
            median_y = subset["amount_carbon"].median()

            # Annotate with dynamic positioning
            ax.text(i + x_offset, median_y + y_offset, f"{trend_value:.2f}", color="black", fontsize=9, ha="center")

# Apply function to each subplot
for ax, (cname, data) in zip(g.axes.flat, dfca_flux.groupby("cname", sort=False)):
    add_trend_labels(data, ax)

# google.download_file(g.fig, f'UFLUX_flux_continental.png')

"""# Baysian inference

## Functions
"""

def rank_corr_mat_fig2(NamesBN, RBN):
    """
    rank_corr_mat_fig plots the Rank Correlation matrix

    Parameters
    ----------
    NamesBN : list
        BN nodes names from model.
    RBN : numpy.ndarray
        Correlation matrix.

    Returns
    -------
    None.

    """
    nam = NamesBN

    # Replace 0 with NaN
    z = np.round(RBN, 4)
    z = z.astype('float')
    z[z == 0] = 'nan'  # or use np.nan

    # reduce the names of the variables if the number of variables is large
    # to plot the only the reduced names
    if len(nam) > 80:
        nv = int(round(len(nam) / 26, 0))
    elif len(nam) <= 80 and len(nam) > 40:
        nv = int(round(len(nam) / 13, 0))
    elif len(nam) <= 40 and len(nam) > 20:
        nv = int(round(len(nam) / 21, 0))
    elif len(nam) <= 20:
        nv = len(nam)

    px = list(range(len(nam)))  # position of the labels

    fig, ax = plt.subplots(figsize=(11, 8))
    im = plt.imshow(z, cmap='Blues', interpolation='nearest')
    cbar = plt.colorbar()  #color bar options
    for t in cbar.ax.get_yticklabels():
         t.set_fontsize(14)
         t.set_fontweight('semibold')

    if len(nam) <= 20:
        plt.xticks(px, nam, rotation=45)
        plt.yticks(px, nam)
    else:  # plot the only the reduces names so the plot doesnt look saturated
        plt.xticks(px[::nv], nam[::nv], rotation=45)
        plt.yticks(px[::nv], nam[::nv])

    font_weight = 550  #a numeric value in range 0-1000
    plt.setp(ax.get_xticklabels(), fontsize=16,fontweight=font_weight)
    plt.setp(ax.get_yticklabels(), fontsize=16,fontweight=font_weight)

    if len(nam) <= 10:
        zz = np.round(RBN, 4)
        zz = zz.astype(str)
        for i in range(len(nam)):
            for j in range(len(nam)):
                if zz[i, j] == '0.0':
                    zz[i, j] = ''
                if zz[i,j] == '1.0':
                    text = ax.text(j, i, zz[i, j],
                               ha="center", va="center",
                               color="white", fontsize=18, fontweight=font_weight)
                else:
                    text = ax.text(j, i, zz[i, j],
                               ha="center", va="center",
                               color="#1E1E1E", fontsize=18, fontweight=font_weight)
    plt.savefig('rank_corr_matrix.png')
    plt.show()

    return None

# -*- coding: utf-8 -*-
"""
Py_BANSHEE
Authors: Paul Koot, Miguel Angel Mendoza-Lugo, Dominik Paprotny,
         Elisa Ragno, Oswaldo Morales-Npoles, Danil Worm

E-mail:  m.a.mendozalugo@tudelft.nl, paulkoot6@gmail.com & O.MoralesNapoles@tudelft.nl
"""

import networkx as nx
import graphviz as gv
import seaborn as sns
import pandas as pd
from matplotlib import pyplot as plt
from IPython.display import Image
import os


def bn_visualize2(parent_cell, R, names, data=None, fig_name=''):
    """ Visualize the structure of a defined Bayesian Network

     bn_visualize creates and saves a directed digraph presenting the
     structure of nodes and arcs of the Bayesian Network (BN), defined by
     parent_cell. The function also displays the conditional rank
     correlations at each arc defined by R.


    Parameters
    ----------
    parent_cell : list
        A list containing the structure of the BN,
        the same as required in the bn_rankcorr function
    R : numpy.ndarray
        Rank Correlation Matrix
    names : list
        a list containing names of the nodes for the plot. Should
        be in the same order as they appear in matrix R and parent_cell
    data : pandas.core.frame.DataFrame
        the same data that can be used as input in bn_rankcorr. When this
        argument is given as input, the nodes in the visualization contain
        the marginal distribution of the data within each node.
    fig_name : string
        Name extension of the .png file with the Bayesian Network that
        is created: BN_visualize_'fig_name'.png.
        The file is saved in the working directory.

    Returns
    -------
    None.
    """

    G = nx.DiGraph()
    if isinstance(data, pd.DataFrame):
        for node in data:
            plt.figure()
            h = sns.histplot(data[node], kde=True)
            h.set_xlabel('x')
            h.set_title('{}'.format(node), fontsize=25)
            plt.xticks(rotation=45)
            plt.ylabel("Count", fontsize=18)
            plt.xlabel("x", fontsize=18)
            plt.xticks(fontsize=14)
            plt.yticks(fontsize=14)
            plt.tight_layout()
            plt.savefig('histogram_{}.png'.format(node))
            G.add_node(node, image='histogram_{}.png'.format(node),
                       fontsize=0)
            plt.show()
    else:
        G.add_nodes_from(names, style='filled', fillcolor='red')
        plt.show()

    #edges and labels
    edgs=[]
    edglbs=[]
    for i in range(len(names)):
        parents = parent_cell[i]
        for j in parents:
            edgs.append((names[j], names[i]))
            edglbs.append(f'{round(R[j, i],2)}')
    #add edges and labels
    for k in range(len(edgs)):
        G.add_edges_from([edgs[k]], label = edglbs[k])
    #dot file
    nx.drawing.nx_pydot.write_dot(G, f'BN_visualize_{fig_name}')
    # Convert dot file to png file
    gv.render('dot', 'png', f'BN_visualize_{fig_name}')
    deleteFile(f'BN_visualize_{fig_name}')

    return f'BN plot saved in : {os.getcwd()}\BN_visualize_{fig_name}.png'


def deleteFile(filename):
    if os.path.exists(filename) and not os.path.isdir(filename) and not os.path.islink(filename):
        os.remove(filename)

"""## Run"""

!pip install py_banshee --quiet

savefile = root_proj_trendy.joinpath('4_analysis/Baysian_inference/ufluxv1_mon-temp.nc')

if savefile.exists():
    ufluxv1_mon = xr.open_dataset(savefile, engine = 'scipy')
    ufluxv1_mon = ufluxv1_mon.rename(
        {k: k.replace('mML', '') + '(u)' for k in ufluxv1_mon.data_vars if k != 'NIRv'}
    )
else:
    ufluxv1_mon = []
    for dt in tqdm(df_path_ufluxv1.index):
        p = df_path_ufluxv1.loc[dt, 'path']
        nct = xr.open_dataset(p, engine = 'netcdf4')
        # nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
        # nct = nct.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).resample(time = '1YS').mean()
        ufluxv1_mon.append(nct)
    ufluxv1_mon = xr.merge(ufluxv1_mon)
    ufluxv1_mon = ufluxv1_mon.where(~ufluxv1_mon['GPPmML'].isnull(), np.nan)

    ufluxv1_mon.to_netcdf(savefile, engine = 'scipy')

# savefile = root_proj_trendy.joinpath('4_analysis/Baysian_inference/ERA5_annual_2000-2022.nc')
# if savefile.exists():
#     era5 = xr.open_dataset(savefile, engine = 'scipy').interp(latitude = nc_continent.latitude, longitude = nc_continent.longitude)
# else:
#     df_path_era5 = []
#     for p in root_proj_trendy.joinpath('2_preproc/ERA5').glob(f'ERA5*.nc'):
#         df_path_era5.append([pd.to_datetime(p.stem.split('_')[-1], format = '%Y'), p])
#     df_path_era5 = pd.DataFrame(df_path_era5, columns = ['time', 'path']).set_index('time')
#     df_path_era5 = df_path_era5.loc['2003'::, :]

#     era5 = []
#     for p_era5 in tqdm(df_path_era5['path']):
#         era5t = load_era5(p_era5, engine = 'netcdf4', drop_variables = drop_variables).resample(time = '1YS').mean()
#         era5.append(era5t)
#         del(era5t)
#     era5 = xr.merge(era5)
#     era5.to_netcdf(savefile, engine = 'scipy')
# del(savefile)

df_path_era5 = []
for p in root_proj_trendy.joinpath('2_preproc/ERA5').glob(f'ERA5*.nc'):
    df_path_era5.append([pd.to_datetime(p.stem.split('_')[-1], format = '%Y'), p])
df_path_era5 = pd.DataFrame(df_path_era5, columns = ['time', 'path']).set_index('time')
df_path_era5 = df_path_era5.loc['2003'::, :]

era5_dd = {
    'temperature_2m': [],
    'soil_temperature_level_1': [],
    'surface_solar_radiation_downwards_sum': [],
    'total_precipitation_sum': [],
    'VPD': [],
    'surface_net_radiation_sum': []
}
for p_era5 in tqdm(df_path_era5['path']):
    era5t = load_era5(p_era5, engine = 'netcdf4', drop_variables = drop_variables)
    for v in era5_dd.keys():
        era5_dd[v].append(era5t[v])
    del(era5t)

for v in tqdm(era5_dd.keys()):
    era5_dd[v] = xr.merge(era5_dd[v])

savefile_co2 = root_proj_trendy.joinpath('2_preproc/co2_info.nc')
if savefile_co2.exists():
    co2r = xr.open_dataset(savefile_co2,  engine="netcdf4")
co2 = co2r.interp(latitude = nc_continent.latitude, longitude = nc_continent.longitude)

continent = 'Oceania'
if continent:
    print(continent)
    df_uflux1 = ufluxv1_mon.rio.write_crs("epsg:4326", inplace = False).rio.clip(world[world['CONTINENT'] == continent].geometry.values, world.crs).mean(dim = ['latitude', 'longitude']).to_dataframe()
    df_era5 = []
    for k, era5 in era5_dd.items():
        df_era5.append(era5.rio.write_crs("epsg:4326", inplace = False).rio.clip(world[world['CONTINENT'] == continent].geometry.values, world.crs).mean(dim = ['latitude', 'longitude']).to_dataframe().drop('spatial_ref', axis = 1))
    df_era5 = pd.concat(df_era5, axis = 1)
    df_co2 = co2.rio.write_crs("epsg:4326", inplace = False).rio.clip(world[world['CONTINENT'] == continent].geometry.values, world.crs).mean(dim = ['latitude', 'longitude']).to_dataframe()
else:
    print('The whole world...')
    df_uflux1 = ufluxv1_mon.rio.write_crs("epsg:4326", inplace = False).mean(dim = ['latitude', 'longitude']).to_dataframe()
    df_era5 = []
    for k, era5 in era5_dd.items():
        df_era5.append(era5.rio.write_crs("epsg:4326", inplace = False).mean(dim = ['latitude', 'longitude']).to_dataframe().drop('spatial_ref', axis = 1))
    df_era5 = pd.concat(df_era5, axis = 1)
    df_co2 = co2.rio.write_crs("epsg:4326", inplace = False).mean(dim = ['latitude', 'longitude']).to_dataframe()
df_bn = pd.concat([df_era5, df_co2, df_uflux1], axis = 1)
df_bn['total_precipitation_sum'] = df_bn['total_precipitation_sum'].shift(1)
df_bn = df_bn.dropna()
if 'spatial_ref' in df_bn.columns: df_bn = df_bn.drop('spatial_ref', axis = 1)

df_bn = df_bn.rename(columns = {
    'temperature_2m': 'TA',
    'soil_temperature_level_1': 'TS',
    'surface_solar_radiation_downwards_sum': 'SWIN',
    'total_precipitation_sum': 'P',
    'co2': 'CO2',
    'GPP(u)': 'GPP',
    'RECO(u)': 'RECO',
    'NEE(u)': 'NEE',
})
df_bn['NEP'] = df_bn['NEE'] * -1
df_bn = df_bn[['SWIN', 'TA', 'TS', 'VPD', 'P', 'CO2', 'GPP', 'RECO', 'NEP']]

print(df_bn.columns)

world['CONTINENT'].sort_values().unique()

cname = 'NEP'
dfi = df_bn[[
    cname,
    'SWIN',
    'TA',
    'TS',
    'CO2',
    'VPD',
    'P'
]].dropna()

data = dfi
if continent:
    fig_name = cname + '_' + continent
else:
    fig_name = cname + '_' + 'World'

# Extract the variable names
names = list(data.columns)
# Extract number of nodes from data
N = data.shape[1]
# Defining the structure of the BN
parent_cell = [None] * N
parent_cell[0] = [1, 2, 3, 4, 5]
parent_cell[1] = []
parent_cell[2] = [1]
parent_cell[3] = [1, 2]
parent_cell[4] = []
parent_cell[5] = [1, 2, 6]
parent_cell[6] = []

from py_banshee.rankcorr import bn_rankcorr
from py_banshee.bn_plot import bn_visualize
from py_banshee.copula_test import cvm_statistic
from py_banshee.d_cal import gaussian_distance
from py_banshee.prediction import inference


correlation_matrix = data.corr(method="pearson")

# # Optional: Visualize with a heatmap
# plt.figure(figsize=(8, 6))
# sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
# plt.title("Pearson Correlation Coefficients")
# plt.show()

#-----------------------------------------------------------------------------
# Function 1 - bn_rankcorr - Conditional rank correlation matrix
#-----------------------------------------------------------------------------

R = bn_rankcorr(parent_cell,        # structure of the BN
                data,               # matrix of data
                var_names = names,  # names of variables
                is_data = True,        # matrix data contains actual data
                plot = False)           # create a plot (False = don't create plot)

# rank_corr_mat_fig2(names, R)

# Optional: Visualize with a heatmap
fig, ax = setup_canvas(1, 1, figsize=(8, 6), labelsize=10, fontsize=10)
dfR = pd.DataFrame(R, columns = correlation_matrix.columns, index = correlation_matrix.index)
dfd = np.abs(dfR - correlation_matrix)
dfd = dfd[(dfR != 0) & (dfd != 0)]
sns.heatmap(dfd, annot=True, cmap="rocket_r", fmt=".2f")
ax.set_title("model difference")
fig.savefig(fig_name + '_model_difference.png')

#-----------------------------------------------------------------------------
# Function 2 - bn_visualize - Plot of the Bayesian Network
#-----------------------------------------------------------------------------

bn_visualize2(parent_cell,           # structure of the BN
             R,                     # the rank correlation matrix (function 1)
             data.columns,          # names of variables
             fig_name = fig_name)   # figure name
# The plot presents the BN with 5 nodes and 7 arcs, with the (conditional)
# rank correlations indicated on the arcs.

# download_temp_storage()
# clear_temp_storage()

"""Further exploration"""

# bn_visualize(parent_cell,                       # structure of the BN
#              R,                                 # the rank correlation matrix
#              data.columns,                      # names of variables
#              data = data,                       # DataFrame with data
#              fig_name = fig_name + '_margins')  # figure name
# # The plot presents the BN with 5 nodes and 7 arcs, with the (conditional)
# # rank correlations indicated on the arcs. In this plot also the marginal
# # distributions for each variable are incorporated.

# #-----------------------------------------------------------------------------
# # Function 3 - cvm_statistics - test goodness-of-fit of the Gaussian copula
# #-----------------------------------------------------------------------------

# M = cvm_statistic(data,                   # DataFrame with data
#                   names = data.columns,   # names of variables
#                   plot = True,               # create a plot (0=don't create plot)
#                   fig_name = fig_name)    # figure name
# # The results of the goodness-of-fit test in terms of Cramer-von Mises
# # statistic highlight that the Gaussian copula is in majority of cases the
# # most suitable for representing the dependency between variables,
# # especially for the variable of interest (safety). This is important as
# # the method utilizes the Gaussian copula for dependence modelling.

# #-----------------------------------------------------------------------------
# # Function 4 - gaussian_distance - measuring d-calibration score
# #-----------------------------------------------------------------------------

# D_ERC,B_ERC,D_BNRC,B_BNRC = gaussian_distance(
#                             R,        # the rank correlation matrix
#                             data,     # DataFrame with data
#                             4000,     # number of samples drawn d-Cal(ERC,NRC)
#                             400,      # number of samples drawn d-Cal(NRC,BNRC)
#                             1000,     # number of iterations to compute CI
#                             Plot=True,          # create a plot (0=don't create plot)
#                             Type='H',           # take Hellinger distance (default)
#                             fig_name=fig_name)  # figure name
# # draw 4000 samples of the normal distribution and perform 1000 iterations to
# # obtain the distribution of the d-cal score (ERC,NRC)
# # draw 400 samples of the normal distribution and perform 1000 iterations to
# # obtain the distribution of the d-cal score (NRC,BNRC)


# # The d-calibration score of the empirical rank correlation matrix is
# # inside the 90# confidence interval of the determinant of the empirical
# # The d-calibration score of the BN's rank correlation matrix is well within
# # the 90# confidence interval of the determinant of the random normal distribution
# # sampled for the same correlation matrix. This supports the assumptions of
# # a joint normal copula used in the BN model. It should be noted that the
# # test is sensitive to the number of samples drawn as well as the number of
# # iterations and is rather severe for large datasets.

#-----------------------------------------------------------------------------
# Function 5 - inference - making inference with the BN model
#-----------------------------------------------------------------------------

condition=np.arange(1, len(names)) #conditionalized variables, all except for safety (predict)
values = data.iloc[:,condition].to_numpy() # data for conditionalization

F = inference(condition,        # nodes that will be conditionalized
              values,           # information used to conditionalize the
                                # nodes of the NPBN
              R,                # the rank correlation matrix
              data,             # DataFrame with data
              Output='mean')    # type of output data


# Evaluation
y_pred = F.squeeze()

# Fill out the column(s) which are predicted
y_original=data.iloc[:,0].to_numpy()

# Calculate coefficient of determination
A = (y_pred - y_pred.mean(axis=0))/y_pred.std(axis=0)
B = (y_original - y_original.mean(axis=0))/y_original.std(axis=0)
correlation = (np.dot(B, A.T)/B.shape[0])
R2=correlation**2

# Calculate mean absolute error
MAE = np.mean(np.abs(y_pred-y_original))

# Calculate mean bias error
MBE = np.mean(y_pred-y_original)

# Calculate root mean square error
RMSE = (np.mean((y_pred-y_original)**2))**(1/2)

# The coefficient of determination between modelled and observed safety in
# US cities is rather low (0.21), but the average error equals only around
# a quarter of the average value of safety, and there is almost no bias.
# However, for proper validation, if sufficient data is available, a split-
# -sample validation or a k-fold cross-validation should be performed.

R2, MAE, MBE, RMSE

"""# Appendix ERA-5 uncertainty"""

meta = pd.read_csv((r'https://github.com/soonyenju/scitbx/blob/master/scitbx/data/fluxnet_meta_212.csv?raw=true'),index_col = 0)

df_path_fluxnet = []
for p in root.joinpath('fmt_fluxdata/fluxnet_DD').glob('*.csv'):
    site = p.stem.split('_')[0]
    df_path_fluxnet.append([site, p])

df_path_fluxnet = pd.DataFrame(df_path_fluxnet, columns = ['ID', 'PATH']).set_index('ID')

df_ec = []
for site in tqdm(df_path_fluxnet.index):
    p = df_path_fluxnet.loc[site, 'PATH']
    dft = pd.read_csv(p, index_col = 0, usecols = [
        'TIMESTAMP', 'TA_F', 'SW_IN_F', 'LW_IN_F', 'VPD_F', 'P_F', 'TA_ERA', 'SW_IN_ERA', 'LW_IN_ERA', 'VPD_ERA', 'P_ERA'
    ]).replace(-9999., np.nan)
    dft.index = pd.to_datetime(dft.index, format = '%Y%m%d')
    dft['ID'] = site
    # dft['IGBP'] = meta.loc[site, 'IGBP']
    dft = dft.reset_index().rename(columns = {'TIMESTAMP': 'time'}).set_index(['ID', 'time'])
    df_ec.append(dft)
df_ec = pd.concat(df_ec)

# p = root_proj_platform.joinpath('0tower_level/ERA5-Land-daily-var19.nc')
# nc_era5 = xr.open_dataset(p)

df_era5 = nc_era5.to_dataframe()
df_era5['dewpoint_temperature_2m'] -= 273.15
df_era5['temperature_2m'] -= 273.15
df_era5['soil_temperature_level_1'] -= 273.15

df_era5['surface_latent_heat_flux_sum'] /= 86400
df_era5['surface_sensible_heat_flux_sum'] /= 86400
df_era5['surface_solar_radiation_downwards_sum'] /= 86400
df_era5['surface_thermal_radiation_downwards_sum'] /= 86400
df_era5['surface_net_solar_radiation_sum'] /= 86400

df_era5['surface_pressure'] /= 100

df_era5['temperature_2m_min'] -= 273.15
df_era5['temperature_2m_max'] -= 273.15

df_era5['total_precipitation_sum'] *= 1000

df_era5['VPD'] = saturation_vapor_pressure(df_era5['temperature_2m']) - saturation_vapor_pressure(df_era5['dewpoint_temperature_2m'])

dfm_r2 = []; dfm_nrmse = []

for site in tqdm(df_ec.index.get_level_values('ID').drop_duplicates()):
    df_ec_t = df_ec.loc[site, :, :]

    df_era5_t = df_era5.loc[:, site, :]
    df_era5_t = df_era5_t[['temperature_2m', 'surface_solar_radiation_downwards_sum', 'surface_thermal_radiation_downwards_sum', 'VPD', 'total_precipitation_sum']].rename(columns = {
        'temperature_2m': 'TA_ERA5_land',
        'surface_solar_radiation_downwards_sum': 'SW_IN_ERA5_land',
        'surface_thermal_radiation_downwards_sum': 'LW_IN_ERA5_land',
        'VPD': 'VPD_ERA5_land',
        'total_precipitation_sum': 'P_ERA5_land'
    })

    dft = pd.concat([df_ec_t, df_era5_t], axis = 1).dropna()

    TA_r2_0 = get_r2(dft['TA_F'], dft['TA_ERA'])
    TA_r2_1 = get_r2(dft['TA_F'], dft['TA_ERA5_land'])

    SW_IN_r2_0 = get_r2(dft['SW_IN_F'], dft['SW_IN_ERA'])
    SW_IN_r2_1 = get_r2(dft['SW_IN_F'], dft['SW_IN_ERA5_land'])

    LW_IN_r2_0 = get_r2(dft['LW_IN_F'], dft['LW_IN_ERA'])
    LW_IN_r2_1 = get_r2(dft['LW_IN_F'], dft['LW_IN_ERA5_land'])

    VPD_r2_0 = get_r2(dft['VPD_F'], dft['VPD_ERA'])
    VPD_r2_1 = get_r2(dft['VPD_F'], dft['VPD_ERA5_land'])

    P_r2_0 = get_r2(dft['P_F'], dft['P_ERA'])
    P_r2_1 = get_r2(dft['P_F'], dft['P_ERA5_land'])

    TA_nrmse_0 = get_rmse(dft['TA_F'], dft['TA_ERA']) / np.abs(dft['TA_F'].mean())
    TA_nrmse_1 = get_rmse(dft['TA_F'], dft['TA_ERA5_land']) / np.abs(dft['TA_F'].mean())

    SW_IN_nrmse_0 = get_rmse(dft['SW_IN_F'], dft['SW_IN_ERA']) / np.abs(dft['SW_IN_F'].mean())
    SW_IN_nrmse_1 = get_rmse(dft['SW_IN_F'], dft['SW_IN_ERA5_land']) / np.abs(dft['SW_IN_F'].mean())

    LW_IN_nrmse_0 = get_rmse(dft['LW_IN_F'], dft['LW_IN_ERA']) / np.abs(dft['LW_IN_F'].mean())
    LW_IN_nrmse_1 = get_rmse(dft['LW_IN_F'], dft['LW_IN_ERA5_land']) / np.abs(dft['LW_IN_F'].mean())

    VPD_nrmse_0 = get_rmse(dft['VPD_F'], dft['VPD_ERA']) / np.abs(dft['VPD_F'].mean())
    VPD_nrmse_1 = get_rmse(dft['VPD_F'], dft['VPD_ERA5_land']) / np.abs(dft['VPD_F'].mean())

    P_nrmse_0 = get_rmse(dft['P_F'], dft['P_ERA']) / np.abs(dft['P_F'].mean())
    P_nrmse_1 = get_rmse(dft['P_F'], dft['P_ERA5_land']) / np.abs(dft['P_F'].mean())


    IGBP_ = meta.loc[site, 'IGBP']
    dfm_r2.append([TA_r2_0, SW_IN_r2_0, LW_IN_r2_0, VPD_r2_0, P_r2_0, IGBP_]) # ERA-Interim
    dfm_nrmse.append([TA_nrmse_0, SW_IN_nrmse_0, LW_IN_nrmse_0, VPD_nrmse_0, P_nrmse_0, IGBP_]) # ERA-Interim

    # dfm_r2.append([TA_r2_1, SW_IN_r2_1, LW_IN_r2_1, VPD_r2_1, P_r2_1, IGBP_]) # ERA5
    # dfm_nrmse.append([TA_nrmse_1, SW_IN_nrmse_1, LW_IN_nrmse_1, VPD_nrmse_1, P_nrmse_1, IGBP_]) # ERA5

dfm_r2 = pd.DataFrame(dfm_r2, columns = ['TA', 'SW_IN', 'LW_IN', 'VPD', 'P', 'IGBP'])
dfm_nrmse = pd.DataFrame(dfm_nrmse, columns = ['TA', 'SW_IN', 'LW_IN', 'VPD', 'P', 'IGBP'])

dfm_r2.loc[:, dfm_r2.select_dtypes(include='number').columns] = \
    dfm_r2.select_dtypes(include='number').applymap(lambda x: np.nan if x < 0 else x)

dfm_nrmse.loc[:, dfm_nrmse.select_dtypes(include='number').columns] = \
    dfm_nrmse.select_dtypes(include='number').applymap(lambda x: np.nan if x > 1 else x)

dfm_r2 = dfm_r2.sort_values(by = 'IGBP')
dfm_nrmse = dfm_nrmse.sort_values(by = 'IGBP')

# Melt both for plotting
r2_melted = dfm_r2.melt(id_vars='IGBP', var_name='Variable', value_name='value')
nrmse_melted = dfm_nrmse.melt(id_vars='IGBP', var_name='Variable', value_name='value')

# Merge to bring R2 into nrmse dataframe
# df_merged = r2_melted; title_name = '$r^2$'
df_merged = nrmse_melted; title_name = 'nRMSE'

# Create a color palette with as many colors as needed for IGBP groups
palette = sns.color_palette("Set3", n_colors=df_merged['IGBP'].nunique())

# Plot with seaborn but loop over Variable to set individual box colors
g = sns.catplot(
    data=df_merged,
    x='IGBP', y='value', col='Variable',
    kind='box', col_wrap=5,
    height=3, aspect=0.8, sharey = False,
    # palette=palette
    hue = 'IGBP'
)

# # Set individual box colors using ax.patches
# for ax, color in zip(g.axes.flatten(), palette):
#     for patch in ax.patches:
#         patch.set_facecolor(color)
#         patch.set_edgecolor("black")
#         patch.set_alpha(0.9)

g.axes[0].set_ylabel('')

g.fig.suptitle(title_name, y=0.9, fontsize=14)
# Adjust font size of axis labels
# g.set_axis_labels('IGBP', None, fontsize=12)

for ax in g.axes.flatten():
    ax.set_xlabel('')

g.set_xticklabels(rotation=60)

plt.tight_layout()

# google.download_file(g.fig, f'{title_name}-EC_vs_ERA5.png')

dfm_r2.IGBP.unique()

"""# Obsolete

## Reco with Trendy
"""

def get_Trendy_reco():
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_ra_05deg_monthly_2000-2022.nc')
    raT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    # raT = raT.resample(time = '1YS').mean()
    raT = raT.where(raT['ra'] > 1e-9)
    p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_rh_05deg_monthly_2000-2022.nc')
    rhT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
    # rhT = rhT.resample(time = '1YS').mean()
    rhT = rhT.where(rhT['rh'] > 1e-9)

    recoT = (raT['ra'] + rhT['rh']).rename('Trendy_Reco').resample(time = '1YS').mean()
    recoT_25th = (raT['ra_25th'] + rhT['rh_25th']).rename('Trendy_Reco_25th').resample(time = '1YS').mean()
    recoT_75th = (raT['ra_75th'] + rhT['rh_75th']).rename('Trendy_Reco_75th').resample(time = '1YS').mean()

    return xr.merge([recoT, recoT_25th, recoT_75th])

trendy_reco = get_Trendy_reco()

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
dfp = pd.concat([
    ufluxv2e['reco'].drop_vars('spatial_ref').rename({'reco': 'UFLUXv2e_Reco', 'reco_25th': 'UFLUXv2e_Reco_25th', 'reco_75th': 'UFLUXv2e_Reco_75th'}).mean(dim = ['longitude', 'latitude']).to_dataframe(),
    trendy_reco.mean(dim = ['longitude', 'latitude']).to_dataframe(),
    ufluxv2[['RECOmML']].drop_vars('spatial_ref').rename({'RECOmML': 'UFLUXv2_Reco'}).mean(dim = ['longitude', 'latitude']).to_dataframe(),
], axis = 1)

# dfp = pd.concat([
#     (ufluxv2e['reco'].where((~trendy_reco['Trendy_Reco'].isnull()) & (~ufluxv2['RECOmML'].isnull()), np.nan) * coef_mat).drop_vars('spatial_ref').rename({'reco': 'UFLUXv2e_Reco', 'reco_25th': 'UFLUXv2e_Reco_25th', 'reco_75th': 'UFLUXv2e_Reco_75th'}).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
#     (trendy_reco.where((~trendy_reco['Trendy_Reco'].isnull()) & (~ufluxv2['RECOmML'].isnull()), np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
#     (ufluxv2.where((~trendy_reco['Trendy_Reco'].isnull()) & (~ufluxv2['RECOmML'].isnull()), np.nan)[['RECOmML']] * coef_mat).drop_vars('spatial_ref').rename({'RECOmML': 'UFLUXv2_Reco'}).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
# ], axis = 1)

# # colorful:
# ax.plot(dfp.index, dfp['UFLUXv2_Reco'], c = colors[0], label = 'UFLUXv2_Reco')
# ax.plot(dfp.index, dfp['UFLUXv2e_Reco'], c = colors[1], label = 'UFLUXv2e_Reco')
# ax.fill_between(dfp.index, dfp['UFLUXv2e_Reco_25th'], dfp['UFLUXv2e_Reco_75th'], color = colors[1], alpha = 0.2)
# ax.plot(dfp.index, dfp['Trendy_Reco'], c = colors[2], label = 'Trendy_Reco')
# ax.fill_between(dfp.index, dfp['Trendy_Reco_25th'], dfp['Trendy_Reco_75th'], color = colors[2], alpha = 0.2)
# grey:
ax.plot(dfp.index, dfp['UFLUXv2_Reco'], 'k-', label = 'UFLUXv2_Reco')
ax.plot(dfp.index, dfp['UFLUXv2e_Reco'], 'k--', label = 'UFLUXv2e_Reco')
ax.plot(dfp.index, dfp['Trendy_Reco'], 'k:', label = 'Trendy_Reco')
# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('Reco (gC m-2d-1)')

cond = (~(ufluxv2e['reco']['reco'].isnull())) & (~(trendy_reco['Trendy_Reco'].isnull())) & (~(ufluxv2['RECOmML'].isnull()))

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
dfp = pd.concat([
    ufluxv2e['reco'].where(cond, np.nan).drop_vars('spatial_ref').rename({'reco': 'UFLUXv2e_Reco', 'reco_25th': 'UFLUXv2e_Reco_25th', 'reco_75th': 'UFLUXv2e_Reco_75th'}).median(dim = ['longitude', 'latitude']).to_dataframe(),
    trendy_reco.where(cond, np.nan).median(dim = ['longitude', 'latitude']).to_dataframe(),
    ufluxv2[['RECOmML']].where(cond, np.nan).drop_vars('spatial_ref').rename({'RECOmML': 'UFLUXv2_Reco'}).median(dim = ['longitude', 'latitude']).to_dataframe(),
], axis = 1)

# # colorful:
# ax.plot(dfp.index, dfp['UFLUXv2_Reco'], c = colors[0], label = 'UFLUXv2_Reco')
# ax.plot(dfp.index, dfp['UFLUXv2e_Reco'], c = colors[1], label = 'UFLUXv2e_Reco')
# ax.fill_between(dfp.index, dfp['UFLUXv2e_Reco_25th'], dfp['UFLUXv2e_Reco_75th'], color = colors[1], alpha = 0.2)
# ax.plot(dfp.index, dfp['Trendy_Reco'], c = colors[2], label = 'Trendy_Reco')
# ax.fill_between(dfp.index, dfp['Trendy_Reco_25th'], dfp['Trendy_Reco_75th'], color = colors[2], alpha = 0.2)
# grey:
ax.plot(dfp.index, dfp['UFLUXv2_Reco'], 'k-', label = 'UFLUXv2_Reco')
ax.plot(dfp.index, dfp['UFLUXv2e_Reco'], 'k--', label = 'UFLUXv2e_Reco')
ax.plot(dfp.index, dfp['Trendy_Reco'], 'k:', label = 'Trendy_Reco')
# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('Reco (gC m-2d-1)')

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
dfp = pd.concat([
    (ufluxv2e['reco'].where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').rename(
        {'reco': 'UFLUXv2e_Reco', 'reco_25th': 'UFLUXv2e_Reco_25th', 'reco_75th': 'UFLUXv2e_Reco_75th'}
    ).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (trendy_reco.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (ufluxv2[['RECOmML']].where(cond, np.nan) * coef_mat).drop_vars('spatial_ref').rename(
        {'RECOmML': 'UFLUXv2_Reco'}
    ).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
], axis = 1)

# # colorful:
# ax.plot(dfp.index, dfp['UFLUXv2_Reco'], c = colors[0], label = 'UFLUXv2_Reco')
# ax.plot(dfp.index, dfp['UFLUXv2e_Reco'], c = colors[1], label = 'UFLUXv2e_Reco')
# ax.fill_between(dfp.index, dfp['UFLUXv2e_Reco_25th'], dfp['UFLUXv2e_Reco_75th'], color = colors[1], alpha = 0.2)
# ax.plot(dfp.index, dfp['Trendy_Reco'], c = colors[2], label = 'Trendy_Reco')
# ax.fill_between(dfp.index, dfp['Trendy_Reco_25th'], dfp['Trendy_Reco_75th'], color = colors[2], alpha = 0.2)
# grey:
ax.plot(dfp.index, dfp['UFLUXv2_Reco'], 'k-', label = 'UFLUXv2_Reco')
ax.plot(dfp.index, dfp['UFLUXv2e_Reco'], 'k--', label = 'UFLUXv2e_Reco')
ax.plot(dfp.index, dfp['Trendy_Reco'], 'k:', label = 'Trendy_Reco')
# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('Reco (gC m-2d-1)')

"""## Maps"""

ufluxv2e = {}
for p in root_proj_trendy.joinpath(f'4_analysis/annual_dynamics').glob('*.nc'):
    name = p.stem
    if name == 'ufluxv2':
        ufluxv2 = xr.open_dataset(p, engine = 'netcdf4')
        ufluxv2 = ufluxv2
        # ufluxv2 = ufluxv2[['GPPmML', 'RECOmML', 'LEmML', 'NEEmML', 'NIRv']]
    else:
        ufluxv2e[name] = xr.open_dataset(p, engine = 'netcdf4')
ufluxv2e['reco'] = xr.merge([
    (ufluxv2e['ra']['ra'] + ufluxv2e['rh']['rh']).rename('reco'),
    (ufluxv2e['ra']['ra_25th'] + ufluxv2e['rh']['rh_25th']).rename('reco_25th'),
    (ufluxv2e['ra']['ra_75th'] + ufluxv2e['rh']['rh_75th']).rename('reco_75th')
])
print(list(ufluxv2e.keys()))

"""### GPP IAV"""

p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_gpp_05deg_monthly_2000-2022.nc')
gppT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
gppT = gppT.where(gppT['gpp'] != 0, np.nan)
gppT = gppT.resample(time = '1YS').mean()

cond = (~(ufluxv2['GPPmML'].isnull())) & (~(gppT['gpp'].isnull()))

dfp = pd.concat([
    ufluxv2.where(cond, np.nan)['GPPmML'].median(dim = ['longitude', 'latitude']).rename('UFLUXv2_GPP').drop_vars('spatial_ref').to_dataframe(),
    gppT.where(cond, np.nan).median(dim = ['longitude', 'latitude']).to_dataframe().rename(columns = {'gpp': 'Trendy_GPP', 'gpp_25th': 'Trendy_GPP_25th', 'gpp_75th': 'Trendy_GPP_75th'}),
], axis = 1)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
ax.plot(dfp.index, dfp['UFLUXv2_GPP'], c = colors[0], label = 'UFLUXv2_GPP')
ax.plot(dfp.index, dfp['Trendy_GPP'], c = colors[1], label = 'Trendy_GPP')
ax.fill_between(dfp.index, dfp['Trendy_GPP_25th'], dfp['Trendy_GPP_75th'], color = colors[1], alpha = 0.2)

upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('GPP (gC m-2d-1)')

# (dfp['UFLUXv2_GPP'] * 51042 * 365 * 0.5 * 0.5 * 1e5 * 1e5 / 1e15).plot()
((ufluxv2['GPPmML'] * coef_mat).sum(dim = ['longitude', 'latitude']).rename('UFLUXv2_GPP').drop_vars('spatial_ref').to_dataframe() / coef_PgC_gC * 365).plot(ylim = [100, 170])
# ufluxv2.where(cond, np.nan)['GPPmML'].mean(dim = 'time').to_dataframe().dropna()

dfp = pd.concat([
    (ufluxv2['GPPmML'].where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).rename('UFLUXv2_GPP').drop_vars('spatial_ref').to_dataframe() / coef_PgC_gC * 365,
    (gppT.where(cond, np.nan) * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe().rename(columns = {'gpp': 'Trendy_GPP', 'gpp_25th': 'Trendy_GPP_25th', 'gpp_75th': 'Trendy_GPP_75th'}) / coef_PgC_gC * 365,
], axis = 1)

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
ax.plot(dfp.index, dfp['UFLUXv2_GPP'], c = colors[0], label = 'UFLUXv2_GPP')
ax.plot(dfp.index, dfp['Trendy_GPP'], c = colors[1], label = 'Trendy_GPP')
# ax.fill_between(dfp.index, dfp['Trendy_GPP_25th'], dfp['Trendy_GPP_75th'], color = colors[1], alpha = 0.2)

upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('GPP (gC m-2d-1)')

"""### With old version"""

nc = []
for p in tqdm(list(root.joinpath("workspace/project_data/platform/2output_global_Pmodel-high").glob('MODIS*.nc'))):
    nct = xr.open_dataset(p, engine = 'netcdf4')['GPP'].resample(time = '1YS').mean()
    nc.append(nct)
nc = xr.merge(nc)

coef_mat = xr.DataArray(
    deg2m(nc.longitude, nc.latitude, 0.25, 0.25),
    dims = ['latitude', 'longitude'],
    coords = {'longitude': nc.longitude, 'latitude': nc.latitude}
).expand_dims(time = nc.time)

((nc * coef_mat).sum(dim = ['longitude', 'latitude']) / coef_PgC_gC * 365).to_dataframe().plot()

ncp_NIRv = []
for p in tqdm(list(root.joinpath("workspace/project_data/platform/1grid_NIRv_yearly").glob('MODIS*.nc'))):
    nct = xr.open_dataset(p, engine = 'netcdf4')# ['NIRv']
    dt = pd.to_datetime(p.stem.split('_')[1])
    nct = nct.where(nct['NIRv'] > 0, np.nan)
    ncp_NIRv.append(nct.expand_dims(time = [dt]))
ncp_NIRv = xr.merge(ncp_NIRv)

ncp_NIRv.mean(dim = ['longitude', 'latitude'])['NIRv'].plot()

ufluxv2['NIRv'].where(ufluxv2['NIRv'] > 0, np.nan).mean(dim = ['longitude', 'latitude']).plot()

# ufluxv2['NIRv'].mean(dim = 'time').plot()

# (ufluxv2['GPPmML'].mean(dim = 'time') * deg2m(NEE_dir.longitude, NEE_dir.latitude, 0.5, 0.5)).sum() / coef_PgC_gC * 365
# ((ufluxv2['GPPmML'] * deg2m(NEE_dir.longitude, NEE_dir.latitude, 0.5, 0.5)).sum(dim = ['longitude', 'latitude']) / coef_PgC_gC * 365).drop_vars('spatial_ref').to_dataframe().plot()

ufluxv2['GPPmML'].mean(dim = ['longitude', 'latitude']).drop_vars('spatial_ref').to_dataframe().plot()

ufluxv2['GPPm'].mean(dim = ['longitude', 'latitude']).drop_vars('spatial_ref').to_dataframe().plot()

(ufluxv2['GPPmML'].sel(time = "2002").mean(dim = 'time') - ufluxv2['GPPmML'].sel(time = "2001").mean(dim = 'time')).plot()

ufluxv2['GPPmML'].sel(time = "2001").mean(dim = 'time').mean()

ufluxv2['GPPmML'].sel(time = "2002").mean(dim = 'time').mean()

"""### NEE"""

NEE_ind = (ufluxv2['RECOmML'] - ufluxv2['GPPmML']).rename('NEE_ind').drop_vars('spatial_ref')
NEE_dir = ufluxv2['NEEmML'].rename('NEE_dir').drop_vars('spatial_ref')

(NEE_ind.mean(dim = 'time') - NEE_dir.mean(dim = 'time')).plot()

p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_npp_05deg_monthly_2000-2022.nc')
nppT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
nppT = nppT.where(nppT['npp'] != 0, np.nan)
nppT = nppT.resample(time = '1YS').mean()

neeT = xr.merge([
    nppT['npp'].rename('Trendy_NEE') * -1,
    nppT['npp_25th'].rename('Trendy_NEE_25th') * -1,
    nppT['npp_75th'].rename('Trendy_NEE_75th') * -1
])

neeT = (nppT * -1).rename({'npp': 'Trendy_NEE', 'npp_25th': 'Trendy_NEE_25th', 'npp_75th': 'Trendy_NEE_75th'})

NPPe = ufluxv2e['npp'].drop_vars('spatial_ref')
NEEe = (NPPe * -1).rename({
    'npp': 'UFLUXv2e_NEE',
    'npp_25th': 'UFLUXv2e_NEE_25th',
    'npp_75th': 'UFLUXv2e_NEE_75th',
})

# ==============================================================================

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
# dfp = pd.concat([
#     NEEe.mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     NEE_dir.mean(dim = ['longitude', 'latitude']).to_dataframe().rename(columns = {'NEE_dir': 'UFLUXv2_NEE_dir'}),
#     NEE_ind.mean(dim = ['longitude', 'latitude']).to_dataframe().rename(columns = {'NEE_ind': 'UFLUXv2_NEE_ind'}),
#     neeT.mean(dim = ['longitude', 'latitude']).to_dataframe(),
# ], axis = 1)

dfp = pd.concat([
    (NEEe * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
    (NEE_dir.to_dataset() * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe().rename(columns = {'NEE_dir': 'UFLUXv2_NEE_dir'}) / coef_PgC_gC * 365,
    (NEE_ind.to_dataset() * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe().rename(columns = {'NEE_ind': 'UFLUXv2_NEE_ind'}) / coef_PgC_gC * 365,
    (neeT * coef_mat).sum(dim = ['longitude', 'latitude']).to_dataframe() / coef_PgC_gC * 365,
], axis = 1)

# colorful:
ax.plot(dfp.index, dfp['UFLUXv2_NEE_dir'], c = colors[0], label = 'UFLUXv2_NEE_dir')
ax.plot(dfp.index, dfp['UFLUXv2_NEE_ind'], c = colors[1], label = 'UFLUXv2_NEE_ind')
ax.plot(dfp.index, dfp['UFLUXv2e_NEE'], c = colors[2], label = 'UFLUXv2e_NEE')
ax.fill_between(dfp.index, dfp['UFLUXv2e_NEE_25th'], dfp['UFLUXv2e_NEE_75th'], color = colors[2], alpha = 0.2)
ax.plot(dfp.index, dfp['Trendy_NEE'], c = colors[3], label = 'Trendy_NEE')
ax.fill_between(dfp.index, dfp['Trendy_NEE_25th'], dfp['Trendy_NEE_75th'], color = colors[3], alpha = 0.2)
# # grey:
# ax.plot(dfp.index, dfp['UFLUXv2_NEE_dir'], 'k-', label = 'UFLUXv2_NEE_dir')
# ax.plot(dfp.index, dfp['UFLUXv2_NEE_ind'], 'k-.', label = 'UFLUXv2_NEE_ind')
# ax.plot(dfp.index, dfp['UFLUXv2e_NEE'], 'k--', label = 'UFLUXv2e_NEE')
# ax.plot(dfp.index, dfp['Trendy_NEE'], 'k:', label = 'Trendy_NEE')
# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('NEE (Pg C yr-1)')

p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_npp_05deg_monthly_2000-2022.nc')
nppT = xr.open_dataset(p, engine="netcdf4") * 1000 * 86400 # kg m-2 s-1 => g m-2 d-1
nppT = nppT.where(nppT['npp'] != 0, np.nan)
nppT = nppT.resample(time = '1YS').mean()

neeT = xr.merge([
    nppT['npp'].rename('Trendy_NEE') * -1,
    nppT['npp_25th'].rename('Trendy_NEE_25th') * -1,
    nppT['npp_75th'].rename('Trendy_NEE_75th') * -1
])

neeT = (nppT * -1).rename({'npp': 'Trendy_NEE', 'npp_25th': 'Trendy_NEE_25th', 'npp_75th': 'Trendy_NEE_75th'})

NPPe = ufluxv2e['npp'].drop_vars('spatial_ref')
NEEe = (NPPe * -1).rename({
    'npp': 'UFLUXv2e_NEE',
    'npp_25th': 'UFLUXv2e_NEE_25th',
    'npp_75th': 'UFLUXv2e_NEE_75th',
})

# ==============================================================================

cond = (~(NEEe['UFLUXv2e_NEE'].isnull())) & (~(neeT['Trendy_NEE'].isnull())) & (~(NEE_dir.isnull())) & (~(NEE_ind.isnull()))

fig, ax = setup_canvas(1, 1, figsize = (6, 4), fontsize = 10, labelsize = 10)
# dfp = pd.concat([
#     NEEe.mean(dim = ['longitude', 'latitude']).to_dataframe(),
#     NEE_dir.mean(dim = ['longitude', 'latitude']).to_dataframe().rename(columns = {'NEE_dir': 'UFLUXv2_NEE_dir'}),
#     NEE_ind.mean(dim = ['longitude', 'latitude']).to_dataframe().rename(columns = {'NEE_ind': 'UFLUXv2_NEE_ind'}),
#     neeT.mean(dim = ['longitude', 'latitude']).to_dataframe(),
# ], axis = 1)

dfp = pd.concat([
    NEEe.where(cond, np.nan).median(dim = ['longitude', 'latitude']).to_dataframe(),
    NEE_dir.where(cond, np.nan).to_dataset().median(dim = ['longitude', 'latitude']).to_dataframe().rename(columns = {'NEE_dir': 'UFLUXv2_NEE_dir'}),
    NEE_ind.where(cond, np.nan).to_dataset().median(dim = ['longitude', 'latitude']).to_dataframe().rename(columns = {'NEE_ind': 'UFLUXv2_NEE_ind'}),
    neeT.where(cond, np.nan).median(dim = ['longitude', 'latitude']).to_dataframe(),
], axis = 1)

# colorful:
ax.plot(dfp.index, dfp['UFLUXv2_NEE_dir'], c = colors[0], label = 'UFLUXv2_NEE_dir')
ax.plot(dfp.index, dfp['UFLUXv2_NEE_ind'], c = colors[1], label = 'UFLUXv2_NEE_ind')
ax.plot(dfp.index, dfp['UFLUXv2e_NEE'], c = colors[2], label = 'UFLUXv2e_NEE')
ax.fill_between(dfp.index, dfp['UFLUXv2e_NEE_25th'], dfp['UFLUXv2e_NEE_75th'], color = colors[2], alpha = 0.2)
ax.plot(dfp.index, dfp['Trendy_NEE'], c = colors[3], label = 'Trendy_NEE')
ax.fill_between(dfp.index, dfp['Trendy_NEE_25th'], dfp['Trendy_NEE_75th'], color = colors[3], alpha = 0.2)
# # grey:
# ax.plot(dfp.index, dfp['UFLUXv2_NEE_dir'], 'k-', label = 'UFLUXv2_NEE_dir')
# ax.plot(dfp.index, dfp['UFLUXv2_NEE_ind'], 'k-.', label = 'UFLUXv2_NEE_ind')
# ax.plot(dfp.index, dfp['UFLUXv2e_NEE'], 'k--', label = 'UFLUXv2e_NEE')
# ax.plot(dfp.index, dfp['Trendy_NEE'], 'k:', label = 'Trendy_NEE')
# end
upper_legend(ax, nrows = 1, yloc = 1.15)
ax.set_ylabel('NEE (gC m-2 d-1)')

ufluxv2['NIRv'].mean(dim = 'time').plot(vmin = 0)

ufluxv2['LEmML'].mean(dim = 'time').plot()

ufluxv2['NEEmML'].mean(dim = 'time').plot()

ufluxv2['GPPmML'].mean(dim = 'time').plot(vmin = 0)

ufluxv2['GPPm'].mean(dim = 'time').plot(vmin = 0)

(ufluxv2['GPPmML'] - ufluxv2['GPPm']).mean(dim = 'time').plot()

# nct = xr.open_dataset(p, engine = 'netcdf4')
# if dt.year == 2000:
#     lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
# else:
#     lucc = luccr.sel(time = dt)
# monthly_time = pd.date_range(
#     dt.strftime('%Y-%m-%d'),
#     (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
#     freq = "MS"
# )
# lucc = lucc.expand_dims(time = monthly_time).rename({'LC_Type1': 'IGBP'})
# nct = xr.merge([nct, lucc])
# nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
# # nct['GPPmML'].mean(dim = 'time').plot()

# nct['GPPmML'].mean(dim = 'time')

ufluxv2['GPPmML'].mean(dim = 'time').plot()

ufluxv2['GPPmML'].mean(dim = ['longitude', 'latitude']).plot(ylim = [0, 5])

ra, rh = [], []
for dt in tqdm(df_path_ufluxv2e.index):
    p_ra = df_path_ufluxv2e.loc[dt, 'ra']
    p_rh = df_path_ufluxv2e.loc[dt, 'rh']

    rat = xr.open_dataset(p_ra, engine = 'netcdf4').resample(time = '1YS').mean()
    rht = xr.open_dataset(p_rh, engine = 'netcdf4').resample(time = '1YS').mean()
    ra.append(rat); rh.append(rht)

ra = xr.merge(ra); rh = xr.merge(rh)

reco = (ra['ra'] + rh['rh']).rename('reco')
reco_25th = (ra['ra_25th'] + rh['rh_25th']).rename('reco_25th')
reco_75th = (ra['ra_75th'] + rh['rh_75th']).rename('reco_75th')



dfp = pd.concat([
    ufluxv2[['GPPmML', 'RECOmML']].drop_vars('spatial_ref').rename({'GPPmML': 'gppU', 'RECOmML': 'recoU'}).mean(dim = ['longitude', 'latitude']).to_dataframe(),
    reco.drop_vars('spatial_ref').mean(dim = ['longitude', 'latitude']).to_dataframe(),
    reco_25th.drop_vars('spatial_ref').mean(dim = ['longitude', 'latitude']).to_dataframe(),
    reco_75th.drop_vars('spatial_ref').mean(dim = ['longitude', 'latitude']).to_dataframe(),
    recoT.mean(dim = ['longitude', 'latitude']).to_dataframe(),
    recoT_25th.mean(dim = ['longitude', 'latitude']).to_dataframe(),
    recoT_75th.mean(dim = ['longitude', 'latitude']).to_dataframe()
], axis = 1).drop('gppU', axis = 1)
dfp.plot()

dfp = pd.concat([
    ufluxv2[['GPPmML', 'RECOmML']].drop_vars('spatial_ref').rename({'GPPmML': 'gppU', 'RECOmML': 'recoU'}).mean(dim = ['longitude', 'latitude']).to_dataframe(),
    reco.drop_vars('spatial_ref').mean(dim = ['longitude', 'latitude']).to_dataframe(),
    reco_25th.drop_vars('spatial_ref').mean(dim = ['longitude', 'latitude']).to_dataframe(),
    reco_75th.drop_vars('spatial_ref').mean(dim = ['longitude', 'latitude']).to_dataframe(),
    recoT.mean(dim = ['longitude', 'latitude']).to_dataframe(),
    recoT_25th.mean(dim = ['longitude', 'latitude']).to_dataframe(),
    recoT_75th.mean(dim = ['longitude', 'latitude']).to_dataframe()
], axis = 1).drop('gppU', axis = 1)
dfp.plot()

cVeg = []
for dt in tqdm(df_path_ufluxv2e.index):
    p = df_path_ufluxv2e.loc[dt, 'cSoil']

    cVegt = xr.open_dataset(p, engine = 'netcdf4')
    cVeg.append(cVegt)

cVeg = xr.merge(cVeg)

cVeg.drop_vars(['spatial_ref', 'IGBP']).mean(dim = ['longitude', 'latitude']).to_dataframe().plot() # vmin = 150000, vmax = 350000

p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_cSoil_05deg_yearly_2000-2022.nc')
cVeg = xr.open_dataset(p, engine="netcdf4") * 1000
cVeg = cVeg.where(cVeg['cSoil'] > 1e-9)
cVeg.mean(dim = ['longitude', 'latitude']).to_dataframe().plot()

cname = 'ra'
ufluxv2e_ra = []
for dt in df_path_ufluxv2e.index:
    p = df_path_ufluxv2e.loc[dt, cname]
    nct = xr.open_dataset(p, engine = 'netcdf4')
    nct[cname] = nct[cname].where(nct[cname] >= 1e-9, np.nan)
    if dt.year == 2000:
        lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
    else:
        lucc = luccr.sel(time = dt)
    monthly_time = pd.date_range(
        dt.strftime('%Y-%m-%d'),
        (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
        freq = "MS"
    )
    lucc = lucc.expand_dims(time = monthly_time).rename({'LC_Type1': 'IGBP'})
    nct = xr.merge([nct, lucc])
    nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
    # nct = nct.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).resample(time = '1YS').mean()
    ufluxv2e_ra.append(nct.resample(time = '1YS').mean())
ufluxv2e_ra = xr.merge(ufluxv2e_ra)

cname = 'rh'
ufluxv2e_rh = []
for dt in df_path_ufluxv2e.index:
    p = df_path_ufluxv2e.loc[dt, cname]
    nct = xr.open_dataset(p, engine = 'netcdf4')
    nct[cname] = nct[cname].where(nct[cname] >= 1e-9, np.nan)
    if dt.year == 2000:
        lucc = luccr.sel(time = pd.to_datetime('2001-01-01'))
    else:
        lucc = luccr.sel(time = dt)
    monthly_time = pd.date_range(
        dt.strftime('%Y-%m-%d'),
        (dt + pd.DateOffset(months = 11)).strftime('%Y-%m-%d'),
        freq = "MS"
    )
    lucc = lucc.expand_dims(time = monthly_time).rename({'LC_Type1': 'IGBP'})
    nct = xr.merge([nct, lucc])
    nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
    # nct = nct.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).resample(time = '1YS').mean()
    ufluxv2e_rh.append(nct.resample(time = '1YS').mean())
ufluxv2e_rh = xr.merge(ufluxv2e_rh)

df_reco_uflux2 = ufluxv2['RECOmML'].rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).mean(dim = ['longitude', 'latitude']).drop_vars('spatial_ref').to_dataframe()

reco = ufluxv2e_ra['ra'] + ufluxv2e_rh['rh']
df_reco_mn = reco.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).mean(dim = ['longitude', 'latitude']).drop_vars('spatial_ref').rename('trendy_reco_e').to_dataframe()

reco = ufluxv2e_ra['ra_25th'] + ufluxv2e_rh['rh_25th']
df_reco_25th = reco.rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).mean(dim = ['longitude', 'latitude']).drop_vars('spatial_ref').rename('trendy_reco_25').to_dataframe()

reco = ufluxv2e_ra['ra_75th'] + ufluxv2e_rh['rh_75th']
df_reco_75th = reco.rename('trendy_reco_75').mean(dim = ['longitude', 'latitude']).drop_vars('spatial_ref').to_dataframe()

# .rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).mean(dim = ['longitude', 'latitude']).drop_vars('spatial_ref')

ax = pd.concat([
    df_reco_uflux2,
    df_reco_mn,
    df_reco_25th,
    df_reco_75th,
    (nct.where(nct > 1e-9) * 1000 * 86400).mean(dim = ['longitude', 'latitude']).rename('reco_trendy').to_dataframe().resample('1YS').mean()
], axis = 1).plot()#.scatter(x = 'RECOmML', y = 'trendy_reco_e')

# ax.set_ylim(2, 3)

stats.linregress(df_reco_mn['trendy_reco_e'], df_reco_uflux2['RECOmML'])

ufluxv2['GPPmML'].rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).mean(dim = ['longitude', 'latitude']).drop_vars('spatial_ref').to_dataframe().plot()

ufluxv2['GPPmML'].mean(dim = ['longitude', 'latitude']).plot(ylim = [0, 5])

(ncc_in['gpp'].rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).mean(dim = ['longitude', 'latitude']).to_dataframe().resample('1YS').mean()['gpp'] * 86400 * 1000).plot(ylim = [0, 5])

cname = 'ra' # ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']
if cname in ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']:
    cfreq = 'yearly'
elif cname in ['gpp', 'npp', 'nbp', 'ra', 'rh', 'fGrazing']:
    cfreq = 'monthly'
else:
    raise ValueError('Unknown cname')
    cfreq = None
p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_{cfreq}_2000-2022.nc')
# ncc_in = xr.open_dataset(p,  engine="netcdf4")
ncc_ra = xr.open_dataset(p,  engine="netcdf4")

cname = 'rh' # ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']
if cname in ['cVeg', 'cLitter', 'cSoil', 'cLeaf', 'cWood']:
    cfreq = 'yearly'
elif cname in ['gpp', 'npp', 'nbp', 'ra', 'rh', 'fGrazing']:
    cfreq = 'monthly'
else:
    raise ValueError('Unknown cname')
    cfreq = None
p = root_proj_trendy.joinpath('2_preproc/Trendy_model_averages').joinpath(f'TrendyV12_{cname}_05deg_{cfreq}_2000-2022.nc')
ncc_rh = xr.open_dataset(p,  engine="netcdf4")

nct = (ncc_ra['ra'] + ncc_rh['rh'])
(nct.where(nct > 1e-9) * 1000 * 86400).mean(dim = ['longitude', 'latitude']).rename('reco_trendy').to_dataframe().resample('1YS').mean().plot()

((ncc_ra['ra'] + ncc_rh['rh']).mean(dim = ['longitude', 'latitude']).rename('reco_trendy').to_dataframe().resample('1YS').mean() * 1000 * 86400).plot()

# nct['GPPmML'].rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs).mean(dim = 'time').plot()

# ufluxv2.mean(dim = ['longitude', 'latitude']).to_dataframe()[['GPPmML', 'RECOmML']].plot()

# (ufluxv2['RECOmML'] - ufluxv2['GPPmML']).rename('NEEmML').mean(dim = ['longitude', 'latitude']).to_dataframe()['NEEmML'].plot()

# (ufluxv2['RECOmML'] - ufluxv2['GPPmML']).mean(dim = 'time').plot(figsize = (10, 10), vmin = -3, vmax = 3, cmap = 'coolwarm')

# ufluxv2['GPPmML'].mean(dim = 'time').plot(figsize = (10, 10))

# ufluxv2['RECOmML'].mean(dim = 'time').plot(figsize = (10, 10))

# nct = (ufluxv2['RECOmML'] - ufluxv2['GPPmML']).mean(dim = 'time')
# # nct.where(~ufluxv2e.mean(dim = 'time')['IGBP'].isin([13, 15, 16, 17]), drop = True).plot()
# nct.where(~ufluxv2e[cname].mean(dim = 'time').isnull()).plot()
# google.download_file(plt.gcf(), 'test.pdf')

# igbp = ufluxv2['IGBP'].mean(dim = 'time').astype(int)

cname = 'cWood'
ufluxv2e = []
for p in tqdm(df_path_ufluxv2e[cname]):
    nct = xr.open_dataset(p, engine = 'netcdf4')
    nct = nct.where(~nct['IGBP'].isin([13, 15, 16, 17]), np.nan)
    nct = nct.resample(time = '1YS').mean()
    ufluxv2e.append(nct)
ufluxv2e = xr.merge(ufluxv2e)

ncce = ufluxv2e[cname].mean(dim = 'time')
# ncce = ncce.where(~ufluxv2e['IGBP'].isin([13, 15, 16, 17]), drop = True).rio.write_crs("epsg:4326", inplace = False).rio.clip(world.buffer(0.0001).geometry.values, world.crs)
ncce.plot(vmin = 0)

ncc = ncc_in[cname].mean(dim = 'time')
nct = -ncc.where(ncc != 0)
nct.plot()

ncc